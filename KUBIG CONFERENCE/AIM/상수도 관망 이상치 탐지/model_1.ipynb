{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0832db36",
   "metadata": {},
   "source": [
    "## 상수도 이상 수압계 감지 - 모델기반\n",
    "총 3개의 코드로 분할되어 있어 병합 필수!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bcab0b",
   "metadata": {},
   "source": [
    "### module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b6f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c157e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6191de",
   "metadata": {},
   "source": [
    "### data & pipe network load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f28b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = pd.read_csv(\"./train/TRAIN_A.csv\").sort_values(\"timestamp\").reset_index(drop=True)\n",
    "train_b = pd.read_csv(\"./train/TRAIN_B.csv\").sort_values(\"timestamp\").reset_index(drop=True)\n",
    "test_info = pd.read_csv(\"./test.csv\")\n",
    "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
    "\n",
    "exclude_cols = [\"timestamp\",\"anomaly\"]\n",
    "use_cols_a = [c for c in train_a.columns if c not in exclude_cols and not c.endswith(\"_flag\")]\n",
    "use_cols_b = [c for c in train_b.columns if c not in exclude_cols and not c.endswith(\"_flag\")]\n",
    "intersect_cols = list(set(use_cols_a).intersection(set(use_cols_b)))\n",
    "\n",
    "df_all = pd.concat([train_a[intersect_cols], train_b[intersect_cols]], axis=0, ignore_index=True)\n",
    "min_vals = df_all.min()\n",
    "max_vals = df_all.max() + 1e-8  # 분모 0 방지용\n",
    "\n",
    "train_a_scaled = train_a.copy()\n",
    "train_b_scaled = train_b.copy()\n",
    "\n",
    "for c in intersect_cols:\n",
    "    # Min-Max 스케일링\n",
    "    train_a_scaled[c] = (train_a_scaled[c] - min_vals[c]) / (max_vals[c] - min_vals[c])\n",
    "    train_b_scaled[c] = (train_b_scaled[c] - min_vals[c]) / (max_vals[c] - min_vals[c])\n",
    "\n",
    "print(\"Train A scaled shape:\", train_a_scaled.shape)\n",
    "print(\"Train B scaled shape:\", train_b_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed167d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 관망 구조\n",
    "adjacency_list_A = {\n",
    "    'P1' :  [['P5','P6'], [], ['Q1']],\n",
    "    'P2' :  [['P4'], [], ['Q3']],\n",
    "    'P3' :  [['P5','P6'], ['Q2'], ['Q5']],\n",
    "    'P4' :  [['P5','P6'], ['Q3'], ['Q5']],\n",
    "    'P5' :  [['P8','P9'], ['Q1','Q2','Q3'], ['Q5']],\n",
    "    'P6' :  [['P8','P9'], ['Q1','Q2','Q3'], ['Q5']],\n",
    "    'P7' :  [['P9'], ['Q4'], ['Q5']],\n",
    "    'P8' :  [['P10','P12','P19','P26'], ['Q1','Q2','Q3'], ['Q5']],\n",
    "    'P9' :  [['P10','P12','P19','P26'], ['Q1','Q2','Q3','Q4'], ['Q5']],\n",
    "    'P10':  [['P11'], ['Q1','Q2','Q3','Q4'], ['Q5']],\n",
    "    'P11':  [['P14'], ['Q1','Q2','Q3','Q4'], ['Q5']],\n",
    "    'P12':  [['P13'], ['Q1','Q2','Q3','Q4'], ['Q5']],\n",
    "    'P13':  [['P18','P14'], ['Q1','Q2','Q3','Q4'], ['Q5']],\n",
    "    'P14':  [['P13','P15'], ['Q1','Q2','Q3','Q4'], ['Q5']],\n",
    "    'P15':  [['P16'], ['Q1','Q2','Q3','Q4'], ['Q5']],\n",
    "    'P16':  [['P17','P19'], ['Q1','Q2','Q3','Q4'], ['Q5']],\n",
    "    'P17':  [[], ['Q1','Q2','Q3','Q4'], ['Q5']],\n",
    "    'P18':  [['P19','P20'], ['Q1','Q2','Q3','Q4'], ['Q5']],\n",
    "    'P19':  [['P18','P22','P26'], ['Q1','Q2','Q3','Q4'], ['Q5']],\n",
    "    'P20':  [['P21'], ['Q1','Q2','Q3','Q4'], ['Q5']],\n",
    "    'P21':  [['P24'], ['Q1','Q2','Q3','Q4'], ['Q5']],\n",
    "    'P22':  [['P23'], ['Q1','Q2','Q3','Q4'], ['Q5']],\n",
    "    'P23':  [['P25'], ['Q1','Q2','Q3','Q4'], ['Q5']],\n",
    "    'P24':  [[], ['Q1','Q2','Q3','Q4'], ['Q5']],\n",
    "    'P25':  [[], ['Q1','Q2','Q3','Q4'], ['Q5']],\n",
    "    'P26':  [[], ['Q1','Q2','Q3','Q4'], ['Q5']]\n",
    "}\n",
    "nodes_A = list(adjacency_list_A.keys())\n",
    "\n",
    "adjacency_list_B = {\n",
    "    'P1': [['P2'], [], ['Q1']],\n",
    "    'P2': [['P3'], ['Q1'], ['Q2','Q3','Q4']],\n",
    "    'P3': [['P4','P5'], ['Q1'], ['Q2','Q3','Q4']],\n",
    "    'P4': [[], ['Q1'], ['Q2']],\n",
    "    'P5': [['P6'], ['Q1'], ['Q3','Q4']],\n",
    "    'P6': [['P7','P8'], ['Q1'], ['Q3','Q4']],\n",
    "    'P7': [[], ['Q1'], ['Q3']],\n",
    "    'P8': [['P9'], ['Q1'], ['Q4']],\n",
    "    'P9': [['P10'], ['Q1'], ['Q4']],\n",
    "    'P10': [[], ['Q1'], ['Q4']]\n",
    "}\n",
    "nodes_B = list(adjacency_list_B.keys())\n",
    "\n",
    "adjacency_list_C = {\n",
    "    'P1':  [['P3'], ['Q1'], ['Q2','Q3','Q4','Q5','Q8']],\n",
    "    'P2':  [['P4'], ['Q1'], ['Q2','Q3','Q4','Q5','Q8']],\n",
    "    'P3':  [['P5','P8'], ['Q1'], ['Q2','Q3','Q4','Q5','Q8']],\n",
    "    'P4':  [['P5','P8'], ['Q1'], ['Q2','Q3','Q4','Q5','Q8']],\n",
    "    'P5':  [[], ['Q1'], ['Q2','Q3','Q4','Q5']],\n",
    "    'P6':  [[], ['Q1'], ['Q2','Q3','Q4','Q5']],\n",
    "    'P7':  [[], ['Q7'], []],\n",
    "    'P8':  [[], ['Q8'], []]\n",
    "}\n",
    "nodes_C = list(adjacency_list_C.keys())\n",
    "\n",
    "adjacency_list_D = {\n",
    "    'P1':  [['P3'], ['Q1'], ['Q2','Q3','Q4','Q5']],\n",
    "    'P2':  [['P3'], ['Q1'], ['Q2','Q3','Q4','Q5']],\n",
    "    'P3':  [['P4','P6'], ['Q1'], ['Q3','Q5']],\n",
    "    'P4':  [[], ['Q3'], []],\n",
    "    'P5':  [[], ['Q1'], ['Q4']],\n",
    "    'P6':  [[], ['Q1'], ['Q5']]\n",
    "}\n",
    "nodes_D = list(adjacency_list_D.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d695fa8",
   "metadata": {},
   "source": [
    "### Dataset Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d7c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def bfs_n(start_p, adjacency, n=2):\n",
    "    visited = set([start_p])\n",
    "    queue = deque([(start_p, 0)])\n",
    "    res = []\n",
    "    pbar = tqdm(desc=f\"BFS for {start_p}\", leave=False)\n",
    "\n",
    "    while queue:\n",
    "        cur, depth = queue.popleft()\n",
    "        pbar.update(1)\n",
    "\n",
    "        if depth == n:\n",
    "            continue\n",
    "        nexts = adjacency[cur][0]\n",
    "        for nxt in nexts:\n",
    "            if nxt not in visited:\n",
    "                visited.add(nxt)\n",
    "                res.append(nxt)\n",
    "                queue.append((nxt, depth+1))\n",
    "    pbar.close()\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14214e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pre_post_nodes(pnode, adjacency, n=2):\n",
    "    pre_nodes = bfs_n(pnode, adjacency, n)\n",
    "    post_nodes = bfs_n(pnode, adjacency, n)\n",
    "    in_q_list = adjacency[pnode][1] if len(adjacency[pnode])>1 else []\n",
    "    out_q_list = adjacency[pnode][2] if len(adjacency[pnode])>2 else []\n",
    "    return pre_nodes, post_nodes, in_q_list, out_q_list\n",
    "\n",
    "def window_features_expanded(\n",
    "    df,\n",
    "    adjacency,\n",
    "    pnode,\n",
    "    n=2,\n",
    "    window_size=60,\n",
    "    step=60,\n",
    "    flag_prefix=\"P\"\n",
    "):\n",
    "    pre_nodes, post_nodes, in_q_list, out_q_list = find_pre_post_nodes(pnode, adjacency, n)\n",
    "    pnode_flag = f\"{pnode}_flag\" if flag_prefix else f\"{pnode}_flag\"\n",
    "\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "    ln = len(df)\n",
    "    df_cols = set(df.columns)\n",
    "\n",
    "    loop_range = range(0, ln - window_size + 1, step)\n",
    "    pbar = tqdm(loop_range, desc=f\"window_features({pnode})\", leave=False)\n",
    "\n",
    "    for start_idx in pbar:\n",
    "        chunk = df.iloc[start_idx : start_idx + window_size]\n",
    "\n",
    "        # inQ 합\n",
    "        if len(in_q_list)>0:\n",
    "            q_in_sum = chunk[in_q_list].sum(axis=1).values.reshape(window_size,1)\n",
    "        else:\n",
    "            q_in_sum = np.zeros((window_size,1))\n",
    "\n",
    "        # pre(n개)\n",
    "        pre_feats = []\n",
    "        for pn in pre_nodes[:n]:\n",
    "            if pn in df_cols:\n",
    "                pre_feats.append(chunk[pn].values.reshape(window_size,1))\n",
    "            else:\n",
    "                pre_feats.append(np.zeros((window_size,1)))\n",
    "        if len(pre_feats)<n:\n",
    "            for _ in range(n-len(pre_feats)):\n",
    "                pre_feats.append(np.zeros((window_size,1)))\n",
    "        pre_cat = np.concatenate(pre_feats, axis=1) if len(pre_feats)>0 else np.zeros((window_size,n))\n",
    "\n",
    "        # pnode\n",
    "        if pnode in df_cols:\n",
    "            pnode_val = chunk[pnode].values.reshape(window_size,1)\n",
    "        else:\n",
    "            pnode_val = np.zeros((window_size,1))\n",
    "\n",
    "        # post(n개)\n",
    "        post_feats = []\n",
    "        for pn in post_nodes[:n]:\n",
    "            if pn in df_cols:\n",
    "                post_feats.append(chunk[pn].values.reshape(window_size,1))\n",
    "            else:\n",
    "                post_feats.append(np.zeros((window_size,1)))\n",
    "        if len(post_feats)<n:\n",
    "            for _ in range(n-len(post_feats)):\n",
    "                post_feats.append(np.zeros((window_size,1)))\n",
    "        post_cat = np.concatenate(post_feats, axis=1) if len(post_feats)>0 else np.zeros((window_size,n))\n",
    "\n",
    "        # outQ\n",
    "        if len(out_q_list)>0:\n",
    "            q_out_sum = chunk[out_q_list].sum(axis=1).values.reshape(window_size,1)\n",
    "        else:\n",
    "            q_out_sum = np.zeros((window_size,1))\n",
    "\n",
    "        feat = np.concatenate([q_in_sum, pre_cat, pnode_val, post_cat, q_out_sum], axis=1)\n",
    "        X_list.append(feat)\n",
    "\n",
    "        # 라벨\n",
    "        if pnode_flag in df_cols:\n",
    "            y_chunk = chunk[pnode_flag].values\n",
    "        else:\n",
    "            y_chunk = np.zeros(window_size)\n",
    "        y_val = y_chunk.max()\n",
    "        y_list.append(y_val)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    X_arr = np.array(X_list)\n",
    "    y_arr = np.array(y_list)\n",
    "    return X_arr, y_arr\n",
    "\n",
    "def window_features_expanded_allP(\n",
    "    df,\n",
    "    adjacency,\n",
    "    pnodes=None,\n",
    "    n=2,\n",
    "    window_size=60,\n",
    "    step=60,\n",
    "    flag_prefix=\"P\"\n",
    "):\n",
    "    if pnodes is None:\n",
    "        pnodes = list(adjacency.keys())\n",
    "\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    pbar = tqdm(pnodes, desc=\"All P nodes\", leave=True)\n",
    "\n",
    "    for pnode in pbar:\n",
    "        X_p, y_p = window_features_expanded(\n",
    "            df=df,\n",
    "            adjacency=adjacency,\n",
    "            pnode=pnode,\n",
    "            n=n,\n",
    "            window_size=window_size,\n",
    "            step=step,\n",
    "            flag_prefix=flag_prefix\n",
    "        )\n",
    "        X_list.append(X_p)\n",
    "        y_list.append(y_p)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    if len(X_list)>0:\n",
    "        X_all = np.concatenate(X_list, axis=0)\n",
    "        y_all = np.concatenate(y_list, axis=0)\n",
    "    else:\n",
    "        X_all = np.array([])\n",
    "        y_all = np.array([])\n",
    "\n",
    "    return X_all, y_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1772db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_distance = 2\n",
    "window_size = 60\n",
    "step = 60\n",
    "\n",
    "Xa_all, ya_all = window_features_expanded_allP(\n",
    "    df=train_a_scaled,\n",
    "    adjacency=adjacency_list_A,\n",
    "    n=n_distance,\n",
    "    window_size=window_size,\n",
    "    step=step\n",
    ")\n",
    "print(\"Xa_all.shape =\", Xa_all.shape, \"ya_all.shape =\", ya_all.shape)\n",
    "\n",
    "Xb_all, yb_all = window_features_expanded_allP(\n",
    "    df=train_b_scaled,\n",
    "    adjacency=adjacency_list_B,\n",
    "    n=n_distance,\n",
    "    window_size=window_size,\n",
    "    step=step\n",
    ")\n",
    "print(\"Xb_all.shape =\", Xb_all.shape, \"yb_all.shape =\", yb_all.shape)\n",
    "\n",
    "X_train = np.concatenate([Xa_all, Xb_all], axis=0) if Xa_all.size>0 and Xb_all.size>0 else Xa_all\n",
    "y_train = np.concatenate([ya_all, yb_all], axis=0) if ya_all.size>0 and yb_all.size>0 else ya_all\n",
    "print(\"X_train.shape =\", X_train.shape)\n",
    "print(\"y_train.shape =\", y_train.shape)\n",
    "\n",
    "class WaterDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        x_data = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        y_data = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        return x_data, y_data\n",
    "\n",
    "train_dataset = WaterDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
