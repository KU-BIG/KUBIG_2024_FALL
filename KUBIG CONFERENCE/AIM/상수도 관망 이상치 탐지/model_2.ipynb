{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65b1fee",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "파일 병합 필수!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b79be",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_distance = 2\n",
    "window_size = 60\n",
    "step = 60\n",
    "\n",
    "Xa_all, ya_all = window_features_expanded_allP(\n",
    "    df=train_a_scaled,\n",
    "    adjacency=adjacency_list_A,\n",
    "    n=n_distance,\n",
    "    window_size=window_size,\n",
    "    step=step\n",
    ")\n",
    "print(\"Xa_all.shape =\", Xa_all.shape, \"ya_all.shape =\", ya_all.shape)\n",
    "\n",
    "Xb_all, yb_all = window_features_expanded_allP(\n",
    "    df=train_b_scaled,\n",
    "    adjacency=adjacency_list_B,\n",
    "    n=n_distance,\n",
    "    window_size=window_size,\n",
    "    step=step\n",
    ")\n",
    "print(\"Xb_all.shape =\", Xb_all.shape, \"yb_all.shape =\", yb_all.shape)\n",
    "\n",
    "X_train = np.concatenate([Xa_all, Xb_all], axis=0) if Xa_all.size>0 and Xb_all.size>0 else Xa_all\n",
    "y_train = np.concatenate([ya_all, yb_all], axis=0) if ya_all.size>0 and yb_all.size>0 else ya_all\n",
    "print(\"X_train.shape =\", X_train.shape)\n",
    "print(\"y_train.shape =\", y_train.shape)\n",
    "\n",
    "class WaterDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        x_data = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        y_data = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "        return x_data, y_data\n",
    "\n",
    "train_dataset = WaterDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eece6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assoc_discrep(a, b):\n",
    "    return torch.sum(a * torch.log((a+1e-6)/(b+1e-6) + 1e-6))\n",
    "\n",
    "class PosEnc(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        max_len = 10000\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0,d_model,2,dtype=torch.float)*(-math.log(10000.0)/d_model))\n",
    "        pe[:,0::2] = torch.sin(pos*div)\n",
    "        pe[:,1::2] = torch.cos(pos*div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x):\n",
    "        L = x.size(1)\n",
    "        return x + self.pe[:,:L,:]\n",
    "\n",
    "class AnomalyBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dk = d_model//n_heads\n",
    "        self.qA = nn.Linear(d_model,d_model)\n",
    "        self.kA = nn.Linear(d_model,d_model)\n",
    "        self.vA = nn.Linear(d_model,d_model)\n",
    "        self.qB = nn.Linear(d_model,d_model)\n",
    "        self.kB = nn.Linear(d_model,d_model)\n",
    "        self.vB = nn.Linear(d_model,d_model)\n",
    "        self.out_lin = nn.Linear(d_model,d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model,4*d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*d_model,d_model)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        B,L,D = x.shape\n",
    "        qa = self.qA(x).reshape(B,L,self.n_heads,self.dk).permute(0,2,1,3)\n",
    "        ka = self.kA(x).reshape(B,L,self.n_heads,self.dk).permute(0,2,1,3)\n",
    "        va = self.vA(x).reshape(B,L,self.n_heads,self.dk).permute(0,2,1,3)\n",
    "        sc_a = torch.matmul(qa,ka.transpose(-2,-1))/math.sqrt(self.dk)\n",
    "        attn_a = F.softmax(sc_a, dim=-1)\n",
    "        out_a = torch.matmul(attn_a, va).permute(0,2,1,3).reshape(B,L,D)\n",
    "\n",
    "        qb = self.qB(x).reshape(B,L,self.n_heads,self.dk).permute(0,2,1,3)\n",
    "        kb = self.kB(x).reshape(B,L,self.n_heads,self.dk).permute(0,2,1,3)\n",
    "        vb = self.vB(x).reshape(B,L,self.n_heads,self.dk).permute(0,2,1,3)\n",
    "        sc_b = torch.matmul(qb,kb.transpose(-2,-1))/math.sqrt(self.dk)\n",
    "        attn_b = F.softmax(sc_b, dim=-1)\n",
    "        out_b = torch.matmul(attn_b, vb).permute(0,2,1,3).reshape(B,L,D)\n",
    "\n",
    "        out = (out_a+out_b)/2.0\n",
    "        out = self.out_lin(out)\n",
    "        x2 = self.norm(x+out)\n",
    "        x3 = self.norm(x2+self.ffn(x2))\n",
    "        return x3, attn_a, attn_b\n",
    "\n",
    "class AnomalyEncoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            AnomalyBlock(d_model, n_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        a_list=[]\n",
    "        b_list=[]\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out, A, B = layer(out)\n",
    "            a_list.append(A)\n",
    "            b_list.append(B)\n",
    "        return out, a_list, b_list\n",
    "\n",
    "class AnomalyTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=7, d_model=64, n_heads=4, num_layers=2, lambd=0.01):\n",
    "        super().__init__()\n",
    "        self.input_fc = nn.Linear(input_dim, d_model)\n",
    "        self.pos = PosEnc(d_model)\n",
    "        self.encoder = AnomalyEncoder(d_model, n_heads, num_layers)\n",
    "        self.recon = nn.Linear(d_model, input_dim)\n",
    "        self.cls_head = nn.Linear(d_model, 1)\n",
    "        self.lambd = lambd\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, input_dim)\n",
    "        x_proj = self.input_fc(x)\n",
    "        x_pe = self.pos(x_proj)\n",
    "        enc_out, A_list, B_list = self.encoder(x_pe)\n",
    "        rec = self.recon(enc_out)\n",
    "        cls_logit = self.cls_head(enc_out.mean(dim=1))\n",
    "        return rec, cls_logit, A_list, B_list\n",
    "    def compute_loss(self, x, rec, logit, y, A_list, B_list):\n",
    "        r_loss = F.mse_loss(rec, x, reduction='mean')\n",
    "        d_loss=0\n",
    "        for A,B in zip(A_list,B_list):\n",
    "            d_loss+=assoc_discrep(A,B)\n",
    "        d_loss/= len(A_list)\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logit.squeeze(-1), y)\n",
    "        return r_loss + self.lambd*d_loss + bce_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d4a05b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5459d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=5, delta=1e-4, path=\"best_anomaly_transformer.pth\"):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.best_score=None\n",
    "        self.counter=0\n",
    "        self.early_stop=False\n",
    "    def __call__(self, val_loss, model):\n",
    "        score=-val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score=score\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "        elif score<self.best_score+self.delta:\n",
    "            self.counter+=1\n",
    "            if self.counter>=self.patience:\n",
    "                self.early_stop=True\n",
    "        else:\n",
    "            self.best_score=score\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "            self.counter=0\n",
    "\n",
    "\n",
    "model = AnomalyTransformer(input_dim=7, d_model=64, n_heads=4, num_layers=2, lambd=0.01).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "stopper = EarlyStopper(patience=5, delta=1e-4)\n",
    "num_epochs=30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "    for x_batch, y_batch in pbar:\n",
    "        xb = x_batch.to(device)\n",
    "        yb = y_batch.to(device)\n",
    "        rec_out, cls_logit, As, Bs = model(xb)\n",
    "        loss = model.compute_loss(xb, rec_out, cls_logit, yb, As, Bs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()*len(xb)\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    pbar.close()\n",
    "\n",
    "    final_loss = total_loss/len(train_loader.dataset)\n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}] total_loss={final_loss:.5f}\")\n",
    "    \n",
    "    stopper(final_loss, model)\n",
    "    if stopper.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "    \n",
    "\n",
    "model.load_state_dict(torch.load(\"best_anomaly_transformer.pth\"))\n",
    "print(\"Training done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d019379",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_features_expanded_step(df_in, adjacency, p_node, n=2, window_size=24, step=24):\n",
    "\n",
    "    pre_nodes, post_nodes, in_q_list, out_q_list = find_pre_post_nodes(p_node, adjacency, n)\n",
    "\n",
    "    feats=[]\n",
    "    idxs=[]\n",
    "    ln=len(df_in)\n",
    "    df_cols = set(df_in.columns)\n",
    "\n",
    "    for i in range(0, ln - window_size + 1, step):\n",
    "        chunk = df_in.iloc[i : i+window_size]\n",
    "\n",
    "        # inQ\n",
    "        if len(in_q_list)>0:\n",
    "            q_in_sum = chunk[in_q_list].sum(axis=1).values.reshape(window_size,1)\n",
    "        else:\n",
    "            q_in_sum = np.zeros((window_size,1))\n",
    "\n",
    "        # pre(n)\n",
    "        pre_feats=[]\n",
    "        for pn in pre_nodes[:n]:\n",
    "            if pn in df_cols:\n",
    "                pre_feats.append(chunk[pn].values.reshape(window_size,1))\n",
    "            else:\n",
    "                pre_feats.append(np.zeros((window_size,1)))\n",
    "        if len(pre_feats)<n:\n",
    "            for _ in range(n-len(pre_feats)):\n",
    "                pre_feats.append(np.zeros((window_size,1)))\n",
    "        pre_cat = np.concatenate(pre_feats, axis=1) if len(pre_feats)>0 else np.zeros((window_size,n))\n",
    "\n",
    "        # pnode\n",
    "        if p_node in df_cols:\n",
    "            p_val = chunk[p_node].values.reshape(window_size,1)\n",
    "        else:\n",
    "            p_val = np.zeros((window_size,1))\n",
    "\n",
    "        # post(n)\n",
    "        post_feats=[]\n",
    "        for pn in post_nodes[:n]:\n",
    "            if pn in df_cols:\n",
    "                post_feats.append(chunk[pn].values.reshape(window_size,1))\n",
    "            else:\n",
    "                post_feats.append(np.zeros((window_size,1)))\n",
    "        if len(post_feats)<n:\n",
    "            for _ in range(n-len(post_feats)):\n",
    "                post_feats.append(np.zeros((window_size,1)))\n",
    "        post_cat = np.concatenate(post_feats, axis=1) if len(post_feats)>0 else np.zeros((window_size,n))\n",
    "\n",
    "        # outQ\n",
    "        if len(out_q_list)>0:\n",
    "            q_out_sum = chunk[out_q_list].sum(axis=1).values.reshape(window_size,1)\n",
    "        else:\n",
    "            q_out_sum = np.zeros((window_size,1))\n",
    "\n",
    "        arr = np.concatenate([q_in_sum, pre_cat, p_val, post_cat, q_out_sum], axis=1)\n",
    "        feats.append(arr)\n",
    "        idxs.append(i)\n",
    "    return np.array(feats), idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_test_windows_batch(df_test, adjacency, p_nodes, n=2, window_size=24, step=24):\n",
    "    \n",
    "    df_cols = df_test.columns\n",
    "    X_list = []\n",
    "    meta_list = []  \n",
    "\n",
    "    for p_node in p_nodes:\n",
    "        fx, idxs = make_test_features_expanded_step(\n",
    "            df_test, adjacency, p_node,\n",
    "            n=n, window_size=window_size, step=step\n",
    "        )\n",
    "        # fx.shape => (num_windows_for_p, window_size, 2n+3)\n",
    "\n",
    "        if len(fx) == 0:\n",
    "            continue\n",
    "\n",
    "        # 기록\n",
    "        start_pos = len(X_list)  \n",
    "        X_list.append(fx)\n",
    "        for i_idx in idxs:\n",
    "            meta_list.append( (p_node, i_idx) )\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        return None, None  \n",
    "\n",
    "    # 한 번에 concatenate\n",
    "    X_batch = np.concatenate(X_list, axis=0)  # (N_total, window_size, 2n+3)\n",
    "    return X_batch, meta_list\n",
    "\n",
    "def collect_test_windows_batch(df_test, adjacency, p_nodes, n=2, window_size=24, step=24):\n",
    "    \n",
    "    df_cols = df_test.columns\n",
    "    X_list = []\n",
    "    meta_list = []  \n",
    "\n",
    "    for p_node in p_nodes:\n",
    "        fx, idxs = make_test_features_expanded_step(\n",
    "            df_test, adjacency, p_node,\n",
    "            n=n, window_size=window_size, step=step\n",
    "        )\n",
    "        # fx.shape => (num_windows_for_p, window_size, 2n+3)\n",
    "\n",
    "        if len(fx) == 0:\n",
    "            continue\n",
    "\n",
    "        # 기록\n",
    "        start_pos = len(X_list)  \n",
    "        X_list.append(fx)\n",
    "        for i_idx in idxs:\n",
    "            meta_list.append( (p_node, i_idx) )\n",
    "\n",
    "    if len(X_list) == 0:\n",
    "        return None, None  \n",
    "\n",
    "    # 한 번에 concatenate\n",
    "    X_batch = np.concatenate(X_list, axis=0)  # (N_total, window_size, 2n+3)\n",
    "    return X_batch, meta_list\n",
    "\n",
    "def infer_flags_entire_expanded_batch(\n",
    "    df_test,\n",
    "    adjacency,\n",
    "    p_nodes,\n",
    "    model,\n",
    "    min_vals,\n",
    "    max_vals,\n",
    "    n=2,\n",
    "    window_size=60,\n",
    "    step=60,\n",
    "    threshold=0.5,\n",
    "    device=torch.device(\"cuda\")\n",
    "):\n",
    "\n",
    "    # 스케일링\n",
    "    df_norm = df_test.copy()\n",
    "    for c in df_norm.columns:\n",
    "        if c in min_vals.index:\n",
    "            df_norm[c] = (df_norm[c] - min_vals[c]) / (max_vals[c] - min_vals[c])\n",
    "        else:\n",
    "            df_norm[c] = 0.0\n",
    "\n",
    "    X_batch, meta_list = collect_test_windows_batch(\n",
    "        df_norm, adjacency, p_nodes, n=n, window_size=window_size, step=step\n",
    "    )\n",
    "    result_flags = {pn: 0 for pn in p_nodes}\n",
    "\n",
    "    if X_batch is None:\n",
    "        return result_flags\n",
    "\n",
    "    # batch inference\n",
    "    flags_bool = inference_batch(X_batch, model, threshold=threshold, device=device)\n",
    "    for i, (pn, idx) in enumerate(meta_list):\n",
    "        if flags_bool[i]:\n",
    "            result_flags[pn] = 1  \n",
    "\n",
    "    return result_flags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa7625",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = []\n",
    "test_rows = list(test_info.itertuples(index=False))\n",
    "\n",
    "for row in tqdm(test_rows, desc=\"Test Inference\", leave=True):\n",
    "    file_id = row.ID\n",
    "    file_path = row.path\n",
    "    df_t = pd.read_csv(file_path).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    if \"TEST_C\" in file_id:\n",
    "        # 한 번에 batch\n",
    "        flg_dict = infer_flags_entire_expanded_batch(\n",
    "            df_test=df_t,\n",
    "            adjacency=adjacency_list_C,\n",
    "            p_nodes=nodes_C,\n",
    "            model=model,\n",
    "            min_vals=min_vals,\n",
    "            max_vals=max_vals,\n",
    "            n=2,\n",
    "            window_size=60,\n",
    "            step=60,\n",
    "            threshold=0.5,\n",
    "            device=device\n",
    "        )\n",
    "        flags_ordered = [flg_dict[p] for p in nodes_C]\n",
    "        predicts.append((file_id, flags_ordered))\n",
    "\n",
    "    else:\n",
    "        flg_dict = infer_flags_entire_expanded_batch(\n",
    "            df_test=df_t,\n",
    "            adjacency=adjacency_list_D,\n",
    "            p_nodes=nodes_D,\n",
    "            model=model,\n",
    "            min_vals=min_vals,\n",
    "            max_vals=max_vals,\n",
    "            n=2,\n",
    "            window_size=60,\n",
    "            step=60,\n",
    "            threshold=0.5,\n",
    "            device=device\n",
    "        )\n",
    "        flags_ordered = [flg_dict[p] for p in nodes_D]\n",
    "        predicts.append((file_id, flags_ordered))\n",
    "\n",
    "pdic = {fid: arr for fid, arr in predicts}\n",
    "fl = []\n",
    "for i, row in sample_submission.iterrows():\n",
    "    idx = row['ID']\n",
    "    if idx in pdic:\n",
    "        fl.append(pdic[idx])\n",
    "    else:\n",
    "        fl.append([0])\n",
    "sample_submission['flag_list'] = fl\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
