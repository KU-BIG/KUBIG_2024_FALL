{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"qSASe8--TMmo"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fxuocTPANjQp","outputId":"7e7afef4-7d6b-4b88-ce20-72ebbb4bb31e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","\n","API_KEYS = {\n","    \"UPSTAGE_API_KEY\": None,\n","    \"TAVILY_API_KEY\" : None\n","}\n","\n","def load_env():\n","    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n","    if \"google.colab\" in str(get_ipython()):  # Google Colab í™˜ê²½\n","        os.environ['UPSTAGE_API_KEY'] = ''\n","        API_KEYS[\"UPSTAGE_API_KEY\"] = os.environ.get(\"UPSTAGE_API_KEY\")\n","        os.environ['TAVILY_API_KEY'] = ''\n","        API_KEYS[\"TAVILY_API_KEY\"] = os.environ.get(\"TAVILY_API_KEY\")\n","    else:  # ë¡œì»¬ í™˜ê²½\n","        load_dotenv()  # .env íŒŒì¼ ë¡œë“œ\n","        API_KEYS[\"UPSTAGE_API_KEY\"] = os.environ.get(\"UPSTAGE_API_KEY\")\n","        API_KEYS[\"TAVILY_API_KEY\"] = os.environ.get(\"TAVILY_API_KEY\")\n","\n","    return API_KEYS[\"UPSTAGE_API_KEY\"], API_KEYS[\"TAVILY_API_KEY\"]\n","\n","UPSTAGE_API_KEY, TAVILY_API_KEY= load_env()"],"metadata":{"id":"lFsFlQevHpYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q streamlit\n","!npm install localtunnel\n","!pip install --upgrade -q accelerate bitsandbytes\n","!pip install git+https://github.com/huggingface/transformers.git\n","!pip install sentence_transformers\n","!pip install streamlit-folium\n","!npm audit fix\n","\n","!pip install -U langchain_community tiktoken langchainhub langchain langgraph chromadb\n","!pip install tavily-python\n","!pip install -qU langchain-core langchain-upstage langchain-chroma\n","!pip install -qU python-dotenv\n","!pip install -U langchain-upstage\n","!pip install -U openai\n","\n","!pip install streamlit_chat\n","!pip install qdrant_client"],"metadata":{"id":"GTPgrgx0H-ks"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# config.py"],"metadata":{"id":"Si-vwNROFtpE"}},{"cell_type":"code","source":["%%writefile config.py\n","\n","from langchain.docstore.document import Document\n","from langchain_community.retrievers import TavilySearchAPIRetriever\n","from langchain import hub\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_upstage import UpstageEmbeddings, ChatUpstage\n","from langchain.vectorstores import Chroma\n","from langgraph.graph import END, StateGraph, START\n","from langchain_core.pydantic_v1 import BaseModel, Field\n","from dotenv import load_dotenv\n","from IPython import get_ipython\n","import numpy as np\n","import os\n","import pandas as pd\n","from pprint import pprint\n","import re\n","from typing import List, Dict\n","from typing_extensions import TypedDict\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","from openai import OpenAI\n","import json\n","\n","\n","client =OpenAI(api_key='')\n","\n","\n","'''\n","Embedding and Database\n","'''\n","\n","# ê¸°ì¡´ ì„¤ì •\n","embedding_function = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n","\n","# ë‘ ê°œì˜ Chroma ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n","persist_directory = '/content/drive/MyDrive/policy_chatbot/policy_combined'\n","db = Chroma(embedding_function=embedding_function, persist_directory=persist_directory)\n","retriever = db.as_retriever(search_kwargs={\"k\": 3})\n","\n","\n","'''\n","Retrieval Grader\n","'''\n","\n","# Data model\n","class GradeDocuments(BaseModel):\n","    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n","    # ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ì— ì í•©í•œì§€ë¥¼ 'yes' ë˜ëŠ” 'no'ë¡œ ì €ì¥\n","    binary_score: str = Field(\n","        description=\"Documents are relevant to the conversation history and question, 'yes' or 'no'\"\n","    )\n","\n","# LLM with function call\n","llm_1 = ChatUpstage(max_tokens=200, temperature=0) # Parameter tuning 1\n","# ëª¨ë¸ì´ êµ¬ì¡°í™”ëœ ë°ì´í„°(GradeDocuments)ë¥¼ ë°˜í™˜í•˜ë„ë¡ ì„¤ì •\n","structured_llm_grader = llm_1.with_structured_output(GradeDocuments)\n","\n","# System Prompt: ê²€ìƒ‰í•œ ë¬¸ì„œì— ìœ ì € ì§ˆë¬¸ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ê±°ë‚˜ ì˜ë¯¸ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆìœ¼ë©´, 'relevant'ë¡œ íŒë‹¨. ê²°ê³¼ëŠ” 'yes' ë˜ëŠ” 'no'ë¡œ ì´ì§„ ì ìˆ˜ë¥¼ ì¤Œ.\n","system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n","    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n","    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n","    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n","    'Yes' means that the document is relevant to the question.\"\"\"\n","\n","\n","# grade_prompt = System prompt + ê²€ìƒ‰ëœ ë¬¸ì„œ + ì‚¬ìš©ì ì§ˆë¬¸\n","grade_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system),\n","        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n","    ]\n",")\n","# grade_promptì™€ structured_llm_graderì„ ê²°í•©í•˜ì—¬ ë¬¸ì„œ í‰ê°€ë¥¼ ìœ„í•œ ê°ì²´ ìƒì„±\n","retrieval_grader = grade_prompt | structured_llm_grader\n","\n","# Wrapper for consistent output\n","def grade_retrieval(doc_txt, user_question):\n","    \"\"\"\n","    Grades the relevance of a document to the combined question.\n","\n","    Args:\n","        doc_txt (str): The text of the document to grade.\n","        combined_question (str): The combined question including chat history.\n","\n","    Returns:\n","        str: 'yes' or 'no' based on relevance grading.\n","    \"\"\"\n","    try:\n","        # Invoke grader\n","        result = retrieval_grader.invoke({\"document\": doc_txt, \"question\": user_question})\n","        print(f\"Raw result from grader: {result}\")\n","\n","        if result and hasattr(result, \"binary_score\"):\n","            return result.binary_score\n","        else:\n","            print(f\"Invalid result structure: {result}\")\n","    except Exception as e:\n","        print(f\"Error during retrieval grading: {e}\")\n","\n","    # Default to 'no' if something goes wrong\n","    print('None')\n","    return \"no\"\n","\n","\n","'''\n","Generate\n","'''\n","\n","generate_system = \"\"\"\n","    ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ë¹„ì„œì…ë‹ˆë‹¤. ê²€ìƒ‰ëœ contextë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ë§Œì•½ ë‹µì„ ëª¨ë¥¼ ê²½ìš°, ëª¨ë¥¸ë‹¤ê³  ì†”ì§íˆ ë§í•˜ì„¸ìš”. \\n\n","    ì²­ë…„ ì—°ë ¹, ì§ì—…, ì£¼ê±° ìƒí™©, ì •ì±… ê²½í—˜ ì—¬ë¶€ ë“± ë‹¤ì–‘í•œ ìƒí™©ì„ ê³ ë ¤í•˜ì—¬ ë§ì¶¤í˜• ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\n","\n","    ì§ˆë¬¸ì˜ ìœ í˜•ì— ë”°ë¼ ì•„ë˜ ì§€ì¹¨ì„ ì„ íƒí•˜ê³ , ì´ ì§€ì¹¨ì„ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”: \\n\n","    1.** ì •ì±… ì¶”ì²œ ì§ˆë¬¸ì— ëŒ€í•œ ì§€ì¹¨ **\n","    - ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:\n","      â‘  ì •ì±…ëª…:\n","      â‘¡ ì¶”ì²œ ì´ìœ : ì§ˆë¬¸ì—ì„œ ì œê³µëœ ì—°ë ¹, ìƒí™©, ì„ í˜¸ë„ ë“±ì„ ë°”íƒ•ìœ¼ë¡œ ì •ì±…ì´ ì™œ ì í•©í•œì§€ ì„¤ëª…í•˜ì„¸ìš”.\n","      â‘¢ ì œê³µ í˜œíƒ: ì •ì±…ì„ í†µí•´ ì–»ì„ ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì´ì ì„ ì„œìˆ í•˜ì„¸ìš”.\n","    - ì •ì±… ì¶”ì²œ ì‹œ, ìµœì†Œ 2ê°œ ì´ìƒì˜ ì •ì±…ì€ ì¶”ì²œí•´ì•¼ í•˜ê³ , ì•„ë˜ ë°ì´í„°ë¥¼ ì ê·¹ í™œìš©í•˜ì—¬ ê°€ì¥ ì¶”ì²œì´ ìš°ì„ ì‹œ ë˜ëŠ” ì •ì±…ë¶€í„° ë‚˜ì—´í•´ì£¼ì„¸ìš”:\n","        * **ë³µì§€ ë° ë¬¸í™” ê´€ë ¨ ì´ìŠˆ**\n","          - ëŒ€í•™ìƒ: ì§„ë¡œ ë¶ˆí™•ì‹¤ì„±, ì¡¸ì—…ìœ ì˜ˆ ì¦ê°€, ë“±ë¡ê¸ˆ ë¶€ë‹´.\n","          - ì§ì¥ì¸: ì €ì„ê¸ˆ, ê³ ìš©ë¶ˆì•ˆ(1-2ë…„) ìš°ë ¤.\n","          - ë¯¸ì·¨ì—…ì: ì‹¬ë¦¬ìƒë‹´ ì§€ì›, ëŒ€ì¤‘êµí†µë¹„ ì§€ì› ì„ í˜¸.\n","          - ì „ë°˜ì ìœ¼ë¡œ ì²­ë…„ë“¤ì€ ì•ˆì •ì ì¸ ìë¦½ ì§€ì›ê³¼ ê¸ˆì „ì  ì§€ì›ì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ìŒ.\n","          - ëŒ€ì¤‘êµí†µë¹„ ì§€ì›, ì‹¬ë¦¬ìƒë‹´ ì§€ì›, ë¬¸í™”í™œë™ ë° ì—¬í–‰ ì¥ë ¤ê¸ˆ ì§€ì› ë“±ì´ ì£¼ìš” ì„ í˜¸ ì •ì±…ìœ¼ë¡œ ë‚˜íƒ€ë‚¨.\n","    - ë†’ì„ë§ ëŒ€ì‹ , \"~ìš”\"ë¡œ ëë‚˜ëŠ” ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì˜ˆ: \"ì•Œë ¤ë“œë¦´ê²Œìš”\", \"ë„ì›€ì´ ë˜ì…¨ìœ¼ë©´ í•´ìš”\"\n","    - ë‹µë³€ì˜ ì‹œì‘ì€ ë°˜ë“œì‹œ \"ì„¸ì€ë‹˜ì—ê²Œ ë§ëŠ” ì •ì±…ì„ ì°¾ì•„ë³´ì•˜ì–´ìš”ğŸ˜Š\\n\"ë¡œ ì‹œì‘í•˜ì„¸ìš”.\n","    - ë‹µë³€ì˜ ëì€ ë°˜ë“œì‹œ \"\\nì •ì±…ì— ëŒ€í•´ ë” êµ¬ì²´ì ìœ¼ë¡œ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?\"ë¡œ ëë‚´ì„¸ìš”.\n","\n","    2. ** ì •ì±… ì„¸ë¶€ ì •ë³´ ìš”ì²­ ì§ˆë¬¸ì— ëŒ€í•œ ì§€ì¹¨ **\n","    - ë‹µë³€ì˜ ì‹œì‘ì—ëŠ” ë°˜ë“œì‹œ ì‚¬ìš©ìê°€ ì–´ë–¤ ì •ì±…ì˜ ì–´ë–¤ ë‚´ìš©ì„ ê¶ê¸ˆí•´í•˜ëŠ”ì§€ ìš”ì•½í•˜ê³ , \"ì œê°€ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ë“œë¦´ê²Œìš”ğŸ˜„\\n\" ë¬¸ì¥ì„ ì¶”ê°€í•˜ì„¸ìš”.\n","    - ë†’ì„ë§ ëŒ€ì‹ , \"~ìš”\"ë¡œ ëë‚˜ëŠ” ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì˜ˆ: \"ì•Œë ¤ë“œë¦´ê²Œìš”\", \"ë„ì›€ì´ ë˜ì…¨ìœ¼ë©´ í•´ìš”\"\n","    - ë‹µë³€ì€ ë‚˜ì—´ì‹ì´ ì•„ë‹Œ ì¤„ê¸€ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n","    - ë‹µë³€ì˜ ëì—ëŠ” ë°˜ë“œì‹œ \"\\nê¶ê¸ˆì¦ì´ í•´ê²°ë˜ì—ˆë‚˜ìš”?\" ë¬¸ì¥ì„ ì¶”ê°€í•˜ì„¸ìš”.\n","    - **ì˜ˆì‹œ ë‹µë³€ í˜•ì‹**:\n","    > \"ê²½ê¸°ë„ ë©´ì ‘ìˆ˜ë‹¹ ì •ì±…ì˜ ì‹ ì²­ ê¸°ê°„ì´ ê¶ê¸ˆí•˜ì…¨êµ°ìš”. ì œê°€ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ë“œë¦´ê²Œìš”ğŸ˜„\\n ì‹ ì²­ ê¸°ê°„ì€ 2024ë…„ 5ì›” 2ì¼ë¶€í„° 5ì›” 30ì¼ê¹Œì§€ì…ë‹ˆë‹¤. \\nê¶ê¸ˆì¦ì´ í•´ê²°ë˜ì—ˆë‚˜ìš”?\"\n","\n","    3. ** ì •ì±… ìš©ì–´ ì„¤ëª… ìš”ì²­ ì§ˆë¬¸ì— ëŒ€í•œ ì§€ì¹¨ **\n","    - ë‹µë³€ì˜ ì‹œì‘ì—ëŠ” ë°˜ë“œì‹œ ì‚¬ìš©ìê°€ ì–´ë–¤ ë‹¨ì–´ì˜ ëœ»ì„ ê¶ê¸ˆí•´í•˜ëŠ”ì§€ ìš”ì•½í•˜ê³ , \"ì œê°€ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ë“œë¦´ê²Œìš”ğŸ˜„\\n\" ë¬¸ì¥ì„ ì¶”ê°€í•˜ì„¸ìš”.\n","    - ë†’ì„ë§ ëŒ€ì‹ , \"~ìš”\"ë¡œ ëë‚˜ëŠ” ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì˜ˆ: \"ì•Œë ¤ë“œë¦´ê²Œìš”\", \"ë„ì›€ì´ ë˜ì…¨ìœ¼ë©´ í•´ìš”\"\n","    - ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì€ ë‚˜ì—´ì‹ì´ ì•„ë‹Œ ì¤„ê¸€ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ê°„ê²°í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.\n","    - ë‹µë³€ì˜ ëì—ëŠ” ë°˜ë“œì‹œ \"\\nê¶ê¸ˆì¦ì´ í•´ê²°ë˜ì—ˆë‚˜ìš”?\" ë¬¸ì¥ì„ ì¶”ê°€í•˜ì„¸ìš”.\n","    - **ì˜ˆì‹œ ë‹µë³€ í˜•ì‹**:\n","    > \"ê°€ë“±ê¸°ê°€ ë¬´ìŠ¨ ëœ»ì¸ì§€ ê¶ê¸ˆí•˜ì…¨êµ°ìš”. ì œê°€ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ë“œë¦´ê²Œìš”ğŸ˜„\\n ê°€ë“±ê¸°ë€, ë“±ê¸°ë¡œ í‘œì‹œëœ ë¶€ë™ì‚°ì˜ ê¶Œë¦¬ê´€ê³„ê°€ ì™¸ë¶€ì— í‘œì‹œë˜ê¸° ë•Œë¬¸ì— ì œ3ìì—ê²Œë„ ëŒ€í•­í•  ìˆ˜ ìˆëŠ”ë°, ê¶Œë¦¬ê´€ê³„ê°€ í™•ì •ë˜ì§€ ì•ŠëŠ” ë“±ìœ¼ë¡œ ë“±ê¸°ë¥¼ í•  ìˆ˜ ì—†ì„ ê²½ìš°ì— ì„ì‹œë¡œ í•˜ëŠ” ë“±ê¸°ë¥¼ ëœ»í•©ë‹ˆë‹¤. ê¶ê¸ˆì¦ì´ í•´ê²°ë˜ì—ˆë‚˜ìš”?\"\n","\n","    4. ** ì •ì±… í›„ê¸° ìš”ì²­ ì§ˆë¬¸ì— ëŒ€í•œ ì§€ì¹¨ **\n","    - ë‹µë³€ì˜ ì‹œì‘ì—ëŠ” ë°˜ë“œì‹œ ì‚¬ìš©ìê°€ ì–´ë–¤ ì •ì±…ì˜ í›„ê¸°ë¥¼ ê¶ê¸ˆí•´í•˜ëŠ”ì§€ ìš”ì•½í•˜ê³ , \"ì œê°€ ê²€ìƒ‰í•´ë³´ì•˜ì–´ìš”ğŸ”\\n\" ë¬¸ì¥ì„ ì¶”ê°€í•˜ì„¸ìš”.\n","    - ë†’ì„ë§ ëŒ€ì‹ , \"~ìš”\"ë¡œ ëë‚˜ëŠ” ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ì˜ˆ: \"ì•Œë ¤ë“œë¦´ê²Œìš”\", \"ë„ì›€ì´ ë˜ì…¨ìœ¼ë©´ í•´ìš”\"\n","    - ë¬¸ì„œì—ëŠ” í›„ê¸°ê°€ ì—†ìœ¼ë‹ˆ ì˜ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ê³ , ì™¸ë¶€ ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì´ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n","    - ë‹µë³€ì˜ ëì—ëŠ” ë°˜ë“œì‹œ \"\\nê¶ê¸ˆì¦ì´ í•´ê²°ë˜ì—ˆë‚˜ìš”?\" ë¬¸ì¥ì„ ì¶”ê°€í•˜ì„¸ìš”.\n","    - **ì˜ˆì‹œ ë‹µë³€ í˜•ì‹**:\n","    > \"ê²½ê¸°ë„ ë©´ì ‘ìˆ˜ë‹¹ ì •ì±…ì˜ ì‹¤ì œ ë¸”ë¡œê·¸ í›„ê¸°ê°€ ê¶ê¸ˆí•˜ì…¨êµ°ìš”. ì œê°€ ê²€ìƒ‰í•´ë³´ì•˜ì–´ìš”ğŸ”\\n[í›„ê¸° ìš”ì•½]\\nê¶ê¸ˆì¦ì´ í•´ê²°ë˜ì—ˆë‚˜ìš”?\"\n","    \"\"\"\n","\n","TOKEN_PER_CHAR = 4\n","\n","def estimate_tokens(text):\n","    return len(text) // TOKEN_PER_CHAR\n","\n","# Function to format documents without exceeding the token limit\n","def format_docs(docs, max_tokens=110000):\n","    formatted_docs = []\n","    current_chunk = \"\"\n","    current_token_count = 0\n","\n","    for doc in docs:\n","        doc_content = doc.page_content\n","        doc_token_count = estimate_tokens(doc_content)\n","\n","        # If adding this document exceeds the token limit, start a new chunk\n","        if current_token_count + doc_token_count > max_tokens:\n","            formatted_docs.append(current_chunk)\n","            current_chunk = doc_content  # Start a new chunk\n","            current_token_count = doc_token_count\n","        else:\n","            current_chunk += \"\\n\\n\" + doc_content  # Append to the current chunk\n","            current_token_count += doc_token_count\n","\n","    # Add the last chunk if there's any content left\n","    if current_chunk:\n","        formatted_docs.append(current_chunk)\n","\n","    return formatted_docs\n","\n","# ëª¨ë¸ í˜¸ì¶œ ì²´ì¸\n","def rag_chain(question, context):\n","    response = client.chat.completions.create(\n","        model=\"gpt-4o-mini\",\n","        messages=[\n","            {\"role\": \"system\",\n","             \"content\": generate_system},\n","            {\"role\": \"user\",\n","             \"content\": f\"Question: {question}\\n\\nContext: {context}\\nAnswer the question.\"},\n","        ],\n","        max_tokens=400,\n","        temperature=0,\n","    )\n","    return response.choices[0].message.content\n","\n","'''\n","Answer Grader\n","'''\n","\n","# Data model\n","class GradeAnswer(BaseModel):\n","    \"\"\"Binary score to assess answer addresses question.\"\"\"\n","\n","    binary_score: str = Field(\n","        description=\"Answer addresses the question, 'yes' or 'no'\"\n","    )\n","    confidence: float = Field(\n","        description=\"Confidence score for the assessment, range 0 to 1\"\n","    )\n","\n","# Revised System Prompt\n","answer_system_strict = \"\"\"ë‹¹ì‹ ì€ ì‘ë‹µì´ ì§ˆë¬¸ì— ì ì ˆí•œì§€ ì—„ê²©íˆ í‰ê°€í•˜ëŠ” ì±„ì ìì…ë‹ˆë‹¤.\n","    1. ì‘ë‹µì´ ë¬¸ì„œì˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëª…í™•íˆ ë‹µí–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n","    2. ë¬¸ì„œ ë‚´ìš©ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ê±°ë‚˜ ì‘ë‹µì´ ë¬¸ì„œë¥¼ ì •í™•íˆ ë°˜ì˜í•˜ì§€ ì•Šìœ¼ë©´ 'ì•„ë‹ˆì˜¤'ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.\n","    3. ì‘ë‹µì´ ì‹ ë¢°í•  ìˆ˜ ì—†ëŠ” ë‚´ìš©ì„ í¬í•¨í•˜ê±°ë‚˜ ë¶ˆí™•ì‹¤í•˜ë©´ 'ì•„ë‹ˆì˜¤'ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.\n","    4. 'ì˜ˆ'ë¡œ í‰ê°€í•˜ë ¤ë©´ ë¬¸ì„œ ë‚´ìš©ì„ ì¶©ë¶„íˆ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì„ ì§ì ‘ í•´ê²°í•´ì•¼ í•©ë‹ˆë‹¤.\n","    5. ì •ì±…ì˜ í›„ê¸°ë¥¼ ë¬»ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ 'í›„ê¸°ëŠ” snsë‚˜ ë¸”ë¡œê·¸ë¥¼ í†µí•´ ì°¾ì„ ìˆ˜ ìˆì–´ìš”' ì™€ ê°™ì´ ìì„¸í•˜ì§€ ì•Šì€ ë‚´ìš©ì„ í¬í•¨í•œ ê²½ìš° 'ì•„ë‹ˆì˜¤'ë¥¼ ë°˜í™˜í•˜ì„¸ìš”.\n","\n","    ì‘ë‹µì„ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:\n","{\n","    \"binary_score\": \"yes\" ë˜ëŠ” \"no\",\n","    \"confidence\": 0ì—ì„œ 1 ì‚¬ì´ì˜ ë¶€ë™ì†Œìˆ˜ì  ê°’\n","}\n","\"\"\"\n","\n","def answer_grader(question, generation, document):\n","    response = client.chat.completions.create(\n","        model=\"gpt-4o-mini\",\n","        messages=[\n","            {\"role\": \"system\",\n","             \"content\": answer_system_strict},\n","            {\"role\": \"user\",\n","             \"content\": f\"User question: {question}\\n\\nLLM generation: {generation}\\n\\nContext: {document}\\nEvaluate the generated answer strictly based on the context.\"},\n","        ],\n","        max_tokens=400,\n","        temperature=0,\n","    )\n","    print(\"Raw response:\", response.choices[0].message.content)\n","    try:\n","        response_data = json.loads(response.choices[0].message.content)\n","        binary_score = response_data.get(\"binary_score\", \"no\")  # \"binary_score\"ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ \"no\" ë°˜í™˜\n","    except json.JSONDecodeError as e:\n","        print(f\"Error parsing JSON: {e}\")\n","        binary_score = \"no\"  # íŒŒì‹± ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ê¸°ë³¸ê°’ \"no\" ë°˜í™˜\n","\n","    return binary_score\n","\n","\n","'''\n","Question Rewriter\n","'''\n","\n","# LLM\n","llm_5 = ChatUpstage(max_tokens=200, temperature=0)\n","\n","question_rewriter_system = \"\"\"\n","    ë‹¹ì‹ ì€ ì…ë ¥ëœ ì§ˆë¬¸ì„ ë²¡í„° ì €ì¥ì†Œ ê²€ìƒ‰ì— ìµœì í™”ëœ ë” ë‚˜ì€ ë²„ì „ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì§ˆë¬¸ ì¬ì‘ì„±ìì…ë‹ˆë‹¤. ì…ë ¥ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ê°„ë‹¨í•˜ê³  ëª…ë£Œí•œ ì§ˆë¬¸ìœ¼ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”. ë¬¸ì¥ ê°„ ë‚´ìš©ì´ ë‹¤ë¥¸ ë²”ì£¼ë¼ ëŠê»´ì§€ë©´ ê³¼ê°íˆ ì• ë‚´ìš©ì€ ë²„ë¦¬ì„¸ìš”.\n","    \"\"\"\n","\n","re_write_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", question_rewriter_system),\n","        (\n","            \"human\",\n","           \" {question} \\n Formulate an improved question.\",\n","        ),\n","    ]\n",")\n","\n","question_rewriter = re_write_prompt | llm_5 | StrOutputParser()\n","\n","'''\n","Structure\n","'''\n","\n","class GraphState(TypedDict):\n","    \"\"\"\n","    Represents the state of our graph.\n","\n","    Attributes:\n","        question: Combined question including chat history and user query.\n","        generation: LLM generation\n","        documents: list of documents\n","    \"\"\"\n","\n","    question: str\n","    generation: str\n","    documents: List[str]\n","    first_generate_attempt: bool\n","\n","\n","'''\n","Function\n","'''\n","\n","### Nodes\n","\n","def check_documents(state):\n","    \"\"\"\n","    Check if documents exist in the current state and update the state accordingly.\n","\n","    Args:\n","        state (dict): The current graph state.\n","\n","    Returns:\n","        dict: Updated state with a flag indicating the next step.\n","    \"\"\"\n","\n","    if state.get(\"documents\"):\n","        print(\"Documents found, marking for 'generate_first'.\")\n","        state[\"next_node\"] = \"generate_first\"  # Add a marker for the next step\n","    else:\n","        print(\"No documents found, marking for 'retrieve'.\")\n","        state[\"next_node\"] = \"retrieve\"  # Add a marker for the next step\n","\n","    return state\n","\n","\n","def retrieve(state):\n","    \"\"\"\n","    Retrieve documents and update the current state.\n","\n","    Args:\n","        state (dict): The current graph state.\n","\n","    Returns:\n","        dict: Updated state with retrieved documents.\n","    \"\"\"\n","    print(\"---RETRIEVE---\")\n","    question = state[\"question\"]\n","\n","    # Process the question\n","    if re.search(r'\\n\\s*user', question):\n","        lines = question.split(\"\\nuser\")\n","        question_to_retrieve = \"\\nuser\".join(lines[:-1])\n","    else:\n","        question_to_retrieve = question\n","\n","    # Retrieve documents\n","    documents = retriever.get_relevant_documents(question_to_retrieve)\n","    print(\"Retrieved documents:\", documents)\n","\n","    # Update state\n","    state[\"documents\"] = documents\n","    return state\n","\n","\n","def generate(state):\n","    \"\"\"\n","    Generate answer based on the current state.\n","\n","    Args:\n","        state (dict): The current graph state containing question and documents.\n","\n","    Returns:\n","        dict: Updated state with the generated answer added under the key 'generation'.\n","    \"\"\"\n","    print(\"---GENERATE---\")\n","\n","    # Extract question and documents from state\n","    question = state.get(\"question\")\n","    documents = state.get(\"documents\")\n","\n","    if not question or not documents:\n","        raise ValueError(\"State must include both 'question' and 'documents' keys with valid data.\")\n","\n","    # Format the documents into a single context string\n","    context = format_docs(documents)\n","\n","    # Invoke RAG chain to get the generation\n","    generation = rag_chain(question, context)\n","\n","    # Return updated state with the generation\n","    updated_state = {\n","        **state,\n","        \"generation\": generation,\n","    }\n","\n","    return updated_state\n","\n","\n","def extract_last_question(combined_question):\n","    \"\"\"\n","    Extracts the last user question from the combined question.\n","\n","    Args:\n","        combined_question (str): The combined question including chat history.\n","\n","    Returns:\n","        str: The last user question.\n","    \"\"\"\n","    lines = combined_question.split(\"\\n\")\n","    for line in reversed(lines):\n","        if line.startswith(\"user:\"):\n","            return line[len(\"user: \"):].strip()  # Remove \"user: \" prefix\n","    return \"\"  # Default to empty if no user question found\n","\n","\n","\n","def grade_documents(state):\n","    \"\"\"\n","    Determines whether the retrieved documents are relevant to the question.\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        state (dict): Updates documents key with only filtered relevant documents\n","    \"\"\"\n","\n","    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n","    question = state[\"question\"]\n","    documents = state[\"documents\"]\n","\n","    # Extract the last question\n","    last_question = extract_last_question(question)\n","    print(f\"Last User Question: {last_question}\")  # Debugging: Check extracted question\n","\n","    # Score each doc\n","    filtered_docs = []\n","    all_irrelevant = True\n","    for d in documents:\n","        grade = grade_retrieval(d, last_question)\n","        if grade == \"yes\":\n","            print(\"---GRADE: DOCUMENT RELEVANT---\")\n","            filtered_docs.append(d)\n","            all_irrelevant = False\n","              # At least one document is relevant\n","        else:\n","            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n","            continue\n","\n","    # Update state\n","    state[\"documents\"] = filtered_docs\n","\n","    # Add a flag to indicate if all documents were irrelevant\n","    state[\"all_documents_irrelevant\"] = all_irrelevant\n","\n","    return state\n","\n","\n","def transform_query(state):\n","    \"\"\"\n","    Transform the query to produce a better question.\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        state (dict): Updates question key with a re-phrased question\n","    \"\"\"\n","\n","    print(\"---TRANSFORM QUERY---\")\n","    question = state[\"question\"]\n","    documents = state[\"documents\"]\n","\n","    # Re-write question\n","    better_question = question_rewriter.invoke({\"question\": question})\n","    return {\"documents\": documents, \"question\": better_question}\n","\n","\n","### Edges\n","\n","\n","def decide_to_generate(state):\n","    \"\"\"\n","    Determines whether to generate an answer, or re-generate a question.\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        str: Binary decision for next node to call\n","    \"\"\"\n","\n","    print(\"---ASSESS GRADED DOCUMENTS---\")\n","\n","    # Extract the last user question directly from the state\n","    last_question = extract_last_question(state[\"question\"])\n","    print(f\"Last User Question: {last_question}\")  # Debugging\n","\n","    # If all documents are irrelevant, transform the query\n","    if state.get(\"all_documents_irrelevant\", False):  # Check if irrelevant is True\n","        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INIT QUERY---\")\n","        return \"init_query\"\n","\n","    # If documents are relevant, generate a response\n","    else:\n","        print(\"---DECISION: GENERATE---\")\n","        return \"transform_query\"\n","\n","\n","# Initialize the not useful counter\n","not_useful_count = 0\n","first_generate_count=0\n","\n","def grade_generation_v_documents_and_question(state):\n","    \"\"\"\n","    Determines whether the generation answers the question.\n","    Returns:\n","        str: 'useful', 'not useful', or 'fallback'.\n","    \"\"\"\n","    global not_useful_count\n","    global first_generate_count\n","\n","    print(\"---GRADE GENERATION vs QUESTION---\")\n","    question = state[\"question\"]\n","    documents = state[\"documents\"]\n","    generation = state[\"generation\"]\n","\n","    # Extract the last question for grading\n","    last_question = extract_last_question(question)\n","\n","    try:\n","        # Perform grading based on whether it's the first attempt\n","        if first_generate_count==0 :\n","            grade = answer_grader(last_question, generation, documents)\n","        else:\n","            grade = answer_grader(question, generation, documents)\n","    except Exception as e:\n","        print(f\"Error during answer grading: {e}\")\n","        grade = \"no\"\n","    if first_generate_count== 0 :\n","        if grade == \"yes\":\n","            print(\"---FIRST GENERATE ATTEMPT: USEFUL\")\n","            return \"useful\"\n","        else:\n","            print(\"---FIRST GENERATE ATTEMPT: NOT USEFUL\")\n","            first_generate_count += 1\n","            return \"not useful\"\n","    else:\n","        if grade == \"yes\":\n","            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n","            not_useful_count = 0\n","            first_generate_count=0  # Reset the counter\n","            return \"useful\"\n","        else:\n","            print(f\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n","            not_useful_count += 1\n","            print(f\"---NOT USEFUL COUNT INCREMENTED: count={not_useful_count}---\")\n","            if not_useful_count == 1:\n","                return \"not useful\"\n","            else:\n","                return \"fallback\"\n","\n","\n","def init_query(state):\n","    \"\"\"\n","    Removes chat/document history, leaves only the last user question,\n","    and resets not_useful_count to 0.\n","    \"\"\"\n","    print(\"---INIT QUERY (CLEAR HISTORY)---\")\n","    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ì§ˆë¬¸ë§Œ ë½‘ì•„ì˜¤ê¸°\n","    last_question = extract_last_question(state[\"question\"])\n","\n","    # state ì´ˆê¸°í™”\n","    state[\"question\"] = f\"user: {last_question}\"\n","    state[\"documents\"] = []\n","    state[\"generation\"] = \"\"\n","\n","    print(f\"[init_query] Updated question = {state['question']}\")\n","    return state\n","\n","\n","'''\n","Tavily\n","'''\n","\n","def tavily_search(state):\n","    \"\"\"\n","    External search using Tavily and process the results into a structured markdown report.\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        dict: Updated state with processed documents and structured report\n","    \"\"\"\n","    print(\"---TAVILY SEARCH---\")\n","    question = state[\"question\"]\n","\n","    # Perform Tavily search\n","    tavily = TavilySearchAPIRetriever(k=3, search_depth=\"advanced\", include_domains=[\"m.blog.naver.com\"])\n","    print(\"Searching using Tavily...\")\n","\n","    docs_external = tavily.invoke(question)  # Tavily ê²€ìƒ‰ í˜¸ì¶œ\n","    print(f\"Search results: {len(docs_external)} documents found.\")\n","\n","\n","    if docs_external:\n","        print(\"Relevant Tavily search results found.\")\n","        return {\"documents\": docs_external, \"question\": question}\n","    else:\n","        print(\"No relevant documents found after filtering.\")\n","        return {\"documents\": [], \"question\": question}\n","\n","'''\n","Workflow & App\n","'''\n","\n","# Define the workflow\n","workflow = StateGraph(GraphState)\n","\n","# Define the nodes\n","workflow.add_node(\"retrieve\", retrieve)  # Retrieve documents\n","workflow.add_node(\"grade_documents\", grade_documents)\n","workflow.add_node(\"retrieve_no_grade\", retrieve)  # Re-retrieve without grading\n","workflow.add_node(\"generate_first\", generate)  # First generate\n","workflow.add_node(\"generate_second\", generate)\n","workflow.add_node(\"init_query\", init_query)  # Initialize query\n","workflow.add_node(\"transform_query1\", transform_query)\n","workflow.add_node(\"transform_query2\", transform_query)  # Transform query\n","workflow.add_node(\"tavily_search\", tavily_search)  # External search node\n","workflow.add_node(\"check_documents\", check_documents)  # Custom node to check documents\n","\n","\n","# Add edges for document check\n","workflow.add_edge(START, \"check_documents\")\n","\n","workflow.add_conditional_edges(\n","    \"check_documents\",\n","    lambda state: state.get(\"next_node\"),  # Use `next_node` to determine the flow\n","    {\n","        \"generate_first\": \"generate_first\",\n","        \"retrieve\": \"retrieve\"\n","    },\n",")\n","\n","# First generate node\n","workflow.add_conditional_edges(\n","    \"generate_first\",\n","    grade_generation_v_documents_and_question,\n","    {\n","        \"useful\": \"transform_query1\",  # If generation is useful, end\n","        \"not useful\": \"retrieve\"\n","    },\n",")\n","\n","workflow.add_edge(\"transform_query1\", END)\n","\n","\n","# Define the main edges\n","workflow.add_edge(\"retrieve\", \"grade_documents\")\n","workflow.add_conditional_edges(\n","    \"grade_documents\",\n","    decide_to_generate,\n","    {\n","        \"init_query\": \"init_query\",\n","        \"transform_query\": \"transform_query2\"\n","    },\n",")\n","\n","workflow.add_edge(\"init_query\", \"retrieve_no_grade\")\n","workflow.add_edge(\"retrieve_no_grade\", \"transform_query2\")\n","workflow.add_edge(\"transform_query2\", \"generate_second\")\n","\n","# Second generate node\n","workflow.add_conditional_edges(\n","    \"generate_second\",\n","    grade_generation_v_documents_and_question,\n","    {\n","        \"useful\": END,  # If generation is useful, end\n","        \"not useful\": \"generate_second\",  # Retry with transformed query\n","        \"fallback\": \"tavily_search\"  # Retry generation with external search\n","    },\n",")\n","\n","workflow.add_edge(\"tavily_search\", \"generate_second\")\n","\n","# Compile the workflow\n","app = workflow.compile()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5InZHE9w7ONu","outputId":"89c3a2a0-1c3f-4f98-a041-50db5118a7cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing config.py\n"]}]},{"cell_type":"markdown","source":["# app.py"],"metadata":{"id":"pi7EViB6Fx2Q"}},{"cell_type":"code","source":["%%writefile app.py\n","import streamlit as st\n","from pprint import pprint\n","import os\n","import re\n","import time\n","from pprint import pprint\n","from langchain.chains import ConversationalRetrievalChain\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.memory import ConversationSummaryBufferMemory\n","from langchain.vectorstores import Qdrant\n","from streamlit_chat import message\n","from qdrant_client import QdrantClient\n","from openai import OpenAI\n","from config import (\n","    retriever,\n","    GradeDocuments,\n","    format_docs,\n","    rag_chain,\n","    GradeAnswer,\n","    GraphState,\n","    retrieve,\n","    generate,\n","    extract_last_question,\n","    grade_documents,\n","    transform_query,\n","    decide_to_generate,\n","    grade_generation_v_documents_and_question,\n","    init_query,\n","    tavily_search,\n","    workflow\n",")\n","\n","\n","# Initialize workflow\n","app = workflow.compile()\n","# í”„ë ˆì„ì›Œí¬ ì§€ì› ì‹œ ìƒíƒœ ì´ˆê¸°í™”\n","if hasattr(app, \"reset\"):\n","    app.reset()\n","\n","# Streamlit app configuration\n","st.set_page_config(\n","    page_title=\"Policy Chatbot\",\n","    page_icon=\":robot:\",\n",")\n","\n","# Hide Streamlit default style\n","st.markdown(\n","    \"\"\"\n","    <style>\n","    #MainMenu {visibility: hidden;}\n","    footer {visibility: hidden;}\n","    header {visibility: hidden;}\n","    </style>\n","    \"\"\",\n","    unsafe_allow_html=True\n",")\n","\n","\n","# Sidebar for chat history\n","st.sidebar.title(\"ì£¼ìš” ê¸°ëŠ¥\")\n","st.sidebar.write(\"â‘  ê°œì¸ ë§ì¶¤í˜• ì •ì±… ì¶”ì²œ\")\n","st.sidebar.write(\"â‘¡ ì •ì±… ì„¸ë¶€ì‚¬í•­ ì œê³µ\")\n","st.sidebar.write(\"â‘¢ ì •ì±… ìš©ì–´ ì˜ë¯¸ ì„¤ëª…\")\n","st.sidebar.write(\"â‘£ ì‹¤ì œ í›„ê¸° ìš”ì•½\")\n","\n","\n","# Streamlit UI\n","st.title(\"âœ¨ ì²­ë…„ ì •ì±… ì±—ë´‡\")\n","\n","# Initialize session state\n","if 'responses' not in st.session_state:\n","    st.session_state['responses'] = [\"ì²­ë…„ ì •ì±…, ì œê°€ ì•Œë ¤ë“œë¦´ê²Œìš”!\"]\n","if 'requests' not in st.session_state:\n","    st.session_state['requests'] = [\"\"]\n","if 'combined_question' not in st.session_state:\n","    st.session_state['combined_question'] = \"\"\n","st.session_state['not_useful_count'] = 0\n","\n","\n","# User input\n","user_question = st.chat_input(\"Enter your question: \")\n","#submit = st.button('Submit')\n","\n","if user_question:\n","    if st.session_state['combined_question']:\n","        st.session_state['combined_question'] += f\"\\nuser: {user_question}\"\n","    else:\n","        st.session_state['combined_question'] = f\"user: {user_question}\"\n","\n","    inputs = {\"question\": st.session_state['combined_question']}\n","\n","    # Initialize the response to store the final generation\n","    final_response = None\n","\n","    # Execute workflow\n","    for output in app.stream(inputs):\n","        for key, value in output.items():\n","            # Capture the latest LLM generation\n","            if \"generation\" in value:\n","                final_response = value['generation']\n","\n","    # Append the final response to session state\n","    if final_response:\n","        st.session_state['responses'].append(final_response)\n","        st.session_state['requests'].append(user_question)\n","        st.session_state['combined_question'] += f\"\\nassistant: {final_response}\"\n","\n","\n","# Chat interface\n","if st.session_state['responses']:\n","    st.chat_message(\"assistant\").write(st.session_state['responses'][0])\n","    for i in range(1, len(st.session_state['responses'])):\n","        # Display user's question if it exists for the current index\n","        #if i < len(st.session_state['requests']):\n","        if not st.session_state['requests'][i]:\n","            continue\n","        st.chat_message(\"user\").write(st.session_state['requests'][i])\n","\n","        # Display assistant's response\n","        st.chat_message(\"assistant\").write(st.session_state['responses'][i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zoy_PeBgBhBK","outputId":"efd90b1e-5669-4bc9-97a3-cd5d8ebb1fe6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"markdown","source":["# run"],"metadata":{"id":"k_Gjwa_8F2Jg"}},{"cell_type":"code","source":["import urllib\n","print(\"Password/Enpoint IP for localtunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n","\n","# \"Password/Enpoint IP for localtunnel is:\" ìš°ì¸¡ì— xx.xxx.xx.xxx í˜¹ì€ xx.xxx.xxx.xxx í˜•ì‹ì˜ ìˆ«ìê°€ ë‚˜ì˜¨ë‹¤.\n","\n","!streamlit run app.py &>/content/logs.txt &\n","!npx localtunnel --port 8501"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B7SVSGLZF3vr","outputId":"bbc18689-7826-479a-dbf7-203625f3b886"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Password/Enpoint IP for localtunnel is: 34.147.87.145\n","\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kyour url is: https://stupid-lands-march.loca.lt\n","^C\n"]}]}]}
