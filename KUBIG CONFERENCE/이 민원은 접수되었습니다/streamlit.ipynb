{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"qSASe8--TMmo"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fxuocTPANjQp","outputId":"7e7afef4-7d6b-4b88-ce20-72ebbb4bb31e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","\n","API_KEYS = {\n","    \"UPSTAGE_API_KEY\": None,\n","    \"TAVILY_API_KEY\" : None\n","}\n","\n","def load_env():\n","    # 환경 변수 설정\n","    if \"google.colab\" in str(get_ipython()):  # Google Colab 환경\n","        os.environ['UPSTAGE_API_KEY'] = ''\n","        API_KEYS[\"UPSTAGE_API_KEY\"] = os.environ.get(\"UPSTAGE_API_KEY\")\n","        os.environ['TAVILY_API_KEY'] = ''\n","        API_KEYS[\"TAVILY_API_KEY\"] = os.environ.get(\"TAVILY_API_KEY\")\n","    else:  # 로컬 환경\n","        load_dotenv()  # .env 파일 로드\n","        API_KEYS[\"UPSTAGE_API_KEY\"] = os.environ.get(\"UPSTAGE_API_KEY\")\n","        API_KEYS[\"TAVILY_API_KEY\"] = os.environ.get(\"TAVILY_API_KEY\")\n","\n","    return API_KEYS[\"UPSTAGE_API_KEY\"], API_KEYS[\"TAVILY_API_KEY\"]\n","\n","UPSTAGE_API_KEY, TAVILY_API_KEY= load_env()"],"metadata":{"id":"lFsFlQevHpYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q streamlit\n","!npm install localtunnel\n","!pip install --upgrade -q accelerate bitsandbytes\n","!pip install git+https://github.com/huggingface/transformers.git\n","!pip install sentence_transformers\n","!pip install streamlit-folium\n","!npm audit fix\n","\n","!pip install -U langchain_community tiktoken langchainhub langchain langgraph chromadb\n","!pip install tavily-python\n","!pip install -qU langchain-core langchain-upstage langchain-chroma\n","!pip install -qU python-dotenv\n","!pip install -U langchain-upstage\n","!pip install -U openai\n","\n","!pip install streamlit_chat\n","!pip install qdrant_client"],"metadata":{"id":"GTPgrgx0H-ks"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# config.py"],"metadata":{"id":"Si-vwNROFtpE"}},{"cell_type":"code","source":["%%writefile config.py\n","\n","from langchain.docstore.document import Document\n","from langchain_community.retrievers import TavilySearchAPIRetriever\n","from langchain import hub\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_upstage import UpstageEmbeddings, ChatUpstage\n","from langchain.vectorstores import Chroma\n","from langgraph.graph import END, StateGraph, START\n","from langchain_core.pydantic_v1 import BaseModel, Field\n","from dotenv import load_dotenv\n","from IPython import get_ipython\n","import numpy as np\n","import os\n","import pandas as pd\n","from pprint import pprint\n","import re\n","from typing import List, Dict\n","from typing_extensions import TypedDict\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","from openai import OpenAI\n","import json\n","\n","\n","client =OpenAI(api_key='')\n","\n","\n","'''\n","Embedding and Database\n","'''\n","\n","# 기존 설정\n","embedding_function = UpstageEmbeddings(model=\"solar-embedding-1-large\")\n","\n","# 두 개의 Chroma 인스턴스 생성\n","persist_directory = '/content/drive/MyDrive/policy_chatbot/policy_combined'\n","db = Chroma(embedding_function=embedding_function, persist_directory=persist_directory)\n","retriever = db.as_retriever(search_kwargs={\"k\": 3})\n","\n","\n","'''\n","Retrieval Grader\n","'''\n","\n","# Data model\n","class GradeDocuments(BaseModel):\n","    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n","    # 검색된 문서가 질문에 적합한지를 'yes' 또는 'no'로 저장\n","    binary_score: str = Field(\n","        description=\"Documents are relevant to the conversation history and question, 'yes' or 'no'\"\n","    )\n","\n","# LLM with function call\n","llm_1 = ChatUpstage(max_tokens=200, temperature=0) # Parameter tuning 1\n","# 모델이 구조화된 데이터(GradeDocuments)를 반환하도록 설정\n","structured_llm_grader = llm_1.with_structured_output(GradeDocuments)\n","\n","# System Prompt: 검색한 문서에 유저 질문 관련 키워드가 포함되거나 의미적으로 관련이 있으면, 'relevant'로 판단. 결과는 'yes' 또는 'no'로 이진 점수를 줌.\n","system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n","    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n","    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n","    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n","    'Yes' means that the document is relevant to the question.\"\"\"\n","\n","\n","# grade_prompt = System prompt + 검색된 문서 + 사용자 질문\n","grade_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", system),\n","        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n","    ]\n",")\n","# grade_prompt와 structured_llm_grader을 결합하여 문서 평가를 위한 객체 생성\n","retrieval_grader = grade_prompt | structured_llm_grader\n","\n","# Wrapper for consistent output\n","def grade_retrieval(doc_txt, user_question):\n","    \"\"\"\n","    Grades the relevance of a document to the combined question.\n","\n","    Args:\n","        doc_txt (str): The text of the document to grade.\n","        combined_question (str): The combined question including chat history.\n","\n","    Returns:\n","        str: 'yes' or 'no' based on relevance grading.\n","    \"\"\"\n","    try:\n","        # Invoke grader\n","        result = retrieval_grader.invoke({\"document\": doc_txt, \"question\": user_question})\n","        print(f\"Raw result from grader: {result}\")\n","\n","        if result and hasattr(result, \"binary_score\"):\n","            return result.binary_score\n","        else:\n","            print(f\"Invalid result structure: {result}\")\n","    except Exception as e:\n","        print(f\"Error during retrieval grading: {e}\")\n","\n","    # Default to 'no' if something goes wrong\n","    print('None')\n","    return \"no\"\n","\n","\n","'''\n","Generate\n","'''\n","\n","generate_system = \"\"\"\n","    당신은 질문에 답변을 제공하는 AI 비서입니다. 검색된 context를 바탕으로 질문에 답변하세요. 만약 답을 모를 경우, 모른다고 솔직히 말하세요. \\n\n","    청년 연령, 직업, 주거 상황, 정책 경험 여부 등 다양한 상황을 고려하여 맞춤형 답변을 제공합니다.\n","\n","    질문의 유형에 따라 아래 지침을 선택하고, 이 지침을 준수하여 답변을 작성하세요: \\n\n","    1.** 정책 추천 질문에 대한 지침 **\n","    - 아래 형식으로 답변하세요:\n","      ① 정책명:\n","      ② 추천 이유: 질문에서 제공된 연령, 상황, 선호도 등을 바탕으로 정책이 왜 적합한지 설명하세요.\n","      ③ 제공 혜택: 정책을 통해 얻을 수 있는 구체적인 이점을 서술하세요.\n","    - 정책 추천 시, 최소 2개 이상의 정책은 추천해야 하고, 아래 데이터를 적극 활용하여 가장 추천이 우선시 되는 정책부터 나열해주세요:\n","        * **복지 및 문화 관련 이슈**\n","          - 대학생: 진로 불확실성, 졸업유예 증가, 등록금 부담.\n","          - 직장인: 저임금, 고용불안(1-2년) 우려.\n","          - 미취업자: 심리상담 지원, 대중교통비 지원 선호.\n","          - 전반적으로 청년들은 안정적인 자립 지원과 금전적 지원에 대한 관심이 높음.\n","          - 대중교통비 지원, 심리상담 지원, 문화활동 및 여행 장려금 지원 등이 주요 선호 정책으로 나타남.\n","    - 높임말 대신, \"~요\"로 끝나는 말투를 사용하세요. 예: \"알려드릴게요\", \"도움이 되셨으면 해요\"\n","    - 답변의 시작은 반드시 \"세은님에게 맞는 정책을 찾아보았어요😊\\n\"로 시작하세요.\n","    - 답변의 끝은 반드시 \"\\n정책에 대해 더 구체적으로 알고 싶으신가요?\"로 끝내세요.\n","\n","    2. ** 정책 세부 정보 요청 질문에 대한 지침 **\n","    - 답변의 시작에는 반드시 사용자가 어떤 정책의 어떤 내용을 궁금해하는지 요약하고, \"제가 구체적으로 알려드릴게요😄\\n\" 문장을 추가하세요.\n","    - 높임말 대신, \"~요\"로 끝나는 말투를 사용하세요. 예: \"알려드릴게요\", \"도움이 되셨으면 해요\"\n","    - 답변은 나열식이 아닌 줄글 형식으로 작성하세요.\n","    - 답변의 끝에는 반드시 \"\\n궁금증이 해결되었나요?\" 문장을 추가하세요.\n","    - **예시 답변 형식**:\n","    > \"경기도 면접수당 정책의 신청 기간이 궁금하셨군요. 제가 구체적으로 알려드릴게요😄\\n 신청 기간은 2024년 5월 2일부터 5월 30일까지입니다. \\n궁금증이 해결되었나요?\"\n","\n","    3. ** 정책 용어 설명 요청 질문에 대한 지침 **\n","    - 답변의 시작에는 반드시 사용자가 어떤 단어의 뜻을 궁금해하는지 요약하고, \"제가 구체적으로 알려드릴게요😄\\n\" 문장을 추가하세요.\n","    - 높임말 대신, \"~요\"로 끝나는 말투를 사용하세요. 예: \"알려드릴게요\", \"도움이 되셨으면 해요\"\n","    - 문서에 있는 내용을 기반으로 답변은 나열식이 아닌 줄글 형식으로 작성하고, 간결하게 설명하세요.\n","    - 답변의 끝에는 반드시 \"\\n궁금증이 해결되었나요?\" 문장을 추가하세요.\n","    - **예시 답변 형식**:\n","    > \"가등기가 무슨 뜻인지 궁금하셨군요. 제가 구체적으로 알려드릴게요😄\\n 가등기란, 등기로 표시된 부동산의 권리관계가 외부에 표시되기 때문에 제3자에게도 대항할 수 있는데, 권리관계가 확정되지 않는 등으로 등기를 할 수 없을 경우에 임시로 하는 등기를 뜻합니다. 궁금증이 해결되었나요?\"\n","\n","    4. ** 정책 후기 요청 질문에 대한 지침 **\n","    - 답변의 시작에는 반드시 사용자가 어떤 정책의 후기를 궁금해하는지 요약하고, \"제가 검색해보았어요🔎\\n\" 문장을 추가하세요.\n","    - 높임말 대신, \"~요\"로 끝나는 말투를 사용하세요. 예: \"알려드릴게요\", \"도움이 되셨으면 해요\"\n","    - 문서에는 후기가 없으니 잘 모른다고 답변하고, 외부 검색 기능을 이용하여 답변하세요.\n","    - 답변의 끝에는 반드시 \"\\n궁금증이 해결되었나요?\" 문장을 추가하세요.\n","    - **예시 답변 형식**:\n","    > \"경기도 면접수당 정책의 실제 블로그 후기가 궁금하셨군요. 제가 검색해보았어요🔎\\n[후기 요약]\\n궁금증이 해결되었나요?\"\n","    \"\"\"\n","\n","TOKEN_PER_CHAR = 4\n","\n","def estimate_tokens(text):\n","    return len(text) // TOKEN_PER_CHAR\n","\n","# Function to format documents without exceeding the token limit\n","def format_docs(docs, max_tokens=110000):\n","    formatted_docs = []\n","    current_chunk = \"\"\n","    current_token_count = 0\n","\n","    for doc in docs:\n","        doc_content = doc.page_content\n","        doc_token_count = estimate_tokens(doc_content)\n","\n","        # If adding this document exceeds the token limit, start a new chunk\n","        if current_token_count + doc_token_count > max_tokens:\n","            formatted_docs.append(current_chunk)\n","            current_chunk = doc_content  # Start a new chunk\n","            current_token_count = doc_token_count\n","        else:\n","            current_chunk += \"\\n\\n\" + doc_content  # Append to the current chunk\n","            current_token_count += doc_token_count\n","\n","    # Add the last chunk if there's any content left\n","    if current_chunk:\n","        formatted_docs.append(current_chunk)\n","\n","    return formatted_docs\n","\n","# 모델 호출 체인\n","def rag_chain(question, context):\n","    response = client.chat.completions.create(\n","        model=\"gpt-4o-mini\",\n","        messages=[\n","            {\"role\": \"system\",\n","             \"content\": generate_system},\n","            {\"role\": \"user\",\n","             \"content\": f\"Question: {question}\\n\\nContext: {context}\\nAnswer the question.\"},\n","        ],\n","        max_tokens=400,\n","        temperature=0,\n","    )\n","    return response.choices[0].message.content\n","\n","'''\n","Answer Grader\n","'''\n","\n","# Data model\n","class GradeAnswer(BaseModel):\n","    \"\"\"Binary score to assess answer addresses question.\"\"\"\n","\n","    binary_score: str = Field(\n","        description=\"Answer addresses the question, 'yes' or 'no'\"\n","    )\n","    confidence: float = Field(\n","        description=\"Confidence score for the assessment, range 0 to 1\"\n","    )\n","\n","# Revised System Prompt\n","answer_system_strict = \"\"\"당신은 응답이 질문에 적절한지 엄격히 평가하는 채점자입니다.\n","    1. 응답이 문서의 신뢰할 수 있는 정보를 바탕으로 질문에 명확히 답했는지 확인하세요.\n","    2. 문서 내용이 질문과 관련 없거나 응답이 문서를 정확히 반영하지 않으면 '아니오'를 반환하세요.\n","    3. 응답이 신뢰할 수 없는 내용을 포함하거나 불확실하면 '아니오'를 반환하세요.\n","    4. '예'로 평가하려면 문서 내용을 충분히 참고하여 질문을 직접 해결해야 합니다.\n","    5. 정책의 후기를 묻는 질문에 대한 답변이 '후기는 sns나 블로그를 통해 찾을 수 있어요' 와 같이 자세하지 않은 내용을 포함한 경우 '아니오'를 반환하세요.\n","\n","    응답을 반드시 다음 JSON 형식으로 반환하세요:\n","{\n","    \"binary_score\": \"yes\" 또는 \"no\",\n","    \"confidence\": 0에서 1 사이의 부동소수점 값\n","}\n","\"\"\"\n","\n","def answer_grader(question, generation, document):\n","    response = client.chat.completions.create(\n","        model=\"gpt-4o-mini\",\n","        messages=[\n","            {\"role\": \"system\",\n","             \"content\": answer_system_strict},\n","            {\"role\": \"user\",\n","             \"content\": f\"User question: {question}\\n\\nLLM generation: {generation}\\n\\nContext: {document}\\nEvaluate the generated answer strictly based on the context.\"},\n","        ],\n","        max_tokens=400,\n","        temperature=0,\n","    )\n","    print(\"Raw response:\", response.choices[0].message.content)\n","    try:\n","        response_data = json.loads(response.choices[0].message.content)\n","        binary_score = response_data.get(\"binary_score\", \"no\")  # \"binary_score\"가 없으면 기본값 \"no\" 반환\n","    except json.JSONDecodeError as e:\n","        print(f\"Error parsing JSON: {e}\")\n","        binary_score = \"no\"  # 파싱 오류가 발생하면 기본값 \"no\" 반환\n","\n","    return binary_score\n","\n","\n","'''\n","Question Rewriter\n","'''\n","\n","# LLM\n","llm_5 = ChatUpstage(max_tokens=200, temperature=0)\n","\n","question_rewriter_system = \"\"\"\n","    당신은 입력된 질문을 벡터 저장소 검색에 최적화된 더 나은 버전으로 변환하는 질문 재작성자입니다. 입력 내용을 바탕으로, 간단하고 명료한 질문으로 재작성하세요. 문장 간 내용이 다른 범주라 느껴지면 과감히 앞 내용은 버리세요.\n","    \"\"\"\n","\n","re_write_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", question_rewriter_system),\n","        (\n","            \"human\",\n","           \" {question} \\n Formulate an improved question.\",\n","        ),\n","    ]\n",")\n","\n","question_rewriter = re_write_prompt | llm_5 | StrOutputParser()\n","\n","'''\n","Structure\n","'''\n","\n","class GraphState(TypedDict):\n","    \"\"\"\n","    Represents the state of our graph.\n","\n","    Attributes:\n","        question: Combined question including chat history and user query.\n","        generation: LLM generation\n","        documents: list of documents\n","    \"\"\"\n","\n","    question: str\n","    generation: str\n","    documents: List[str]\n","    first_generate_attempt: bool\n","\n","\n","'''\n","Function\n","'''\n","\n","### Nodes\n","\n","def check_documents(state):\n","    \"\"\"\n","    Check if documents exist in the current state and update the state accordingly.\n","\n","    Args:\n","        state (dict): The current graph state.\n","\n","    Returns:\n","        dict: Updated state with a flag indicating the next step.\n","    \"\"\"\n","\n","    if state.get(\"documents\"):\n","        print(\"Documents found, marking for 'generate_first'.\")\n","        state[\"next_node\"] = \"generate_first\"  # Add a marker for the next step\n","    else:\n","        print(\"No documents found, marking for 'retrieve'.\")\n","        state[\"next_node\"] = \"retrieve\"  # Add a marker for the next step\n","\n","    return state\n","\n","\n","def retrieve(state):\n","    \"\"\"\n","    Retrieve documents and update the current state.\n","\n","    Args:\n","        state (dict): The current graph state.\n","\n","    Returns:\n","        dict: Updated state with retrieved documents.\n","    \"\"\"\n","    print(\"---RETRIEVE---\")\n","    question = state[\"question\"]\n","\n","    # Process the question\n","    if re.search(r'\\n\\s*user', question):\n","        lines = question.split(\"\\nuser\")\n","        question_to_retrieve = \"\\nuser\".join(lines[:-1])\n","    else:\n","        question_to_retrieve = question\n","\n","    # Retrieve documents\n","    documents = retriever.get_relevant_documents(question_to_retrieve)\n","    print(\"Retrieved documents:\", documents)\n","\n","    # Update state\n","    state[\"documents\"] = documents\n","    return state\n","\n","\n","def generate(state):\n","    \"\"\"\n","    Generate answer based on the current state.\n","\n","    Args:\n","        state (dict): The current graph state containing question and documents.\n","\n","    Returns:\n","        dict: Updated state with the generated answer added under the key 'generation'.\n","    \"\"\"\n","    print(\"---GENERATE---\")\n","\n","    # Extract question and documents from state\n","    question = state.get(\"question\")\n","    documents = state.get(\"documents\")\n","\n","    if not question or not documents:\n","        raise ValueError(\"State must include both 'question' and 'documents' keys with valid data.\")\n","\n","    # Format the documents into a single context string\n","    context = format_docs(documents)\n","\n","    # Invoke RAG chain to get the generation\n","    generation = rag_chain(question, context)\n","\n","    # Return updated state with the generation\n","    updated_state = {\n","        **state,\n","        \"generation\": generation,\n","    }\n","\n","    return updated_state\n","\n","\n","def extract_last_question(combined_question):\n","    \"\"\"\n","    Extracts the last user question from the combined question.\n","\n","    Args:\n","        combined_question (str): The combined question including chat history.\n","\n","    Returns:\n","        str: The last user question.\n","    \"\"\"\n","    lines = combined_question.split(\"\\n\")\n","    for line in reversed(lines):\n","        if line.startswith(\"user:\"):\n","            return line[len(\"user: \"):].strip()  # Remove \"user: \" prefix\n","    return \"\"  # Default to empty if no user question found\n","\n","\n","\n","def grade_documents(state):\n","    \"\"\"\n","    Determines whether the retrieved documents are relevant to the question.\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        state (dict): Updates documents key with only filtered relevant documents\n","    \"\"\"\n","\n","    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n","    question = state[\"question\"]\n","    documents = state[\"documents\"]\n","\n","    # Extract the last question\n","    last_question = extract_last_question(question)\n","    print(f\"Last User Question: {last_question}\")  # Debugging: Check extracted question\n","\n","    # Score each doc\n","    filtered_docs = []\n","    all_irrelevant = True\n","    for d in documents:\n","        grade = grade_retrieval(d, last_question)\n","        if grade == \"yes\":\n","            print(\"---GRADE: DOCUMENT RELEVANT---\")\n","            filtered_docs.append(d)\n","            all_irrelevant = False\n","              # At least one document is relevant\n","        else:\n","            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n","            continue\n","\n","    # Update state\n","    state[\"documents\"] = filtered_docs\n","\n","    # Add a flag to indicate if all documents were irrelevant\n","    state[\"all_documents_irrelevant\"] = all_irrelevant\n","\n","    return state\n","\n","\n","def transform_query(state):\n","    \"\"\"\n","    Transform the query to produce a better question.\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        state (dict): Updates question key with a re-phrased question\n","    \"\"\"\n","\n","    print(\"---TRANSFORM QUERY---\")\n","    question = state[\"question\"]\n","    documents = state[\"documents\"]\n","\n","    # Re-write question\n","    better_question = question_rewriter.invoke({\"question\": question})\n","    return {\"documents\": documents, \"question\": better_question}\n","\n","\n","### Edges\n","\n","\n","def decide_to_generate(state):\n","    \"\"\"\n","    Determines whether to generate an answer, or re-generate a question.\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        str: Binary decision for next node to call\n","    \"\"\"\n","\n","    print(\"---ASSESS GRADED DOCUMENTS---\")\n","\n","    # Extract the last user question directly from the state\n","    last_question = extract_last_question(state[\"question\"])\n","    print(f\"Last User Question: {last_question}\")  # Debugging\n","\n","    # If all documents are irrelevant, transform the query\n","    if state.get(\"all_documents_irrelevant\", False):  # Check if irrelevant is True\n","        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INIT QUERY---\")\n","        return \"init_query\"\n","\n","    # If documents are relevant, generate a response\n","    else:\n","        print(\"---DECISION: GENERATE---\")\n","        return \"transform_query\"\n","\n","\n","# Initialize the not useful counter\n","not_useful_count = 0\n","first_generate_count=0\n","\n","def grade_generation_v_documents_and_question(state):\n","    \"\"\"\n","    Determines whether the generation answers the question.\n","    Returns:\n","        str: 'useful', 'not useful', or 'fallback'.\n","    \"\"\"\n","    global not_useful_count\n","    global first_generate_count\n","\n","    print(\"---GRADE GENERATION vs QUESTION---\")\n","    question = state[\"question\"]\n","    documents = state[\"documents\"]\n","    generation = state[\"generation\"]\n","\n","    # Extract the last question for grading\n","    last_question = extract_last_question(question)\n","\n","    try:\n","        # Perform grading based on whether it's the first attempt\n","        if first_generate_count==0 :\n","            grade = answer_grader(last_question, generation, documents)\n","        else:\n","            grade = answer_grader(question, generation, documents)\n","    except Exception as e:\n","        print(f\"Error during answer grading: {e}\")\n","        grade = \"no\"\n","    if first_generate_count== 0 :\n","        if grade == \"yes\":\n","            print(\"---FIRST GENERATE ATTEMPT: USEFUL\")\n","            return \"useful\"\n","        else:\n","            print(\"---FIRST GENERATE ATTEMPT: NOT USEFUL\")\n","            first_generate_count += 1\n","            return \"not useful\"\n","    else:\n","        if grade == \"yes\":\n","            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n","            not_useful_count = 0\n","            first_generate_count=0  # Reset the counter\n","            return \"useful\"\n","        else:\n","            print(f\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n","            not_useful_count += 1\n","            print(f\"---NOT USEFUL COUNT INCREMENTED: count={not_useful_count}---\")\n","            if not_useful_count == 1:\n","                return \"not useful\"\n","            else:\n","                return \"fallback\"\n","\n","\n","def init_query(state):\n","    \"\"\"\n","    Removes chat/document history, leaves only the last user question,\n","    and resets not_useful_count to 0.\n","    \"\"\"\n","    print(\"---INIT QUERY (CLEAR HISTORY)---\")\n","    # 마지막 사용자 질문만 뽑아오기\n","    last_question = extract_last_question(state[\"question\"])\n","\n","    # state 초기화\n","    state[\"question\"] = f\"user: {last_question}\"\n","    state[\"documents\"] = []\n","    state[\"generation\"] = \"\"\n","\n","    print(f\"[init_query] Updated question = {state['question']}\")\n","    return state\n","\n","\n","'''\n","Tavily\n","'''\n","\n","def tavily_search(state):\n","    \"\"\"\n","    External search using Tavily and process the results into a structured markdown report.\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        dict: Updated state with processed documents and structured report\n","    \"\"\"\n","    print(\"---TAVILY SEARCH---\")\n","    question = state[\"question\"]\n","\n","    # Perform Tavily search\n","    tavily = TavilySearchAPIRetriever(k=3, search_depth=\"advanced\", include_domains=[\"m.blog.naver.com\"])\n","    print(\"Searching using Tavily...\")\n","\n","    docs_external = tavily.invoke(question)  # Tavily 검색 호출\n","    print(f\"Search results: {len(docs_external)} documents found.\")\n","\n","\n","    if docs_external:\n","        print(\"Relevant Tavily search results found.\")\n","        return {\"documents\": docs_external, \"question\": question}\n","    else:\n","        print(\"No relevant documents found after filtering.\")\n","        return {\"documents\": [], \"question\": question}\n","\n","'''\n","Workflow & App\n","'''\n","\n","# Define the workflow\n","workflow = StateGraph(GraphState)\n","\n","# Define the nodes\n","workflow.add_node(\"retrieve\", retrieve)  # Retrieve documents\n","workflow.add_node(\"grade_documents\", grade_documents)\n","workflow.add_node(\"retrieve_no_grade\", retrieve)  # Re-retrieve without grading\n","workflow.add_node(\"generate_first\", generate)  # First generate\n","workflow.add_node(\"generate_second\", generate)\n","workflow.add_node(\"init_query\", init_query)  # Initialize query\n","workflow.add_node(\"transform_query1\", transform_query)\n","workflow.add_node(\"transform_query2\", transform_query)  # Transform query\n","workflow.add_node(\"tavily_search\", tavily_search)  # External search node\n","workflow.add_node(\"check_documents\", check_documents)  # Custom node to check documents\n","\n","\n","# Add edges for document check\n","workflow.add_edge(START, \"check_documents\")\n","\n","workflow.add_conditional_edges(\n","    \"check_documents\",\n","    lambda state: state.get(\"next_node\"),  # Use `next_node` to determine the flow\n","    {\n","        \"generate_first\": \"generate_first\",\n","        \"retrieve\": \"retrieve\"\n","    },\n",")\n","\n","# First generate node\n","workflow.add_conditional_edges(\n","    \"generate_first\",\n","    grade_generation_v_documents_and_question,\n","    {\n","        \"useful\": \"transform_query1\",  # If generation is useful, end\n","        \"not useful\": \"retrieve\"\n","    },\n",")\n","\n","workflow.add_edge(\"transform_query1\", END)\n","\n","\n","# Define the main edges\n","workflow.add_edge(\"retrieve\", \"grade_documents\")\n","workflow.add_conditional_edges(\n","    \"grade_documents\",\n","    decide_to_generate,\n","    {\n","        \"init_query\": \"init_query\",\n","        \"transform_query\": \"transform_query2\"\n","    },\n",")\n","\n","workflow.add_edge(\"init_query\", \"retrieve_no_grade\")\n","workflow.add_edge(\"retrieve_no_grade\", \"transform_query2\")\n","workflow.add_edge(\"transform_query2\", \"generate_second\")\n","\n","# Second generate node\n","workflow.add_conditional_edges(\n","    \"generate_second\",\n","    grade_generation_v_documents_and_question,\n","    {\n","        \"useful\": END,  # If generation is useful, end\n","        \"not useful\": \"generate_second\",  # Retry with transformed query\n","        \"fallback\": \"tavily_search\"  # Retry generation with external search\n","    },\n",")\n","\n","workflow.add_edge(\"tavily_search\", \"generate_second\")\n","\n","# Compile the workflow\n","app = workflow.compile()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5InZHE9w7ONu","outputId":"89c3a2a0-1c3f-4f98-a041-50db5118a7cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing config.py\n"]}]},{"cell_type":"markdown","source":["# app.py"],"metadata":{"id":"pi7EViB6Fx2Q"}},{"cell_type":"code","source":["%%writefile app.py\n","import streamlit as st\n","from pprint import pprint\n","import os\n","import re\n","import time\n","from pprint import pprint\n","from langchain.chains import ConversationalRetrievalChain\n","from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.memory import ConversationSummaryBufferMemory\n","from langchain.vectorstores import Qdrant\n","from streamlit_chat import message\n","from qdrant_client import QdrantClient\n","from openai import OpenAI\n","from config import (\n","    retriever,\n","    GradeDocuments,\n","    format_docs,\n","    rag_chain,\n","    GradeAnswer,\n","    GraphState,\n","    retrieve,\n","    generate,\n","    extract_last_question,\n","    grade_documents,\n","    transform_query,\n","    decide_to_generate,\n","    grade_generation_v_documents_and_question,\n","    init_query,\n","    tavily_search,\n","    workflow\n",")\n","\n","\n","# Initialize workflow\n","app = workflow.compile()\n","# 프레임워크 지원 시 상태 초기화\n","if hasattr(app, \"reset\"):\n","    app.reset()\n","\n","# Streamlit app configuration\n","st.set_page_config(\n","    page_title=\"Policy Chatbot\",\n","    page_icon=\":robot:\",\n",")\n","\n","# Hide Streamlit default style\n","st.markdown(\n","    \"\"\"\n","    <style>\n","    #MainMenu {visibility: hidden;}\n","    footer {visibility: hidden;}\n","    header {visibility: hidden;}\n","    </style>\n","    \"\"\",\n","    unsafe_allow_html=True\n",")\n","\n","\n","# Sidebar for chat history\n","st.sidebar.title(\"주요 기능\")\n","st.sidebar.write(\"① 개인 맞춤형 정책 추천\")\n","st.sidebar.write(\"② 정책 세부사항 제공\")\n","st.sidebar.write(\"③ 정책 용어 의미 설명\")\n","st.sidebar.write(\"④ 실제 후기 요약\")\n","\n","\n","# Streamlit UI\n","st.title(\"✨ 청년 정책 챗봇\")\n","\n","# Initialize session state\n","if 'responses' not in st.session_state:\n","    st.session_state['responses'] = [\"청년 정책, 제가 알려드릴게요!\"]\n","if 'requests' not in st.session_state:\n","    st.session_state['requests'] = [\"\"]\n","if 'combined_question' not in st.session_state:\n","    st.session_state['combined_question'] = \"\"\n","st.session_state['not_useful_count'] = 0\n","\n","\n","# User input\n","user_question = st.chat_input(\"Enter your question: \")\n","#submit = st.button('Submit')\n","\n","if user_question:\n","    if st.session_state['combined_question']:\n","        st.session_state['combined_question'] += f\"\\nuser: {user_question}\"\n","    else:\n","        st.session_state['combined_question'] = f\"user: {user_question}\"\n","\n","    inputs = {\"question\": st.session_state['combined_question']}\n","\n","    # Initialize the response to store the final generation\n","    final_response = None\n","\n","    # Execute workflow\n","    for output in app.stream(inputs):\n","        for key, value in output.items():\n","            # Capture the latest LLM generation\n","            if \"generation\" in value:\n","                final_response = value['generation']\n","\n","    # Append the final response to session state\n","    if final_response:\n","        st.session_state['responses'].append(final_response)\n","        st.session_state['requests'].append(user_question)\n","        st.session_state['combined_question'] += f\"\\nassistant: {final_response}\"\n","\n","\n","# Chat interface\n","if st.session_state['responses']:\n","    st.chat_message(\"assistant\").write(st.session_state['responses'][0])\n","    for i in range(1, len(st.session_state['responses'])):\n","        # Display user's question if it exists for the current index\n","        #if i < len(st.session_state['requests']):\n","        if not st.session_state['requests'][i]:\n","            continue\n","        st.chat_message(\"user\").write(st.session_state['requests'][i])\n","\n","        # Display assistant's response\n","        st.chat_message(\"assistant\").write(st.session_state['responses'][i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zoy_PeBgBhBK","outputId":"efd90b1e-5669-4bc9-97a3-cd5d8ebb1fe6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"markdown","source":["# run"],"metadata":{"id":"k_Gjwa_8F2Jg"}},{"cell_type":"code","source":["import urllib\n","print(\"Password/Enpoint IP for localtunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n","\n","# \"Password/Enpoint IP for localtunnel is:\" 우측에 xx.xxx.xx.xxx 혹은 xx.xxx.xxx.xxx 형식의 숫자가 나온다.\n","\n","!streamlit run app.py &>/content/logs.txt &\n","!npx localtunnel --port 8501"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B7SVSGLZF3vr","outputId":"bbc18689-7826-479a-dbf7-203625f3b886"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Password/Enpoint IP for localtunnel is: 34.147.87.145\n","\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://stupid-lands-march.loca.lt\n","^C\n"]}]}]}
