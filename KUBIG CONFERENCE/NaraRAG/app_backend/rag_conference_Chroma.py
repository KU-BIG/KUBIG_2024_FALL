import os
from dotenv import load_dotenv
load_dotenv()
import streamlit as st
import time
import uuid
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from langchain_community.chat_models import ChatOpenAI
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import ParentDocumentRetriever
from langchain_core.messages import HumanMessage, SystemMessage
from langchain.storage._lc_store import create_kv_docstore
from langchain.storage.file_system import LocalFileStore
from langchain.retrievers.multi_query import MultiQueryRetriever

if "id" not in st.session_state:
    st.session_state.id = uuid.uuid4()
    st.session_state.file_cache = {}

session_id = st.session_state.id
client = None

def reset_chat():
    st.session_state.messages = []
    st.session_state.context = None

# ChromaDB ë¡œë“œ
vectorstore = Chroma(
    collection_name="split_parents", embedding_function=UpstageEmbeddings(model="embedding-passage"), 
    persist_directory='child_DB(Chroma)') # DB_PATH

child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)

chat = ChatUpstage()

# LocalFileStore ë¡œë“œ
fs = LocalFileStore('./parent_fs_chroma')
store = create_kv_docstore(fs)

# ChromaDB + LocalFileStore -> ParentDocumentRetriever
retriever = ParentDocumentRetriever(
vectorstore=vectorstore,
docstore=store,
child_splitter=child_splitter,
search_kwargs={'k':5})

# MultiQueryRetriever ë¡œë“œ
retriever = MultiQueryRetriever.from_llm(
    retriever=retriever, llm=chat)

# 1) ì±—ë´‡ì— 'ê¸°ì–µ'ì„ ì…íˆê¸° ìœ„í•œ ì²«ë²ˆì§¸ ë‹¨ê³„

# ì´ì „ ë©”ì‹œì§€ë“¤ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•´, ë¬¸ë§¥ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ì´ í˜¼ìì„œë§Œ ë´¤ì„ ë•Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ì„ ë‹¤ì‹œ êµ¬ì„±í•¨.
# ì¦‰ ìƒˆë¡œ ë“¤ì–´ì˜¨ ê·¸ ì§ˆë¬¸ ìì²´ì—ë§Œ ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ ë‹¤ì‹œ ì¬í¸ì„± (llm ì˜ˆì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•´ì„œ ëŒ€í™”ë¥¼ ì¬êµ¬ì„±)
from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# ì˜ì–´ ë²„ì „
contextualize_q_system_prompt = "When there are older conversations and more recent user questions, these questions may be related to previous conversations. In this case, change the question to a question that can be understood independently without needing to know the content of the conversation. You don't have to answer the question, just reformulate it if necessary or leave it as is."

# í•œê¸€ ë²„ì „
# contextualize_q_system_prompt = "ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì´ ìˆì„ ë•Œ, ì´ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ê²½ìš°, ëŒ€í™” ë‚´ìš©ì„ ì•Œ í•„ìš” ì—†ì´ ë…ë¦½ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë°”ê¾¸ì„¸ìš”. ì§ˆë¬¸ì— ë‹µí•  í•„ìš”ëŠ” ì—†ê³ , í•„ìš”í•˜ë‹¤ë©´ ê·¸ì € ë‹¤ì‹œ êµ¬ì„±í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë‘ì„¸ìš”."

# MessagePlaceHolder: 'chat_history' ì…ë ¥ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì „ ë©”ì„¸ì§€ ê¸°ë¡ë“¤ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ì‹œí‚´. 
# ì¦‰, í”„ë¡¬í”„íŠ¸, ë©”ì„¸ì§€ ê¸°ë¡(ë¬¸ë§¥ ì •ë³´), ì‚¬ìš©ìì˜ ì§ˆë¬¸ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ê°€ êµ¬ì„±ë¨.

contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ('system', contextualize_q_system_prompt),
        MessagesPlaceholder('chat_history'),
        ('human', '{input}'),
    ]
)

# ì´ë¥¼ í† ëŒ€ë¡œ ë©”ì„¸ì§€ ê¸°ë¡ì„ ê¸°ì–µí•˜ëŠ” retrieverë¥¼ ìƒì„±. 
history_aware_retriever = create_history_aware_retriever(
    chat, retriever, contextualize_q_prompt
)

# 2) ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” retriever ì²´ì¸ ìƒì„±
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

qa_system_prompt = """
You are an intelligent assistant helping the members of the Korean National Assembly with questions related to law and policy. Read the given questions carefully and WRITE YOUR ANSWER ONLY BASED ON THE CONTEXT AND DON'T SEARCH ON THE INTERNET. Give the answer in Korean ONLY using the following pieces of the context. You must answer politely.

DO NOT TRY TO MAKE UP AN ANSWER:
Â - If the answer to the question cannot be determined from the context alone, say "I cannot determine the answer to that.".
Â - If the context is empty, just say "I do not know the answer to that.".

Context: {context} """

# qa_system_prompt = """
# ì§ˆë¬¸-ë‹µë³€ ì—…ë¬´ë¥¼ ë•ëŠ” ë³´ì¡°ì›ì…ë‹ˆë‹¤.
# ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ê²€ìƒ‰ëœ ë‚´ìš©ì„ ì‚¬ìš©í•˜ì„¸ìš”.
# ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì„¸ìš”.
# ë‹µë³€ì€ êµ¬ì²´ì ìœ¼ë¡œ ìµœì‹  ì •ë³´ë¶€í„° ì‹œê°„ì˜ íë¦„ì— ë”°ë¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ì´ê±´ ì˜ë¬´ì‚¬í•­ì…ë‹ˆë‹¤.
# ë‹µë³€í•  ë•Œ metadataì— ìˆëŠ” sourceë¥¼ í•¨ê»˜ ì œê³µí•´ì£¼ì„¸ìš”.
# {context} """

qa_prompt = ChatPromptTemplate.from_messages(
    [
        ('system', qa_system_prompt),
        MessagesPlaceholder('chat_history'),
        ('human','{input}'),
    ]
)

question_answer_chain = create_stuff_documents_chain(chat, qa_prompt) # chat

# ê²°ê³¼ê°’ì€ input, chat_history, context, answer í¬í•¨í•¨.
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
# history_aware_retriever ëŒ€ì‹  ì¼ë°˜ retrieverë¡œ ë‹¤ì‹œ ì‹œë„


# ì›¹ì‚¬ì´íŠ¸ ì œëª©
st.title("êµ­íšŒ íšŒì˜ë¡ ê¸°ë°˜ ì±—ë´‡ ì„œë¹„ìŠ¤ :orange[NaraRAG] ğŸ“œâš–ï¸")

if 'messages' not in st.session_state:
        st.session_state['messages'] = [{'role': 'assistant',
                                         'content': 'ì•ˆë…•í•˜ì„¸ìš”! êµ­íšŒ íšŒì˜ë¡ì— ê´€í•´ ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë´ì£¼ì„¸ìš” ğŸ˜Š'}]

# ëŒ€í™” ë‚´ìš©ì„ ê¸°ë¡í•˜ê¸° ìœ„í•´ ì…‹ì—…
# Streamlit íŠ¹ì„±ìƒ í™œì„±í™”í•˜ì§€ ì•Šìœ¼ë©´ ë‚´ìš©ì´ ë‹¤ ë‚ ì•„ê°.
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
# í”„ë¡¬í”„íŠ¸ ë¹„ìš©ì´ ë„ˆë¬´ ë§ì´ ì†Œìš”ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´
MAX_MESSAGES_BEFORE_DELETION = 4

# ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìœ ì €ì˜ ì¸í’‹ì„ ë°›ê³  ìœ„ì—ì„œ ë§Œë“  AI ì—ì´ì „íŠ¸ ì‹¤í–‰ì‹œì¼œì„œ ë‹µë³€ ë°›ê¸°
if prompt := st.chat_input("Ask a question!"):
    
# ìœ ì €ê°€ ë³´ë‚¸ ì§ˆë¬¸ì´ë©´ ìœ ì € ì•„ì´ì½˜ê³¼ ì§ˆë¬¸ ë³´ì—¬ì£¼ê¸°
     # ë§Œì•½ í˜„ì¬ ì €ì¥ëœ ëŒ€í™” ë‚´ìš© ê¸°ë¡ì´ 4ê°œë³´ë‹¤ ë§ìœ¼ë©´ ìë¥´ê¸°
    if len(st.session_state.messages) >= MAX_MESSAGES_BEFORE_DELETION:
        # Remove the first two messages
        del st.session_state.messages[0]
        del st.session_state.messages[0]  
   
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

# AIê°€ ë³´ë‚¸ ë‹µë³€ì´ë©´ AI ì•„ì´ì½˜ì´ë‘ LLM ì‹¤í–‰ì‹œì¼œì„œ ë‹µë³€ ë°›ê³  ìŠ¤íŠ¸ë¦¬ë°í•´ì„œ ë³´ì—¬ì£¼ê¸°
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        result = rag_chain.invoke({"input": prompt, "chat_history": st.session_state.messages})

        # ì¦ê±°ìë£Œ ë³´ì—¬ì£¼ê¸°
        with st.expander("Evidence context"):

            st.write( result['context'])

        for chunk in result["answer"].split(" "):
            full_response += chunk + " "
            time.sleep(0.2)
            message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)
            
    st.session_state.messages.append({"role": "assistant", "content": full_response})

print("_______________________")
print(st.session_state.messages)
