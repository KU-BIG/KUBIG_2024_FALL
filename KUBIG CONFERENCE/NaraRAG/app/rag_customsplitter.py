import os
from dotenv import load_dotenv
load_dotenv()
import streamlit as st
import time
import uuid
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from langchain_community.chat_models import ChatOpenAI
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import ParentDocumentRetriever
from langchain_core.messages import HumanMessage, SystemMessage
from langchain.storage._lc_store import create_kv_docstore
from langchain.storage.file_system import LocalFileStore
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_community.document_transformers import LongContextReorder
from langchain.text_splitter import TextSplitter
from langchain.schema import Document

class CustomColumnSplitter(TextSplitter):
    def string_to_dict(self, input_string):
        """
        Convert a given string with key-value pairs separated by newlines into a dictionary.
        """
        result_dict = {}
        for line in input_string.split("\n"):
            if ": " in line:  # Ensure valid key-value format
                key, value = line.split(": ", 1)  # Split only on the first occurrence of ": "
                result_dict[key.strip()] = value.strip()
        return result_dict

    def split_text(self, text):
        """
        textë¥¼ ì—´ ë°ì´í„°ë¡œ ë¶„ë¦¬í•˜ì—¬ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        # í…ìŠ¤íŠ¸ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (CSV í–‰ í˜•íƒœë¡œ ê°€ì •)
        row_data = self.string_to_dict(text)  # ë¬¸ìì—´ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (Documentì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°)

        # ì—´ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
        doc1 = {
            "íšŒì˜ë‚ ì§œ": row_data.get("íšŒì˜ë‚ ì§œ"),
            "êµ­íšŒ_ëŒ€": row_data.get("êµ­íšŒ_ëŒ€"),
            "íšŒì˜ëª…": row_data.get("íšŒì˜ëª…"),
            "íšŒìˆ˜": row_data.get("íšŒìˆ˜"),
            "ì°¨ìˆ˜": row_data.get("ì°¨ìˆ˜"),
            "ì•ˆê±´": row_data.get("ì•ˆê±´"),
            "ë²•ì•ˆ": row_data.get("ë²•ì•ˆ"),
        }
        doc2 = {
            "ì§ˆë¬¸ì_ì´ë¦„": row_data.get("ì§ˆë¬¸ì_ì´ë¦„"),
            "ì§ˆë¬¸": row_data.get("ì§ˆë¬¸"),
            "ì§ˆë¬¸_í‚¤ì›Œë“œ": row_data.get("ì§ˆë¬¸_í‚¤ì›Œë“œ"),
        }
        doc3 = {
            "ë‹µë³€ì_ì´ë¦„": row_data.get("ë‹µë³€ì_ì´ë¦„"),
            "ë¬¸ë§¥(context)": row_data.get("ë¬¸ë§¥(context)"),
            "ì‹¤ì œ ë‹µë³€": row_data.get("ì‹¤ì œ ë‹µë³€"),
            "ë‹µë³€_í‚¤ì›Œë“œ": row_data.get("ë‹µë³€_í‚¤ì›Œë“œ"),
        }

        # ë°˜í™˜í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        return [doc1, doc2, doc3]

    def split_documents(self, documents):
        split_docs = []
        for document in documents:
            # ê° Documentì˜ ë‚´ìš©ì„ split_textë¡œ ì²˜ë¦¬
            sub_docs = self.split_text(document.page_content)
            for sub_doc in sub_docs:
                # ìƒˆë¡­ê²Œ ë¶„ë¦¬ëœ ë°ì´í„°ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
                split_docs.append(
                    Document(
                        page_content=str(sub_doc),  # ë¬¸ìì—´ë¡œ ë³€í™˜
                        metadata=document.metadata,  # ê¸°ì¡´ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° ìœ ì§€
                    )
                )
        return split_docs
    
if "id" not in st.session_state:
    st.session_state.id = uuid.uuid4()
    st.session_state.file_cache = {}

session_id = st.session_state.id
client = None

def reset_chat():
    st.session_state.messages = []
    st.session_state.context = None

# ChromaDB ë¡œë“œ
vectorstore = Chroma(
    collection_name="split_parents", embedding_function=UpstageEmbeddings(model="embedding-passage"), 
    persist_directory='child_DB(Chroma, Upstage, Custom2)') # DB_PATH

child_splitter = CustomColumnSplitter()

chat = ChatUpstage()
llm = ChatOpenAI()

# LocalFileStore ë¡œë“œ
fs = LocalFileStore('./parent_fs_chroma_Upstage_Custom2')
store = create_kv_docstore(fs)

# ChromaDB + LocalFileStore -> ParentDocumentRetriever
retriever = ParentDocumentRetriever(
vectorstore=vectorstore,
docstore=store,
child_splitter=child_splitter,
search_kwargs={'k':5})

# MultiQueryRetriever ë¡œë“œ
retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=chat)

# 1) ì±—ë´‡ì— 'ê¸°ì–µ'ì„ ì…íˆê¸° ìœ„í•œ ì²«ë²ˆì§¸ ë‹¨ê³„

# ì´ì „ ë©”ì‹œì§€ë“¤ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•´, ë¬¸ë§¥ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ì´ í˜¼ìì„œë§Œ ë´¤ì„ ë•Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ì„ ë‹¤ì‹œ êµ¬ì„±í•¨.
# ì¦‰ ìƒˆë¡œ ë“¤ì–´ì˜¨ ê·¸ ì§ˆë¬¸ ìì²´ì—ë§Œ ì§‘ì¤‘í•  ìˆ˜ ìˆë„ë¡ ë‹¤ì‹œ ì¬í¸ì„± (llm ì˜ˆì „ ëŒ€í™”ë¥¼ ê¸°ì–µí•´ì„œ ëŒ€í™”ë¥¼ ì¬êµ¬ì„±)
from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# ì˜ì–´ ë²„ì „
contextualize_q_system_prompt = "When there are older conversations and more recent user questions, these questions may be related to previous conversations. In this case, change the question to a question that can be understood independently without needing to know the content of the conversation. You don't have to answer the question, just reformulate it if necessary or leave it as is."


# MessagePlaceHolder: 'chat_history' ì…ë ¥ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì „ ë©”ì„¸ì§€ ê¸°ë¡ë“¤ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ì‹œí‚´. 
# ì¦‰, í”„ë¡¬í”„íŠ¸, ë©”ì„¸ì§€ ê¸°ë¡(ë¬¸ë§¥ ì •ë³´), ì‚¬ìš©ìì˜ ì§ˆë¬¸ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ê°€ êµ¬ì„±ë¨.

contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ('system', contextualize_q_system_prompt),
        MessagesPlaceholder('chat_history'),
        ('human', '{input}'),
    ]
)

# ì´ë¥¼ í† ëŒ€ë¡œ ë©”ì„¸ì§€ ê¸°ë¡ì„ ê¸°ì–µí•˜ëŠ” retrieverë¥¼ ìƒì„±. 
history_aware_retriever = create_history_aware_retriever(
    chat, retriever, contextualize_q_prompt
)

# 2) ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” retriever ì²´ì¸ ìƒì„±
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

qa_system_prompt = """
You are an intelligent assistant helping the members of the Korean National Assembly with questions related to law and policy. Read the given questions carefully and WRITE YOUR ANSWER ONLY BASED ON THE CONTEXT AND DON'T SEARCH ON THE INTERNET. Give the answer in Korean ONLY using the following pieces of the context. You must answer politely.

DO NOT TRY TO MAKE UP AN ANSWER:
Â - If the answer to the question cannot be determined from the context alone, say "I cannot determine the answer to that.".
Â - If the context is empty, just say "I do not know the answer to that.".

[You MUST answer only based on this context.]
Context: {context} """


qa_prompt = ChatPromptTemplate.from_messages(
    [
        ('system', qa_system_prompt),
        MessagesPlaceholder('chat_history'),
        ('human','{input}' + 'ë‹µë³€ì€ ì œì‹œëœ Contextì—ë§Œ ê¸°ë°˜í•´ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì¤˜. ìµœì‹  ì •ë³´ë¶€í„° ì‹œê°„ì˜ íë¦„ì— ë”°ë¼ ì‘ì„±í•´ì¤˜.'),
    ]
)

question_answer_chain = create_stuff_documents_chain(chat, qa_prompt) # chat


# ê²°ê³¼ê°’ì€ input, chat_history, context, answer í¬í•¨í•¨.
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
# history_aware_retriever ëŒ€ì‹  ì¼ë°˜ retrieverë¡œ ë‹¤ì‹œ ì‹œë„

# ì›¹ì‚¬ì´íŠ¸ ì œëª©
st.title("êµ­íšŒ íšŒì˜ë¡ ê¸°ë°˜ ì±—ë´‡ ì„œë¹„ìŠ¤ :orange[NaraRAG] ğŸ“œâš–ï¸")

if 'messages' not in st.session_state:
        st.session_state['messages'] = [{'role': 'assistant',
                                         'content': 'ì•ˆë…•í•˜ì„¸ìš”! êµ­íšŒ íšŒì˜ë¡ì— ê´€í•´ ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë´ì£¼ì„¸ìš” ğŸ˜Š'}]

# ëŒ€í™” ë‚´ìš©ì„ ê¸°ë¡í•˜ê¸° ìœ„í•´ ì…‹ì—…
# Streamlit íŠ¹ì„±ìƒ í™œì„±í™”í•˜ì§€ ì•Šìœ¼ë©´ ë‚´ìš©ì´ ë‹¤ ë‚ ì•„ê°.
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
# í”„ë¡¬í”„íŠ¸ ë¹„ìš©ì´ ë„ˆë¬´ ë§ì´ ì†Œìš”ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´
MAX_MESSAGES_BEFORE_DELETION = 4

# ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìœ ì €ì˜ ì¸í’‹ì„ ë°›ê³  ìœ„ì—ì„œ ë§Œë“  AI ì—ì´ì „íŠ¸ ì‹¤í–‰ì‹œì¼œì„œ ë‹µë³€ ë°›ê¸°
if prompt := st.chat_input("Ask a question!"):
    
# ìœ ì €ê°€ ë³´ë‚¸ ì§ˆë¬¸ì´ë©´ ìœ ì € ì•„ì´ì½˜ê³¼ ì§ˆë¬¸ ë³´ì—¬ì£¼ê¸°
     # ë§Œì•½ í˜„ì¬ ì €ì¥ëœ ëŒ€í™” ë‚´ìš© ê¸°ë¡ì´ 4ê°œë³´ë‹¤ ë§ìœ¼ë©´ ìë¥´ê¸°
    if len(st.session_state.messages) >= MAX_MESSAGES_BEFORE_DELETION:
        # Remove the first two messages
        del st.session_state.messages[0]
        del st.session_state.messages[0]  
   
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

# AIê°€ ë³´ë‚¸ ë‹µë³€ì´ë©´ AI ì•„ì´ì½˜ì´ë‘ LLM ì‹¤í–‰ì‹œì¼œì„œ ë‹µë³€ ë°›ê³  ìŠ¤íŠ¸ë¦¬ë°í•´ì„œ ë³´ì—¬ì£¼ê¸°
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        result = rag_chain.invoke({"input": prompt, "chat_history": st.session_state.messages})

        # ì¦ê±°ìë£Œ ë³´ì—¬ì£¼ê¸°
        with st.expander("Evidence context"):

            st.write( result['context'])

        for chunk in result["answer"].split(" "):
            full_response += chunk + " "
            time.sleep(0.2)
            message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)
            
    st.session_state.messages.append({"role": "assistant", "content": full_response})

print("_______________________")
print(st.session_state.messages)
