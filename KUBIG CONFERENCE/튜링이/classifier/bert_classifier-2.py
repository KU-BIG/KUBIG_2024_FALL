# -*- coding: utf-8 -*-
"""BERT_Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PbYyG8gcfDMEySny1Gvv3xRaL3HcgsHl

### Data preparation

- competition: https://www.kaggle.com/competitions/llm-detect-ai-generated-text

- notebook: https://www.kaggle.com/code/sunshine888888/llm-detect-ai-generated-bert
"""

from google.colab import drive
drive.mount('/content/drive')

import json

# JSON 파일 경로 템플릿
file_path_template = "/content/drive/MyDrive/24f_conference/data_{}.json"

# 파일 이름 리스트
list_of_names = ['generated', 'paraphrased', 'generated_2', 'paraphrased_2']

# 데이터를 저장할 딕셔너리
data_dict = {}

# JSON 파일 불러오기 루프
for name in list_of_names:
    json_file = file_path_template.format(name)
    with open(json_file, 'r', encoding='utf-8') as f:
        data_dict[name] = json.load(f)

# 로드된 데이터 확인 (예: 키만 출력)
print("데이터가 성공적으로 로드되었습니다!")
print(data_dict.keys())

generated_1 = data_dict['generated']
generated_2 = data_dict['generated_2']

paraphrased_1 = data_dict['paraphrased']
paraphrased_2 = data_dict['paraphrased_2']

import pandas as pd
pd.DataFrame(generated_1)

pd.DataFrame(paraphrased_1)

gen_2 = pd.DataFrame(generated_2)
# 데이터 이상 -> 수정완료

gen_2['original'] = gen_2['original'].str.replace('#@문장구분#', '', regex=False)

generated_2 = gen_2

pd.DataFrame(paraphrased_2)

par_2 = pd.DataFrame(paraphrased_2)
# 데이터 이상 -> 수정완료

par_2['original'] = par_2['original'].str.replace('#@문장구분#', '', regex=False)

paraphrased_2 = par_2

# df_par = pd.concat([pd.DataFrame(par_middle_1), pd.DataFrame(par_high_2), pd.DataFrame(par_high_3)])
# df_gen = pd.concat([pd.DataFrame(gen_middle_1), pd.DataFrame(gen_high_2), pd.DataFrame(gen_high_3)])

df_par = pd.concat([pd.DataFrame(paraphrased_1), pd.DataFrame(paraphrased_2)], ignore_index=True)
df_gen = pd.concat([pd.DataFrame(generated_1), pd.DataFrame(generated_2)], ignore_index=True)

df_par

df_gen

import pandas as pd

# 데이터프레임 변환
df_original = pd.DataFrame({
    'text': df_par['original'],  # original 열을 text 열로 변환
    'label': 0                   # human written에 해당하는 label
})

df_sampled = pd.DataFrame({
    'text': df_par['sampled'],   # sampled 열을 text 열로 변환
    'label': 1                   # AI generated에 해당하는 label
})

# 두 데이터프레임 합치기
df_combined = pd.concat([df_original, df_sampled], ignore_index=True)

# 결과 출력
df_combined

"""### Training"""

from transformers import BertTokenizer
model_name = 'bert-large-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)

from torch.utils.data import Dataset
import torch

class LLMDataset(Dataset):
    def __init__(self,df,is_grad,tokenizer):
        self.df = df # Pandas.DataFrame
        self.is_grad = is_grad # True: train,valid / False: test
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.df) # number of samples

    def __getitem__(self,idx):
        text = self.df.loc[idx,'text'] # cleaned인데 text로 바꿔

        encoded_dict = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            padding='max_length',
            truncation=True,
            max_length=512, # given to the max_length of tokenized text
            return_tensors='pt', # PyTorch
            return_attention_mask=True,
        )

        if self.is_grad:
            labels = self.df.loc[idx]['label']
            return {'input_ids':encoded_dict['input_ids'].squeeze(),
                    'attention_mask':encoded_dict['attention_mask'].squeeze(),
                    'token_type_ids':encoded_dict['token_type_ids'].squeeze(),
                   # Our loss_fn wants it to be a "float" type
                    'labels':torch.tensor(labels,dtype=torch.float).unsqueeze(dim=0)}
        else:
            # [batch,1,max_len(84)] -> [batch,max_len]
            return {'input_ids':encoded_dict['input_ids'].squeeze(),
                    'attention_mask':encoded_dict['attention_mask'].squeeze(),
                   'token_type_ids':encoded_dict['token_type_ids'].squeeze()}

train_df = df_combined

train_df

"""### n-gram"""

from sklearn.feature_extraction.text import CountVectorizer
def get_top_ngram(corpus, n=None):
    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx])
                  for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:10]

!apt-get update -qq
!apt-get install fonts-nanum -qq

# 캐시 초기화
import matplotlib
matplotlib.rcParams.update(matplotlib.rcParamsDefault)

# 재설정
rc('font', family=font_prop.get_name())

#AI-generated
import matplotlib
import matplotlib.pyplot as plt
from matplotlib import rc
import matplotlib.font_manager as fm

# 캐시 초기화
matplotlib.rcParams.update(matplotlib.rcParamsDefault)

# 한글 폰트 경로 설정
font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'  # Colab 환경 폰트 경로
font_prop = fm.FontProperties(fname=font_path)
rc('font', family=font_prop.get_name())

top_n_bigrams=get_top_ngram(train_df[train_df['label'] == 1]['text'],2)[:10]

x, y = map(list, zip(*top_n_bigrams))

# 한글 그래프 생성
plt.barh(x, y)
plt.xlabel('빈도수', fontsize=12)
plt.ylabel('단어', fontsize=12)
plt.title('Top 10 Bigrams', fontsize=14)
plt.show()

plt.barh(x,y)

from wordcloud import WordCloud
ai_generated_text = " ".join(train_df[train_df['label'] == 1]['text'])

wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate(ai_generated_text)

plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title('Word Cloud for AI-Generated Text')
plt.axis('off')
plt.show()

"""### classifier train"""

train_dataset = LLMDataset(train_df,True,tokenizer)
#test_dataset = LLMDataset(test,False,tokenizer)

from torch.utils.data import random_split

train_size = int(0.8 * len(train_dataset)) # train:valid = 8:2
valid_size = len(train_dataset) - train_size

train_dataset,valid_dataset = random_split(train_dataset,[train_size,valid_size])
print(f'{len(train_dataset)} train samples')
print(f'{len(valid_dataset)} valid samples')

from torch.utils.data import DataLoader

train_dataloader = DataLoader(train_dataset,batch_size=8,shuffle=True,pin_memory=True)#锁页内存(pin_memory)能够保持与GPU进行高速传输,在训练时加快数据的读取
valid_dataloader = DataLoader(valid_dataset,batch_size=8,shuffle=False,pin_memory=True)

valid_eval_dataset = LLMDataset(train_df[train_size:].reset_index(drop=False),False,tokenizer)
valid_eval_dataloader = DataLoader(valid_eval_dataset,batch_size=1,shuffle=False,pin_memory=True)

import torch
import torch.nn as nn

configs = {
    'model_name':'bert-large-uncased',
    'num_labels':2,
    'batch_size':8,
    'epochs':4,
    'learning_rate':5e-6,
}

import numpy as np
import torch
import torch.nn as nn
from transformers import BertForSequenceClassification

# Never Detach Tensor during forward
class LLMModel(nn.Module):
    '''
    To be honest, under the setting like this, there is no need to inherit.
    It's because I used "BertForSequenceClassification" which has final layer
    that is composed of "hidden size 2" for binary classification.

    So, you can think of this unnecessary inheritance is kind of "practice" for myself :)
    '''
    def __init__(self,model_name):
        super().__init__()
        self.model = BertForSequenceClassification.from_pretrained(model_name)

    def forward(self,input_ids,attention_mask):
        output = self.model(input_ids=input_ids,attention_mask=attention_mask)
        logits = output.logits
        return logits

if torch.cuda.is_available():
    device = 'cuda'
    print('GPU is running on..')
else:
    device = 'cpu'
    print('CPU is running on..')

model = LLMModel(configs['model_name']).to(device)

# loss function
loss_fn = nn.CrossEntropyLoss()
# optimizer
from transformers import AdamW

optimizer = AdamW(model.parameters(),
                lr=6e-6,
                eps=1e-8,
                no_deprecation_warning=True)

# metric for validation
# f1_score(y_label,y_pred)
from sklearn.metrics import f1_score

metric = f1_score

import gc,os
from tqdm.auto import tqdm # visualizing tool for progress

# They will be used to pick the best model.pt given to the valid loss
best_model_epoch, valid_loss_values = [],[]
valid_loss_min = [1] # arbitrary loss I set here
def train(model,device,train_dataloader,valid_dataloader,epochs,loss_fn,optimizer,metric):

    for epoch in range(epochs):
        gc.collect()
        model.train()

        train_loss = 0
        train_step = 0
        pbar = tqdm(train_dataloader)

        for batch in pbar: # you can also write like "for batch in tqdm(train_dataloader"
            optimizer.zero_grad() # initialize
            train_step += 1

            train_input_ids = batch['input_ids'].to(device)
            train_attention_mask = batch['attention_mask'].to(device)
            train_labels = batch['labels'].squeeze().to(device).long()

            # You can refer to the class "TweetsModel" for understand
            # what would be logits
            logits = model(train_input_ids, train_attention_mask).to(device)
            predictions = torch.argmax(logits, dim=1) # get an index from larger one
            detached_predictions = predictions.detach().cpu().numpy()

            loss = loss_fn(logits, train_labels)
            loss.backward()
            optimizer.step()
            model.zero_grad()

            train_loss += loss.detach().cpu().numpy().item()

            pbar.set_postfix({'train_loss':train_loss/train_step})
        pbar.close()

        with torch.no_grad():
            model.eval()

            valid_loss = 0
            valid_step = 0
            total_valid_score = 0

            y_pred = [] # for getting f1_score that is a metric of the competition
            y_true = []

            pbar = tqdm(valid_dataloader)
            for batch in pbar:
                valid_step += 1

                valid_input_ids = batch['input_ids'].to(device)
                valid_attention_mask = batch['attention_mask'].to(device)
                valid_labels = batch['labels'].squeeze().to(device).long()

                logits = model(valid_input_ids, valid_attention_mask).to(device)
                predictions = torch.argmax(logits, dim=1)
                detached_predictions = predictions.detach().cpu().numpy()

                loss = loss_fn(logits, valid_labels)
                valid_loss += loss.detach().cpu().numpy().item()

                y_pred.extend(predictions.cpu().numpy())
                y_true.extend(valid_labels.cpu().numpy())

            valid_loss /= valid_step
            f1 = f1_score(y_true,y_pred)

            print(f'Epoch [{epoch+1}/{epochs}] Score: {f1}')
            print(f'Epoch [{epoch+1}/{epochs}] Valid_loss: {valid_loss}')

            if valid_loss < min(valid_loss_min):
                print('model improved!')
            else:
                print('model not improved')

            torch.save(model.state_dict(), f'epoch:{epoch+1}_model.pt')
            print('save checkpoint!')
            valid_loss_min.append(valid_loss)
            print(f'valid_loss_min:{min(valid_loss_min)}')

        best_model_epoch.append(f'/kaggle/working/epoch:{epoch+1}_model.pt')
        valid_loss_values.append(valid_loss)
        print('='*100)

    select_best_model() # refer to below function
    print('Train/Valid Completed!!')
    del train_dataloader, valid_dataloader # memory cleaning
    gc.collect()

def select_best_model():
    best_model = best_model_epoch[np.array(valid_loss_values).argmin()]
    os.rename(best_model, best_model.split('.pt')[0] + '_best.pt')

print(f"Before training, files in current directory: '/content/drive/MyDrive/24f_conference'")

output_dir = '/content/drive/MyDrive/24f_conference'
os.makedirs(output_dir, exist_ok=True)

print(f"Before training, files in current directory: '/content/drive/MyDrive/24f_conference'")

import os

# 원하는 디렉토리 경로
desired_dir = '/kaggle/working/'

# 디렉토리가 없으면 생성
if not os.path.exists(desired_dir):
    os.makedirs(desired_dir, exist_ok=True)  # 중첩 디렉토리도 생성 가능

# 디렉토리로 이동
os.chdir(desired_dir)
current_dir = os.getcwd()

print(f"Current working directory: {current_dir}")

print(f"Current directory: {current_dir}")

import os
import shutil

# 현재 디렉터리의 파일 및 폴더 삭제
current_dir = os.getcwd()  # 현재 작업 디렉터리
files_and_dirs = os.listdir(current_dir)

for item in files_and_dirs:
    item_path = os.path.join(current_dir, item)
    if os.path.isfile(item_path):  # 파일인 경우 삭제
        os.remove(item_path)
    elif os.path.isdir(item_path) and item != '.virtual_documents':  # 폴더인 경우, 특정 폴더 제외하고 삭제
        shutil.rmtree(item_path)

print('Training Start!')
print('=' * 100)

train(model,
    device,
    train_dataloader,
    valid_dataloader,
    configs['epochs'],
    loss_fn,
    optimizer,
    metric)

del model,train_dataloader, valid_dataloader
gc.collect()

"""### Inference

- 고3 맞춤법 검사를 다시 하려고 하니 GPT 4o mini 불러오는데서 자꾸 에러남..
- 시간 여유 있을 때 고치기로 하고 우선은! 원문으로 데이터를 만들어보겠슴다
"""

df2 = pd.read_csv('/content/drive/MyDrive/24f_conference/merged_high2.csv')
df2.head()

"""### test_par inference"""

test_human = pd.DataFrame({
    'text': df2['Correction Text'].values[:100],
    'label': 0,
})

test_par = pd.DataFrame({
    'text': df2['Paraphrased Text'],
    'label': 1,
})

test_gen = pd.DataFrame({
    'text': df2['Generated Texts'],
    'label': 1,
})

test = pd.concat([test_human, test_par], ignore_index=True)

test

test_dataset = LLMDataset(test,False,tokenizer)

test_dataloader = DataLoader(test_dataset,batch_size=1,shuffle=True,pin_memory=True)

def inference(model,test_dataloader):
    all_preds = []
    model.eval()

    with torch.no_grad():
        for batch in tqdm(test_dataloader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)


            logits = model(input_ids,attention_mask)
            logits = logits.detach().cpu().numpy()
            all_preds.append(logits)

    return all_preds

def inference(model,test_dataloader):
    all_preds = []
    model.eval()

    with torch.no_grad():
        for batch in tqdm(test_dataloader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)


            logits = model(input_ids,attention_mask)
            logits = logits.detach().cpu().numpy()
            all_preds.append(logits)

    return all_preds

for filename in os.listdir():
    if 'best.pt' in filename:
        best_pt = filename
print(f'Best model.pt: {best_pt}')
check_point = torch.load(best_pt)

# We have to load a model again because I deleted after training/validation
model = LLMModel(configs['model_name']).to(device)
model.to(device)
model.load_state_dict(check_point)

train_df[train_size:]

pre_valid = inference(model,valid_eval_dataloader)

pre_valid = np.argmax(pre_valid,axis=2)
y_valid = train_df.label[train_size:].reset_index(drop=True)

from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, roc_curve, auc, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_valid, pre_valid)
roc_auc = auc(fpr, tpr)
print(roc_auc)

plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)  # 绘制ROC曲线
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc="lower right")
plt.grid(color='purple', linestyle='--')
plt.show()

ConfusionMatrixDisplay.from_predictions(y_valid,pre_valid, colorbar=True, display_labels=["0", "1"],
                                         cmap=plt.cm.Reds)
plt.title("Confusion Matrix")
plt.show()

"""- precision = 1
- recall = 0.7

- f1 = 0.8
"""

# Pick up the model.pt written with the best
# which has the lowest validation loss through all Epochs.

for filename in os.listdir():
    if 'best.pt' in filename:
        best_pt = filename
print(f'Best model.pt: {best_pt}')
check_point = torch.load(best_pt)

# We have to load a model again because I deleted after training/validation
model = LLMModel(configs['model_name']).to(device)
model.to(device)
model.load_state_dict(check_point)

predictions = inference(model,test_dataloader)

predictions

import torch.nn.functional as F
pre_test = F.softmax(torch.tensor(predictions),dtype=torch.float32,dim=-1)
prob_test = pre_test[:,0,1]
prob_test

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# 주어진 데이터를 NumPy 배열로 변환
predictions = [
    [-0.8577405, -0.12318206], [-0.8923772, -0.1388393], [0.06697523, -0.9703131],
    [-0.12108108, -0.7816031], [0.09632953, -1.0208162], [0.10167781, -1.0441594],
    [-0.90941507, -0.15118481], [0.12638877, -1.0726198], [0.07517684, -0.98688346],
    [0.10319884, -1.0318954], [-0.8886194, -0.20255135], [0.1405771, -1.0933079],
    [0.07775594, -1.0047345], [-0.8170634, -0.14231405], [-0.35386306, -0.5941275],
    [0.12120832, -1.0733576], [-0.4531254, -0.48439997], [-0.7713832, -0.18529668],
    [0.03255523, -0.919378], [-0.5987472, -0.33489975], [-0.5662496, -0.3900757],
    [-0.5219947, -0.4069038], [-0.7286323, -0.19187537], [0.11557709, -1.0625135],
    [0.12867746, -1.0716965], [-0.8327775, -0.24154358], [-0.90908194, -0.15837467],
    [-0.5490756, -0.38361973], [0.09625102, -1.00303], [0.13770944, -1.093851],
    [0.07262956, -0.9835292], [-0.16599184, -0.7540842], [0.0594646, -0.9518013],
    [-0.6946187, -0.22442214], [0.00814188, -0.9199305], [-0.00423892, -0.88083184],
    [-0.8969785, -0.15604812], [-0.8566941, -0.18151738], [-0.5647518, -0.36347437],
    [0.04952459, -0.93237555], [0.0478357, -0.94232833], [-0.7583205, -0.1760194],
    [-0.07482353, -0.82689285], [0.113528, -1.0437398], [-0.22098933, -0.69588417],
    [-0.8129329, -0.144018], [0.08158815, -1.0006363], [0.09348307, -1.0224729],
    [0.00875484, -0.90690804], [-0.09695853, -0.8076317], [-0.16399029, -0.74317],
    [0.10418044, -1.0244927], [0.06304727, -0.9816797], [-0.03584171, -0.85747516],
    [0.1394743, -1.0907671], [-0.8630394, -0.13294938], [0.12032998, -1.0457156],
    [0.02680063, -0.9136043], [0.07568148, -0.98903406], [-0.4000895, -0.54068136],
    [-0.86884, -0.13326104], [0.10486285, -1.0389574], [0.04089389, -0.83653796],
    [-0.9067268, -0.17339739], [-0.8183646, -0.13432226], [0.14883539, -1.1143157],
    [-0.9034563, -0.15866172], [0.0723821, -0.97837055], [-0.88866603, -0.21903124],
    [-0.4533915, -0.49501345], [-0.02440746, -0.8333707], [0.07695773, -0.9999156],
    [0.0236226, -0.92305815], [-0.32238087, -0.6054914], [0.04941288, -0.956704],
    [0.02491056, -0.9210066], [-0.8943172, -0.16240643], [-0.83588785, -0.2423386],
    [-0.907108, -0.18276122], [-0.00317378, -0.8817533], [-0.9586022, -0.20799956],
    [0.13414696, -1.0778065], [-0.8873871, -0.16197482], [-0.9097845, -0.19006124],
    [0.00922443, -0.7262848], [-0.9081868, -0.14933857], [0.08860628, -1.0134423],
    [0.09247176, -1.0348918], [0.06606271, -0.98575383], [-0.72916853, -0.19331378],
    [-0.44332024, -0.49377826], [-0.9113528, -0.15902442], [-0.8838335, -0.14474772],
    [-0.41710702, -0.51707685], [0.09304073, -1.0275416], [-0.8044167, -0.1378927],
    [-0.8233497, -0.140299], [0.1436659, -1.1235923], [0.10066277, -1.027285],
    [0.1063564, -1.0448548], [-0.8954503, -0.15065947], [0.0447536, -0.92774224],
    [-0.28888467, -0.64142495], [0.09522133, -1.026624], [0.05745569, -0.9844588],
    [-0.66164345, -0.25796446], [-0.9024646, -0.23195297], [0.1108919, -1.033081],
    [0.11849535, -1.0481575], [-0.8429275, -0.29006132], [-0.04904468, -0.8534993],
    [-0.49040368, -0.45105293], [0.02733389, -0.92804056], [-0.88387954, -0.20639005],
    [0.11585831, -1.0355246], [0.09660996, -1.0163794], [-0.81043625, -0.28184903],
    [-0.895341, -0.18373036], [0.01735102, -0.9196458], [-0.15743431, -0.75578964],
    [-0.3821505, -0.5500165], [0.07320037, -0.98450315], [-0.9176545, -0.16265379],
    [0.07952712, -0.9995779], [-0.19113977, -0.7282228], [0.12656301, -1.0736552],
    [-0.00642695, -0.8951467], [-0.7469276, -0.1785583], [-0.8884306, -0.1895167],
    [-0.89860016, -0.15561856], [-0.80150354, -0.16102761], [-1.0024083, -0.16841982],
    [0.01712213, -0.9028145], [-0.71336615, -0.22726491], [-0.91217387, -0.18036988],
    [-0.901595, -0.13888058], [-0.17833993, -0.74093497], [-0.9093888, -0.18341616],
    [-0.83341306, -0.13101661], [-0.7269045, -0.19140515], [0.08363643, -0.99299514],
    [-0.15953502, -0.75321966], [0.13746238, -1.0851778], [-0.92755085, -0.25287178],
    [0.1543438, -1.1507124], [0.07672627, -0.97047], [0.06173235, -0.9613471],
    [0.00845766, -0.8859023], [0.14809479, -1.1130853], [-0.5870421, -0.35804752],
    [0.07924899, -0.9828454], [-0.90241945, -0.13936564], [-0.5270817, -0.4034013],
    [0.02948917, -0.8054286], [0.00088918436, -0.90039778], [-0.22426262, -0.6915102],
    [-0.91030395, -0.18957935], [-0.06761786, -0.83046883], [-0.8995627, -0.18718517],
    [-0.2504929, -0.6739478], [-0.91421187, -0.17056197], [0.05766894, -0.936264],
    [0.13592637, -1.0942242], [-0.1988325, -0.71686566], [0.0668235, -0.9463913],
    [-0.9123245, -0.16101517], [-0.90832037, -0.15108679], [0.01441833, -0.9153403],
    [-0.896025, -0.1408298], [-0.89367586, -0.1751177], [-0.86178327, -0.14472301],
    [-0.93730724, -0.29964596], [-0.91294885, -0.18539926], [-0.8982589, -0.14661902],
    [-0.18081933, -0.7324319], [0.06662812, -0.99118984], [-0.9025974, -0.19541277],
    [-0.86171156, -0.34160864], [-0.8967975, -0.16882953], [-0.08404862, -0.8170179],
    [-0.8663018, -0.19386835], [-0.89619553, -0.18504225], [-0.24402608, -0.68305564],
    [0.05438079, -0.92804044], [0.02026718, -0.89508784], [-0.89684576, -0.12882482],
    [0.08393621, -1.0097497], [-0.04734483, -0.8488654], [-0.89108086, -0.15324657],
    [-0.4342129, -0.49828014], [-0.8942263, -0.15603264], [-0.8868878, -0.20497571],
    [0.036459, -0.89115787], [-0.90698683, -0.16968362], [0.0640032, -0.9601257],
    [0.15913802, -1.1405096], [-0.84427935, -0.13861597], [0.0857171, -1.0127336],
    [-0.91445976, -0.1676295], [0.08546432, -0.9943142]
]

# 왼쪽 값과 오른쪽 값 분리
left_values = [item[0] for item in predictions]
right_values = [item[1] for item in predictions]

# KDE 플롯 생성
plt.figure(figsize=(12, 6))

# 왼쪽 값 KDE 플롯
plt.subplot(1, 2, 1)
sns.kdeplot(left_values, shade=True)
plt.title('KDE Plot for Left Values')
plt.xlabel('Value')
plt.ylabel('Density')

# 오른쪽 값 KDE 플롯
plt.subplot(1, 2, 2)
sns.kdeplot(right_values, shade=True)
plt.title('KDE Plot for Right Values')
plt.xlabel('Value')
plt.ylabel('Density')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# 왼쪽 값과 오른쪽 값 분리
left_values = [item[0] for item in predictions]
right_values = [item[1] for item in predictions]

# KDE 플롯 생성
plt.figure(figsize=(8, 6))

# 왼쪽 값 KDE 플롯
sns.kdeplot(left_values, shade=True, label='Left Values', color='blue')

# 오른쪽 값 KDE 플롯
sns.kdeplot(right_values, shade=True, label='Right Values', color='orange')

# 제목 및 레이블 추가
plt.title('Overlaid KDE Plot for Left and Right Values')
plt.xlabel('Value')
plt.ylabel('Density')

# 범례 추가
plt.legend()

# 플롯 표시
plt.show()





"""### test_gen inference"""

test2 = pd.concat([test_human, test_gen], ignore_index=True)

test2_dataset = LLMDataset(test2,False,tokenizer)

test2_dataloader = DataLoader(test2_dataset,batch_size=1,shuffle=True,pin_memory=True)

def inference(model,test2_dataloader):
    all_preds = []
    model.eval()

    with torch.no_grad():
        for batch in tqdm(test2_dataloader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)


            logits = model(input_ids,attention_mask)
            logits = logits.detach().cpu().numpy()
            all_preds.append(logits)

    return all_preds

for filename in os.listdir():
    if 'best.pt' in filename:
        best_pt = filename
print(f'Best model.pt: {best_pt}')
check_point = torch.load(best_pt)

# We have to load a model again because I deleted after training/validation
model = LLMModel(configs['model_name']).to(device)
model.to(device)
model.load_state_dict(check_point)

train_df[train_size:]

pre_valid = inference(model,valid_eval_dataloader)

pre_valid = np.argmax(pre_valid,axis=2)
y_valid = train_df.label[train_size:].reset_index(drop=True)

# Pick up the model.pt written with the best
# which has the lowest validation loss through all Epochs.

for filename in os.listdir():
    if 'best.pt' in filename:
        best_pt = filename
print(f'Best model.pt: {best_pt}')
check_point = torch.load(best_pt)

# We have to load a model again because I deleted after training/validation
model = LLMModel(configs['model_name']).to(device)
model.to(device)
model.load_state_dict(check_point)

predictions2 = inference(model,test2_dataloader)

predictions2

import torch.nn.functional as F
pre_test2 = F.softmax(torch.tensor(predictions2),dtype=torch.float32,dim=-1)
prob_test2 = pre_test2[:,0,1]
prob_test2

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# 주어진 데이터를 NumPy 배열로 변환
import numpy as np

# 데이터를 NumPy 배열로 변환
predictions2 = [
    [-1.094043, -0.4201019],
    [0.17741667, -0.5257949],
    [-1.1390232, -0.5186972],
    [-1.2005651, -0.45024252],
    [0.49684617, -0.5882021],
    [0.683173, -0.42650282],
    [-0.50692505, -0.22850974],
    [-1.1898028, -0.43062642],
    [-1.1566821, -0.5097938],
    [-1.146428, -0.5096783],
    [0.72686094, -0.61327016],
    [-1.1544454, -0.5228448],
    [-1.1326805, -0.5368425],
    [-1.1473323, -0.53822744],
    [-1.2290314, -0.42484388],
    [0.5785866, -0.5806955],
    [0.26827085, -0.35267523],
    [0.2180052, -0.503572],
    [-1.0021648, -0.5153796],
    [-0.20220385, -0.1640831],
    [-1.1917115, -0.5005135],
    [-1.1899763, -0.4987305],
    [-1.1842939, -0.4987481],
    [-1.1701094, -0.4416985],
    [-0.00652002, -0.2730799],
    [-1.1929044, -0.49548706],
    [-1.238635, -0.43379882],
    [-1.1454548, -0.54161364],
    [-1.1587554, -0.52130336],
    [-1.1549815, -0.5427083],
    [0.6592707, -0.6575748],
    [-1.1882733, -0.49410877],
    [-1.2040384, -0.4928266],
    [0.56329614, -0.6220799],
    [-1.1970252, -0.47658804],
    [-1.1305457, -0.54881614],
    [-1.0281838, -0.46807173],
    [-1.1564707, -0.52240103],
    [-1.1906968, -0.4863139],
    [-0.2766135, -0.48278853],
    [0.12992892, -0.22143911],
    [-1.1612817, -0.511345],
    [0.4341621, -0.5452991],
    [-0.00336889, -0.22770454],
    [-1.145413, -0.47370943],
    [-1.1350132, -0.5387972],
    [-1.1704204, -0.51329476],
    [-1.1292608, -0.42652252],
    [0.15115659, -0.44448623],
    [0.49020883, -0.5601959],
    [-1.1901172, -0.4704372],
    [0.11494175, -0.5036478],
    [0.34467998, -0.2584967],
    [-0.3238945, -0.13353625],
    [0.08244575, -0.3110926],
    [-0.838653, -0.39856794],
    [-1.1264049, -0.4839358],
    [-1.147563, -0.53661346],
    [-1.1732134, -0.4570487],
    [-1.1646161, -0.5223655],
    [0.31557566, -0.28692552],
    [-1.1411299, -0.5251456],
    [-1.1309334, -0.5420912],
    [0.7323972, -0.6705746],
    [-1.0974442, -0.5100656],
    [-0.77738917, -0.458896],
    [-1.1723028, -0.52578324],
    [-1.1462134, -0.56000286],
    [0.12411537, -0.27865997],
    [-1.1504769, -0.53365177],
    [-0.0941794, -0.20547943],
    [-1.0858909, -0.3885376],
    [-1.1623622, -0.48427352],
    [-1.1410662, -0.5441894],
    [-1.0968894, -0.4254993],
    [-1.1308903, -0.52250093],
    [0.82960075, -0.62647146],
    [-1.1891019, -0.48041353],
    [0.23771256, -0.5501904],
    [-1.1883391, -0.44927657],
    [0.05748709, -0.1554442],
    [-1.1521813, -0.53721094],
    [-1.1762749, -0.49233016],
    [-1.1406449, -0.54119056],
    [-1.1150571, -0.5515674],
    [-1.162805, -0.53731555],
    [0.3372545, -0.52659583],
    [-0.14924842, -0.40339306],
    [-1.0323906, -0.51468116],
    [-1.205353, -0.45417094],
    [-0.37221903, -0.4507413],
    [-1.1986519, -0.47977296],
    [0.5785718, -0.44399387],
    [-0.3415703, -0.50463265],
    [-1.1780921, -0.52047426],
    [-0.06480266, -0.13653618],
    [-1.1469318, -0.54121464],
    [-1.1593337, -0.515698],
    [-1.1542054, -0.4645258],
    [0.12915221, -0.1797751],
    [-0.5179871, -0.49787426],
    [0.7286164, -0.67167467],
    [-0.18901671, -0.53010434],
    [0.80996054, -0.6294252],
    [0.21489768, -0.5468372],
    [0.38801137, -0.54432946],
    [0.03317192, -0.4692725],
    [-1.2100695, -0.48700845],
    [0.19195306, -0.549036],
    [-1.1399189, -0.501172],
    [-1.1862606, -0.5132742],
    [0.5348259, -0.5670122],
    [0.45929924, -0.33816352],
    [0.8262105, -0.6751416],
    [0.09450286, -0.23684366],
    [-0.95553094, -0.34442624],
    [-1.1279793, -0.5226511],
    [0.13898249, -0.52985644],
    [-1.1755968, -0.49760112],
    [-1.1712388, -0.48714474],
    [-1.1786231, -0.46028894],
    [-0.18725203, -0.06635585],
    [-1.1790177, -0.50950754],
    [-1.1354178, -0.4760491],
    [-1.1763312, -0.5064161],
    [0.67778516, -0.39632833],
    [-1.1044202, -0.5062471],
    [-1.161013, -0.4950624],
    [-0.5972938, -0.4821021],
    [-0.82240844, -0.34798893],
    [-1.1896491, -0.4217846],
    [-1.1438688, -0.5374012],
    [-1.1075844, -0.5423808],
    [-1.1568002, -0.519429],
    [-0.13314584, -0.461191],
    [-1.1467503, -0.52637047],
    [-1.1789719, -0.48481074],
    [-1.154366, -0.51148814],
    [-1.1854501, -0.48867193],
    [0.9491041, -0.6564711],
    [-1.1769818, -0.49733752],
    [-1.2077545, -0.4359],
    [0.7446813, -0.6565061],
    [-1.1794559, -0.49180534],
    [-0.046756, -0.5277125],
    [-0.87836987, -0.4437156],
    [-1.1911079, -0.50572646],
    [-1.0162673, -0.49874002],
    [0.3695872, -0.56815374],
    [-1.1793269, -0.43299088],
    [-1.1074729, -0.5080157],
    [0.70180225, -0.61947024],
    [-1.1769534, -0.50774616],
    [-1.0997183, -0.46624324],
    [-1.125039, -0.56342465],
    [-1.1413071, -0.52637905],
    [-1.147551, -0.52482873],
    [-1.17157, -0.4857771],
    [-1.1558874, -0.502322],
    [-1.1382962, -0.4677203],
    [-1.1539065, -0.52906144],
    [-1.1542811, -0.5371551],
    [-0.2532119, -0.56561],
    [0.7085927, -0.5037527],
    [-1.0817597, -0.46259418],
    [-0.41553482, -0.42207795],
    [-1.1123811, -0.5461306],
    [0.94019663, -0.6177347],
    [-0.00217387, -0.5215295],
    [-1.146461, -0.53400064],
    [-1.1796843, -0.48763272],
    [0.42201275, -0.6111714],
    [-0.10201149, -0.4580011],
    [-0.97299695, -0.44551826],
    [0.7036826, -0.6383491],
    [-1.1854357, -0.5099408],
    [-1.1854624, -0.4520508],
    [0.10293711, -0.5372711],
    [-0.09255066, -0.4186459],
    [-1.1748642, -0.49953732],
    [-1.1880618, -0.47409704],
    [-0.34768465, -0.1921114],
    [-1.1724597, -0.47432217],
    [-1.1752898, -0.48420048],
    [-1.154481, -0.53978467],
    [-1.0741073, -0.3872194],
    [-1.1507865, -0.52537626],
    [0.09415339, -0.45484853],
    [-0.7639337, -0.5092054],
    [0.7103325, -0.62306535],
    [0.2002558, -0.5283853],
    [-1.1557018, -0.5542439],
    [-1.1967791, -0.46827272],
    [-1.1578363, -0.51127535],
    [-1.1608393, -0.5210945],
    [-1.1924067, -0.4783125],
    [-0.93089354, -0.4160287],
    [-1.1519617, -0.5237102],
    [-1.1836319, -0.41490075],
    [-1.157437, -0.47011697]
]

print(predictions)

# 왼쪽 값과 오른쪽 값 분리
left_values = [item[0] for item in predictions2]
right_values = [item[1] for item in predictions2]

# KDE 플롯 생성
plt.figure(figsize=(12, 6))

# 왼쪽 값 KDE 플롯
plt.subplot(1, 2, 1)
sns.kdeplot(left_values, shade=True)
plt.title('KDE Plot for Left Values')
plt.xlabel('Value')
plt.ylabel('Density')

# 오른쪽 값 KDE 플롯
plt.subplot(1, 2, 2)
sns.kdeplot(right_values, shade=True)
plt.title('KDE Plot for Right Values')
plt.xlabel('Value')
plt.ylabel('Density')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# 왼쪽 값과 오른쪽 값 분리
left_values = [item[0] for item in predictions2]
right_values = [item[1] for item in predictions2]

# KDE 플롯 생성
plt.figure(figsize=(8, 6))

# 왼쪽 값 KDE 플롯
sns.kdeplot(left_values, shade=True, label='Left Values', color='blue')

# 오른쪽 값 KDE 플롯
sns.kdeplot(right_values, shade=True, label='Right Values', color='orange')

# 제목 및 레이블 추가
plt.title('Overlaid KDE Plot for Left and Right Values')
plt.xlabel('Value')
plt.ylabel('Density')

# 범례 추가
plt.legend()

# 플롯 표시
plt.show()

