# -*- coding: utf-8 -*-
"""241220_Kaggle_Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PbYyG8gcfDMEySny1Gvv3xRaL3HcgsHl

### Data preparation

- competition: https://www.kaggle.com/competitions/llm-detect-ai-generated-text

- notebook: https://www.kaggle.com/code/sunshine888888/llm-detect-ai-generated-bert
"""

from google.colab import drive
drive.mount('/content/drive')

import json

# JSON 파일 경로 템플릿
file_path_template = "/content/drive/MyDrive/24f_conference/data_{}.json"

# 파일 이름 리스트
list_of_names = ['generated', 'paraphrased', 'generated_2', 'paraphrased_2']

# 데이터를 저장할 딕셔너리
data_dict = {}

# JSON 파일 불러오기 루프
for name in list_of_names:
    json_file = file_path_template.format(name)
    with open(json_file, 'r', encoding='utf-8') as f:
        data_dict[name] = json.load(f)

# 로드된 데이터 확인 (예: 키만 출력)
print("데이터가 성공적으로 로드되었습니다!")
print(data_dict.keys())

generated_1 = data_dict['generated']
generated_2 = data_dict['generated_2']

paraphrased_1 = data_dict['paraphrased']
paraphrased_2 = data_dict['paraphrased_2']

import pandas as pd
pd.DataFrame(generated_1)

pd.DataFrame(paraphrased_1)

gen_2 = pd.DataFrame(generated_2)
# 데이터 이상 -> 수정완료

gen_2['original'] = gen_2['original'].str.replace('#@문장구분#', '', regex=False)

generated_2 = gen_2

pd.DataFrame(paraphrased_2)

par_2 = pd.DataFrame(paraphrased_2)
# 데이터 이상 -> 수정완료

par_2['original'] = par_2['original'].str.replace('#@문장구분#', '', regex=False)

paraphrased_2 = par_2

"""- high 3가 original text가 이상함..
- 다시 생성하려고 하니까 `proxies`? 에러가 뜸..
- 일단 high 3 제외하고 코드가 진행되는지만 확인해보자..
"""

# df_par = pd.concat([pd.DataFrame(par_middle_1), pd.DataFrame(par_high_2), pd.DataFrame(par_high_3)])
# df_gen = pd.concat([pd.DataFrame(gen_middle_1), pd.DataFrame(gen_high_2), pd.DataFrame(gen_high_3)])

df_par = pd.concat([pd.DataFrame(paraphrased_1), pd.DataFrame(paraphrased_2)], ignore_index=True)
df_gen = pd.concat([pd.DataFrame(generated_1), pd.DataFrame(generated_2)], ignore_index=True)

df_par

df_gen



import pandas as pd

# 데이터프레임 변환
df_original = pd.DataFrame({
    'text': df_par['original'],  # original 열을 text 열로 변환
    'label': 0                   # human written에 해당하는 label
})

df_sampled = pd.DataFrame({
    'text': df_par['sampled'],   # sampled 열을 text 열로 변환
    'label': 1                   # AI generated에 해당하는 label
})

# 두 데이터프레임 합치기
df_combined = pd.concat([df_original, df_sampled], ignore_index=True)

# 결과 출력
df_combined

"""- notebook에 텍스트 전처리 과정이 나오긴 함
- human written과 ai generated 텍스트들에 대해서 정규식 이용해서 전처리함
- 근데 우리는 llm으로 맞춤법 검사를 이미 진행하기도 했고(정규식x), 한국어 데이터이기 때문에 또 다른 점이 있을 것을 감안하면..
- 이전에 맞춤법 검사로 이미 전처리가 진행되었다고 가정하고 시작

### Training
"""

from transformers import BertTokenizer
model_name = 'bert-large-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)

from torch.utils.data import Dataset
import torch

class LLMDataset(Dataset):
    def __init__(self,df,is_grad,tokenizer):
        self.df = df # Pandas.DataFrame
        self.is_grad = is_grad # True: train,valid / False: test
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.df) # number of samples

    def __getitem__(self,idx):
        text = self.df.loc[idx,'text'] # cleaned인데 text로 바꿔

        encoded_dict = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            padding='max_length',
            truncation=True,
            max_length=512, # given to the max_length of tokenized text
            return_tensors='pt', # PyTorch
            return_attention_mask=True,
        )

        if self.is_grad:
            labels = self.df.loc[idx]['label']
            return {'input_ids':encoded_dict['input_ids'].squeeze(),
                    'attention_mask':encoded_dict['attention_mask'].squeeze(),
                    'token_type_ids':encoded_dict['token_type_ids'].squeeze(),
                   # Our loss_fn wants it to be a "float" type
                    'labels':torch.tensor(labels,dtype=torch.float).unsqueeze(dim=0)}
        else:
            # [batch,1,max_len(84)] -> [batch,max_len]
            return {'input_ids':encoded_dict['input_ids'].squeeze(),
                    'attention_mask':encoded_dict['attention_mask'].squeeze(),
                   'token_type_ids':encoded_dict['token_type_ids'].squeeze()}

train_df = df_combined

train_df

"""### n-gram"""

from sklearn.feature_extraction.text import CountVectorizer
def get_top_ngram(corpus, n=None):
    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx])
                  for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:10]

!apt-get update -qq
!apt-get install fonts-nanum -qq

# 캐시 초기화
import matplotlib
matplotlib.rcParams.update(matplotlib.rcParamsDefault)

# 다시 설정
rc('font', family=font_prop.get_name())

#AI-generated
import matplotlib
import matplotlib.pyplot as plt
from matplotlib import rc
import matplotlib.font_manager as fm

# 캐시 초기화
matplotlib.rcParams.update(matplotlib.rcParamsDefault)

# 한글 폰트 경로 설정
font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'  # Colab 환경 폰트 경로
font_prop = fm.FontProperties(fname=font_path)
rc('font', family=font_prop.get_name())

top_n_bigrams=get_top_ngram(train_df[train_df['label'] == 1]['text'],2)[:10]

x, y = map(list, zip(*top_n_bigrams))

# 한글 그래프 생성
plt.barh(x, y)
plt.xlabel('빈도수', fontsize=12)
plt.ylabel('단어', fontsize=12)
plt.title('Top 10 Bigrams', fontsize=14)
plt.show()

plt.barh(x,y)

from wordcloud import WordCloud
ai_generated_text = " ".join(train_df[train_df['label'] == 1]['text'])

wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white').generate(ai_generated_text)

plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.title('Word Cloud for AI-Generated Text')
plt.axis('off')
plt.show()

"""### classifier train"""

train_dataset = LLMDataset(train_df,True,tokenizer)
#test_dataset = LLMDataset(test,False,tokenizer)

from torch.utils.data import random_split

train_size = int(0.8 * len(train_dataset)) # train:valid = 8:2
valid_size = len(train_dataset) - train_size

train_dataset,valid_dataset = random_split(train_dataset,[train_size,valid_size])
print(f'{len(train_dataset)} train samples')
print(f'{len(valid_dataset)} valid samples')

from torch.utils.data import DataLoader

train_dataloader = DataLoader(train_dataset,batch_size=8,shuffle=True,pin_memory=True)#锁页内存(pin_memory)能够保持与GPU进行高速传输,在训练时加快数据的读取
valid_dataloader = DataLoader(valid_dataset,batch_size=8,shuffle=False,pin_memory=True)

valid_eval_dataset = LLMDataset(train_df[train_size:].reset_index(drop=False),False,tokenizer)
valid_eval_dataloader = DataLoader(valid_eval_dataset,batch_size=1,shuffle=False,pin_memory=True)

import torch
import torch.nn as nn

configs = {
    'model_name':'bert-large-uncased',
    'num_labels':2,
    'batch_size':8,
    'epochs':4,
    'learning_rate':5e-6,
}

import numpy as np
import torch
import torch.nn as nn
from transformers import BertForSequenceClassification

# Never Detach Tensor during forward
class LLMModel(nn.Module):
    '''
    To be honest, under the setting like this, there is no need to inherit.
    It's because I used "BertForSequenceClassification" which has final layer
    that is composed of "hidden size 2" for binary classification.

    So, you can think of this unnecessary inheritance is kind of "practice" for myself :)
    '''
    def __init__(self,model_name):
        super().__init__()
        self.model = BertForSequenceClassification.from_pretrained(model_name)

    def forward(self,input_ids,attention_mask):
        output = self.model(input_ids=input_ids,attention_mask=attention_mask)
        logits = output.logits
        return logits

if torch.cuda.is_available():
    device = 'cuda'
    print('GPU is running on..')
else:
    device = 'cpu'
    print('CPU is running on..')

model = LLMModel(configs['model_name']).to(device)

# loss function
loss_fn = nn.CrossEntropyLoss()
# optimizer
from transformers import AdamW

optimizer = AdamW(model.parameters(),
                lr=6e-6,
                eps=1e-8,
                no_deprecation_warning=True)

# metric for validation
# f1_score(y_label,y_pred)
from sklearn.metrics import f1_score

metric = f1_score

import gc,os
from tqdm.auto import tqdm # visualizing tool for progress

# They will be used to pick the best model.pt given to the valid loss
best_model_epoch, valid_loss_values = [],[]
valid_loss_min = [1] # arbitrary loss I set here
def train(model,device,train_dataloader,valid_dataloader,epochs,loss_fn,optimizer,metric):

    for epoch in range(epochs):
        gc.collect()
        model.train()

        train_loss = 0
        train_step = 0
        pbar = tqdm(train_dataloader)

        for batch in pbar: # you can also write like "for batch in tqdm(train_dataloader"
            optimizer.zero_grad() # initialize
            train_step += 1

            train_input_ids = batch['input_ids'].to(device)
            train_attention_mask = batch['attention_mask'].to(device)
            train_labels = batch['labels'].squeeze().to(device).long()

            # You can refer to the class "TweetsModel" for understand
            # what would be logits
            logits = model(train_input_ids, train_attention_mask).to(device)
            predictions = torch.argmax(logits, dim=1) # get an index from larger one
            detached_predictions = predictions.detach().cpu().numpy()

            loss = loss_fn(logits, train_labels)
            loss.backward()
            optimizer.step()
            model.zero_grad()

            train_loss += loss.detach().cpu().numpy().item()

            pbar.set_postfix({'train_loss':train_loss/train_step})
        pbar.close()

        with torch.no_grad():
            model.eval()

            valid_loss = 0
            valid_step = 0
            total_valid_score = 0

            y_pred = [] # for getting f1_score that is a metric of the competition
            y_true = []

            pbar = tqdm(valid_dataloader)
            for batch in pbar:
                valid_step += 1

                valid_input_ids = batch['input_ids'].to(device)
                valid_attention_mask = batch['attention_mask'].to(device)
                valid_labels = batch['labels'].squeeze().to(device).long()

                logits = model(valid_input_ids, valid_attention_mask).to(device)
                predictions = torch.argmax(logits, dim=1)
                detached_predictions = predictions.detach().cpu().numpy()

                loss = loss_fn(logits, valid_labels)
                valid_loss += loss.detach().cpu().numpy().item()

                y_pred.extend(predictions.cpu().numpy())
                y_true.extend(valid_labels.cpu().numpy())

            valid_loss /= valid_step
            f1 = f1_score(y_true,y_pred)

            print(f'Epoch [{epoch+1}/{epochs}] Score: {f1}')
            print(f'Epoch [{epoch+1}/{epochs}] Valid_loss: {valid_loss}')

            if valid_loss < min(valid_loss_min):
                print('model improved!')
            else:
                print('model not improved')

            torch.save(model.state_dict(), f'epoch:{epoch+1}_model.pt')
            print('save checkpoint!')
            valid_loss_min.append(valid_loss)
            print(f'valid_loss_min:{min(valid_loss_min)}')

        best_model_epoch.append(f'/kaggle/working/epoch:{epoch+1}_model.pt')
        valid_loss_values.append(valid_loss)
        print('='*100)

    select_best_model() # refer to below function
    print('Train/Valid Completed!!')
    del train_dataloader, valid_dataloader # memory cleaning
    gc.collect()

def select_best_model():
    best_model = best_model_epoch[np.array(valid_loss_values).argmin()]
    os.rename(best_model, best_model.split('.pt')[0] + '_best.pt')

print(f"Before training, files in current directory: '/content/drive/MyDrive/24f_conference'")

output_dir = '/content/drive/MyDrive/24f_conference'
os.makedirs(output_dir, exist_ok=True)

print(f"Before training, files in current directory: '/content/drive/MyDrive/24f_conference'")

import os

# 원하는 디렉토리 경로
desired_dir = '/kaggle/working/'

# 디렉토리가 없으면 생성
if not os.path.exists(desired_dir):
    os.makedirs(desired_dir, exist_ok=True)  # 중첩 디렉토리도 생성 가능

# 디렉토리로 이동
os.chdir(desired_dir)
current_dir = os.getcwd()

print(f"Current working directory: {current_dir}")

print(f"Current directory: {current_dir}")

import os
import shutil

# 현재 디렉터리의 파일 및 폴더 삭제
current_dir = os.getcwd()  # 현재 작업 디렉터리
files_and_dirs = os.listdir(current_dir)

for item in files_and_dirs:
    item_path = os.path.join(current_dir, item)
    if os.path.isfile(item_path):  # 파일인 경우 삭제
        os.remove(item_path)
    elif os.path.isdir(item_path) and item != '.virtual_documents':  # 폴더인 경우, 특정 폴더 제외하고 삭제
        shutil.rmtree(item_path)

print('Training Start!')
print('=' * 100)

train(model,
    device,
    train_dataloader,
    valid_dataloader,
    configs['epochs'],
    loss_fn,
    optimizer,
    metric)

del model,train_dataloader, valid_dataloader
gc.collect()

"""### Inference

- 고3 맞춤법 검사를 다시 하려고 하니 GPT 4o mini 불러오는데서 자꾸 에러남..
- 시간 여유 있을 때 고치기로 하고 우선은! 원문으로 데이터를 만들어보겠슴다
"""

df2 = pd.read_csv('/content/drive/MyDrive/24f_conference/merged_high2.csv')
df2.head()

"""### test_par inference"""

test_human = pd.DataFrame({
    'text': df2['Correction Text'].values[:100],
    'label': 0,
})

test_par = pd.DataFrame({
    'text': df2['Paraphrased Text'],
    'label': 1,
})

test_gen = pd.DataFrame({
    'text': df2['Generated Texts'],
    'label': 1,
})

test = pd.concat([test_human, test_par], ignore_index=True)

test

"""- 앞에 train에서 human written이 중복되서 두번씩 들어갔을 것이라는 것을 지금 깨달음..
- 그래서 일단 여기서는 test_par만 ai written으로 넣기
"""

test_dataset = LLMDataset(test,False,tokenizer)

test_dataloader = DataLoader(test_dataset,batch_size=1,shuffle=True,pin_memory=True)

def inference(model,test_dataloader):
    all_preds = []
    model.eval()

    with torch.no_grad():
        for batch in tqdm(test_dataloader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)


            logits = model(input_ids,attention_mask)
            logits = logits.detach().cpu().numpy()
            all_preds.append(logits)

    return all_preds

def inference(model,test_dataloader):
    all_preds = []
    model.eval()

    with torch.no_grad():
        for batch in tqdm(test_dataloader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)


            logits = model(input_ids,attention_mask)
            logits = logits.detach().cpu().numpy()
            all_preds.append(logits)

    return all_preds

for filename in os.listdir():
    if 'best.pt' in filename:
        best_pt = filename
print(f'Best model.pt: {best_pt}')
check_point = torch.load(best_pt)

# We have to load a model again because I deleted after training/validation
model = LLMModel(configs['model_name']).to(device)
model.to(device)
model.load_state_dict(check_point)

train_df[train_size:]

pre_valid = inference(model,valid_eval_dataloader)

pre_valid = np.argmax(pre_valid,axis=2)
y_valid = train_df.label[train_size:].reset_index(drop=True)

from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, roc_curve, auc, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_valid, pre_valid)
roc_auc = auc(fpr, tpr)
print(roc_auc)

plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)  # 绘制ROC曲线
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc="lower right")
plt.grid(color='purple', linestyle='--')
plt.show()

ConfusionMatrixDisplay.from_predictions(y_valid,pre_valid, colorbar=True, display_labels=["0", "1"],
                                         cmap=plt.cm.Reds)
plt.title("Confusion Matrix")
plt.show()

"""- 이렇게만 보면 precision = 1
- recall = 0.7

- f1 = 0.8
"""

# Pick up the model.pt written with the best
# which has the lowest validation loss through all Epochs.

for filename in os.listdir():
    if 'best.pt' in filename:
        best_pt = filename
print(f'Best model.pt: {best_pt}')
check_point = torch.load(best_pt)

# We have to load a model again because I deleted after training/validation
model = LLMModel(configs['model_name']).to(device)
model.to(device)
model.load_state_dict(check_point)

predictions = inference(model,test_dataloader)

predictions

import torch.nn.functional as F
pre_test = F.softmax(torch.tensor(predictions),dtype=torch.float32,dim=-1)
prob_test = pre_test[:,0,1]
prob_test

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# 주어진 데이터를 NumPy 배열로 변환
predictions = [
    [-0.8577405, -0.12318206], [-0.8923772, -0.1388393], [0.06697523, -0.9703131],
    [-0.12108108, -0.7816031], [0.09632953, -1.0208162], [0.10167781, -1.0441594],
    [-0.90941507, -0.15118481], [0.12638877, -1.0726198], [0.07517684, -0.98688346],
    [0.10319884, -1.0318954], [-0.8886194, -0.20255135], [0.1405771, -1.0933079],
    [0.07775594, -1.0047345], [-0.8170634, -0.14231405], [-0.35386306, -0.5941275],
    [0.12120832, -1.0733576], [-0.4531254, -0.48439997], [-0.7713832, -0.18529668],
    [0.03255523, -0.919378], [-0.5987472, -0.33489975], [-0.5662496, -0.3900757],
    [-0.5219947, -0.4069038], [-0.7286323, -0.19187537], [0.11557709, -1.0625135],
    [0.12867746, -1.0716965], [-0.8327775, -0.24154358], [-0.90908194, -0.15837467],
    [-0.5490756, -0.38361973], [0.09625102, -1.00303], [0.13770944, -1.093851],
    [0.07262956, -0.9835292], [-0.16599184, -0.7540842], [0.0594646, -0.9518013],
    [-0.6946187, -0.22442214], [0.00814188, -0.9199305], [-0.00423892, -0.88083184],
    [-0.8969785, -0.15604812], [-0.8566941, -0.18151738], [-0.5647518, -0.36347437],
    [0.04952459, -0.93237555], [0.0478357, -0.94232833], [-0.7583205, -0.1760194],
    [-0.07482353, -0.82689285], [0.113528, -1.0437398], [-0.22098933, -0.69588417],
    [-0.8129329, -0.144018], [0.08158815, -1.0006363], [0.09348307, -1.0224729],
    [0.00875484, -0.90690804], [-0.09695853, -0.8076317], [-0.16399029, -0.74317],
    [0.10418044, -1.0244927], [0.06304727, -0.9816797], [-0.03584171, -0.85747516],
    [0.1394743, -1.0907671], [-0.8630394, -0.13294938], [0.12032998, -1.0457156],
    [0.02680063, -0.9136043], [0.07568148, -0.98903406], [-0.4000895, -0.54068136],
    [-0.86884, -0.13326104], [0.10486285, -1.0389574], [0.04089389, -0.83653796],
    [-0.9067268, -0.17339739], [-0.8183646, -0.13432226], [0.14883539, -1.1143157],
    [-0.9034563, -0.15866172], [0.0723821, -0.97837055], [-0.88866603, -0.21903124],
    [-0.4533915, -0.49501345], [-0.02440746, -0.8333707], [0.07695773, -0.9999156],
    [0.0236226, -0.92305815], [-0.32238087, -0.6054914], [0.04941288, -0.956704],
    [0.02491056, -0.9210066], [-0.8943172, -0.16240643], [-0.83588785, -0.2423386],
    [-0.907108, -0.18276122], [-0.00317378, -0.8817533], [-0.9586022, -0.20799956],
    [0.13414696, -1.0778065], [-0.8873871, -0.16197482], [-0.9097845, -0.19006124],
    [0.00922443, -0.7262848], [-0.9081868, -0.14933857], [0.08860628, -1.0134423],
    [0.09247176, -1.0348918], [0.06606271, -0.98575383], [-0.72916853, -0.19331378],
    [-0.44332024, -0.49377826], [-0.9113528, -0.15902442], [-0.8838335, -0.14474772],
    [-0.41710702, -0.51707685], [0.09304073, -1.0275416], [-0.8044167, -0.1378927],
    [-0.8233497, -0.140299], [0.1436659, -1.1235923], [0.10066277, -1.027285],
    [0.1063564, -1.0448548], [-0.8954503, -0.15065947], [0.0447536, -0.92774224],
    [-0.28888467, -0.64142495], [0.09522133, -1.026624], [0.05745569, -0.9844588],
    [-0.66164345, -0.25796446], [-0.9024646, -0.23195297], [0.1108919, -1.033081],
    [0.11849535, -1.0481575], [-0.8429275, -0.29006132], [-0.04904468, -0.8534993],
    [-0.49040368, -0.45105293], [0.02733389, -0.92804056], [-0.88387954, -0.20639005],
    [0.11585831, -1.0355246], [0.09660996, -1.0163794], [-0.81043625, -0.28184903],
    [-0.895341, -0.18373036], [0.01735102, -0.9196458], [-0.15743431, -0.75578964],
    [-0.3821505, -0.5500165], [0.07320037, -0.98450315], [-0.9176545, -0.16265379],
    [0.07952712, -0.9995779], [-0.19113977, -0.7282228], [0.12656301, -1.0736552],
    [-0.00642695, -0.8951467], [-0.7469276, -0.1785583], [-0.8884306, -0.1895167],
    [-0.89860016, -0.15561856], [-0.80150354, -0.16102761], [-1.0024083, -0.16841982],
    [0.01712213, -0.9028145], [-0.71336615, -0.22726491], [-0.91217387, -0.18036988],
    [-0.901595, -0.13888058], [-0.17833993, -0.74093497], [-0.9093888, -0.18341616],
    [-0.83341306, -0.13101661], [-0.7269045, -0.19140515], [0.08363643, -0.99299514],
    [-0.15953502, -0.75321966], [0.13746238, -1.0851778], [-0.92755085, -0.25287178],
    [0.1543438, -1.1507124], [0.07672627, -0.97047], [0.06173235, -0.9613471],
    [0.00845766, -0.8859023], [0.14809479, -1.1130853], [-0.5870421, -0.35804752],
    [0.07924899, -0.9828454], [-0.90241945, -0.13936564], [-0.5270817, -0.4034013],
    [0.02948917, -0.8054286], [0.00088918436, -0.90039778], [-0.22426262, -0.6915102],
    [-0.91030395, -0.18957935], [-0.06761786, -0.83046883], [-0.8995627, -0.18718517],
    [-0.2504929, -0.6739478], [-0.91421187, -0.17056197], [0.05766894, -0.936264],
    [0.13592637, -1.0942242], [-0.1988325, -0.71686566], [0.0668235, -0.9463913],
    [-0.9123245, -0.16101517], [-0.90832037, -0.15108679], [0.01441833, -0.9153403],
    [-0.896025, -0.1408298], [-0.89367586, -0.1751177], [-0.86178327, -0.14472301],
    [-0.93730724, -0.29964596], [-0.91294885, -0.18539926], [-0.8982589, -0.14661902],
    [-0.18081933, -0.7324319], [0.06662812, -0.99118984], [-0.9025974, -0.19541277],
    [-0.86171156, -0.34160864], [-0.8967975, -0.16882953], [-0.08404862, -0.8170179],
    [-0.8663018, -0.19386835], [-0.89619553, -0.18504225], [-0.24402608, -0.68305564],
    [0.05438079, -0.92804044], [0.02026718, -0.89508784], [-0.89684576, -0.12882482],
    [0.08393621, -1.0097497], [-0.04734483, -0.8488654], [-0.89108086, -0.15324657],
    [-0.4342129, -0.49828014], [-0.8942263, -0.15603264], [-0.8868878, -0.20497571],
    [0.036459, -0.89115787], [-0.90698683, -0.16968362], [0.0640032, -0.9601257],
    [0.15913802, -1.1405096], [-0.84427935, -0.13861597], [0.0857171, -1.0127336],
    [-0.91445976, -0.1676295], [0.08546432, -0.9943142]
]

# 왼쪽 값과 오른쪽 값 분리
left_values = [item[0] for item in predictions]
right_values = [item[1] for item in predictions]

# KDE 플롯 생성
plt.figure(figsize=(12, 6))

# 왼쪽 값 KDE 플롯
plt.subplot(1, 2, 1)
sns.kdeplot(left_values, shade=True)
plt.title('KDE Plot for Left Values')
plt.xlabel('Value')
plt.ylabel('Density')

# 오른쪽 값 KDE 플롯
plt.subplot(1, 2, 2)
sns.kdeplot(right_values, shade=True)
plt.title('KDE Plot for Right Values')
plt.xlabel('Value')
plt.ylabel('Density')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# 왼쪽 값과 오른쪽 값 분리
left_values = [item[0] for item in predictions]
right_values = [item[1] for item in predictions]

# KDE 플롯 생성
plt.figure(figsize=(8, 6))

# 왼쪽 값 KDE 플롯
sns.kdeplot(left_values, shade=True, label='Left Values', color='blue')

# 오른쪽 값 KDE 플롯
sns.kdeplot(right_values, shade=True, label='Right Values', color='orange')

# 제목 및 레이블 추가
plt.title('Overlaid KDE Plot for Left and Right Values')
plt.xlabel('Value')
plt.ylabel('Density')

# 범례 추가
plt.legend()

# 플롯 표시
plt.show()

"""predictions = inference(model,test_dataloader)


- model 예측값(오른쪽 플롯)
- test 값 (왼쪽 플롯)

근데 왜 modal이 두개가 나오지?
"""



"""### 총평,,
생각보다 학습시킨 모델 성능이 좋은 듯
- f1, confusion matrix에서의 성능으로 봤을때에 한해서.
- roc 커브?

**고려해야 할 점
- kde plot이 다름! 이거 비교해도 괜찮을 것 같구 (mode)
- paraphrased 말고 gen 까지 해보고
- wordcloud 값도 내서 gen의 최상위 단어들 유의미하면 뽑아봐도 좋을듯 -> 요건 전처리 하고 뽑아야할듯
"""



"""### test_gen inference"""

test2 = pd.concat([test_human, test_gen], ignore_index=True)

test2_dataset = LLMDataset(test2,False,tokenizer)

test2_dataloader = DataLoader(test2_dataset,batch_size=1,shuffle=True,pin_memory=True)