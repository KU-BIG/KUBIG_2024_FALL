{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ae958e85570843efb478af8a810b74fd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6b5b83efdc4407d888a0aa2b4fcd523","IPY_MODEL_68d1d36421304dd5b48875a3e6690a9c","IPY_MODEL_fd8ed7cc21b947188c96bce4d154143d"],"layout":"IPY_MODEL_7282489e7e98441bbbf5b747065edb83"}},"e6b5b83efdc4407d888a0aa2b4fcd523":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fcde97a4b984ad19d440595af8a4311","placeholder":"​","style":"IPY_MODEL_e768c45d981749949d05f26c1bd7849b","value":"Loading checkpoint shards: 100%"}},"68d1d36421304dd5b48875a3e6690a9c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8794fbdc382c47b9bb82fdfd4c8d09fa","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f719f63398e4252b4cc1d2aca68e7fc","value":28}},"fd8ed7cc21b947188c96bce4d154143d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c42ec9bf7404c7c9cc44ca5ccbafab9","placeholder":"​","style":"IPY_MODEL_788e705090cd4509a5d92656db392606","value":" 28/28 [02:14&lt;00:00,  3.88s/it]"}},"7282489e7e98441bbbf5b747065edb83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fcde97a4b984ad19d440595af8a4311":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e768c45d981749949d05f26c1bd7849b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8794fbdc382c47b9bb82fdfd4c8d09fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f719f63398e4252b4cc1d2aca68e7fc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1c42ec9bf7404c7c9cc44ca5ccbafab9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"788e705090cd4509a5d92656db392606":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# koalpaca : 한국어 버전의 alpca 모델"],"metadata":{"id":"Z-RmFNKc3sKc"}},{"cell_type":"markdown","source":["alpaca : llama라는 모델을 instruction fine-tuning 해서 사람의 지시를 잘 따르는 모델\n","\n","llama라는 모델은 LLaMA-13B의 경우 GPT-3보다 10배이상 작음에도 불구하고 대부분의 평가서 GPT-3보다 우수한 성능을 보이며, 더 나아가 LLaMA-65B의 경우 대부분의 벤치마크에서 Chinchilla, Gopher, GPT-3, PaLM와 유사하거나 더 뛰어난 결과를 보이는 훌륭한 모델입니다.\n","\n","그러면 지금부터 koalpaca로 생성 모델을 한번 경험해보도록 하겠습니다!"],"metadata":{"id":"fsnGgar7gx83"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"7crJC7X_eb1v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723012082355,"user_tz":-540,"elapsed":537,"user":{"displayName":"이유진","userId":"11630789315296928592"}},"outputId":"c449f4ee-2f7c-49ab-add4-bc65ca5df671"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Aug  7 06:28:02 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   67C    P0              30W /  70W |   7943MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["# GPU 켜져 있는지 확인\n","\n","!nvidia-smi"]},{"cell_type":"code","source":["!pip install -q -U bitsandbytes\n","!pip install -q -U git+https://github.com/huggingface/transformers.git"],"metadata":{"id":"o186Sm7Uef5V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723012108381,"user_tz":-540,"elapsed":25014,"user":{"displayName":"이유진","userId":"11630789315296928592"}},"outputId":"c157c639-c296-408c-fc90-c8b18722a2f0"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["!pip install -q -U git+https://github.com/huggingface/accelerate.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h-UQkE-Nfr-d","executionInfo":{"status":"ok","timestamp":1723012130252,"user_tz":-540,"elapsed":21874,"user":{"displayName":"이유진","userId":"11630789315296928592"}},"outputId":"fa8c5548-8668-4e36-d6ff-a9dfc04d7bc7"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["!pip install -q datasets"],"metadata":{"id":"mgaipVRPeYp3","executionInfo":{"status":"ok","timestamp":1723012135200,"user_tz":-540,"elapsed":4959,"user":{"displayName":"이유진","userId":"11630789315296928592"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"],"metadata":{"id":"VgCKqAXMekYz","executionInfo":{"status":"ok","timestamp":1723012135200,"user_tz":-540,"elapsed":3,"user":{"displayName":"이유진","userId":"11630789315296928592"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# peft\n","peft는 parameter efficient fine-tuning의 약자로, 큰 모델을 코랩에서 돌릴 수 있도록 쪼개서 만든 것입니다.\n","\n","오래 걸립니다!"],"metadata":{"id":"syg88baNin2b"}},{"cell_type":"code","source":["from peft import PeftModel, PeftConfig"],"metadata":{"id":"GlZv-twPerjy","executionInfo":{"status":"ok","timestamp":1723012135200,"user_tz":-540,"elapsed":2,"user":{"displayName":"이유진","userId":"11630789315296928592"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# 효율적인 추론을 위해 사전 학습되고 미세 조정된 언어 모델을 준비하는 과정!!\n","# 양자화를 통해 메모리 사용을 줄이는 데 중점을 둔다.\n","\n","peft_model_id = \"beomi/qlora-koalpaca-polyglot-12.8b-50step\" # 파라미터 효율적인 미세조정(PEFT) 기술을 사용한 사전학습된 모델의 식별자 정의\n","config = PeftConfig.from_pretrained(peft_model_id) #perfit_model_id로 설정된 PEFT 설정을 사용해서 모델에 대한 구성 객체를 초기화함. 모델 아키텍처와 미세 조정 세부 정보를 위한 관련 설정과 매개변수 로드\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True, # 모델을 4비트 정밀도로 로드하도록 지정, 메모리 사용량 줄임.\n","    bnb_4bit_use_double_quant=True, # 이중 양자화를 활성화하여 정밀도 손실을 줄이는 데 도움\n","    bnb_4bit_quant_type=\"nf4\", # nf4라는 특정 양자화 형식을 사용하도록 지정\n","    bnb_4bit_compute_dtype=torch.bfloat16 #계산 데이터 형식을 bfloat16으로 설정, 메모리 사용량 줄이면서 성능 유지하는 데 도움.\n",")\n","model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, quantization_config=bnb_config, device_map={\"\":0})\n","\n","# 사전학습된 모델 config.base_model_name_or_path로부터 인과언어모델링을 위한 기본 모델 초기화\n","# 모델은 지정된 양자화 설정(bnb_config)으로 로드되고 디바이스맵에 할당된다.\n","# device_map={\"\":0}: 첫번째 GPU에 할당\n","\n","model = PeftModel.from_pretrained(model, peft_model_id)\n","\n","# 사전학습된 PEFT 모델을 사용해서 기본 모델(model)을 PEFT로 감쌈.\n","# peft 모델에서 학습된 추가 미세 조정 매개변수와 기술을 기본 모델에 통합해서 모델이 특수한 훈련을 활용할 수 있도록 한다.\n","\n","tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n","\n","# 사전학습된 config.base_model_name_or_path에 해당하는 토크나이저를 초기화\n","# 토크나이저 : 텍스트를 모델이 처리할 수 있는 토큰 ID로 변환하고 모델의 출력을 다시 텍스트로 변환한다.\n","\n","model.eval()\n","# 모델을 평가 모드로 설정."],"metadata":{"id":"gYf0UQ3fetzP","colab":{"base_uri":"https://localhost:8080/","height":830,"referenced_widgets":["ae958e85570843efb478af8a810b74fd","e6b5b83efdc4407d888a0aa2b4fcd523","68d1d36421304dd5b48875a3e6690a9c","fd8ed7cc21b947188c96bce4d154143d","7282489e7e98441bbbf5b747065edb83","6fcde97a4b984ad19d440595af8a4311","e768c45d981749949d05f26c1bd7849b","8794fbdc382c47b9bb82fdfd4c8d09fa","5f719f63398e4252b4cc1d2aca68e7fc","1c42ec9bf7404c7c9cc44ca5ccbafab9","788e705090cd4509a5d92656db392606"]},"executionInfo":{"status":"ok","timestamp":1723012276843,"user_tz":-540,"elapsed":141645,"user":{"displayName":"이유진","userId":"11630789315296928592"}},"outputId":"8e32cbdc-73f0-429c-a0ce-4a14b7284f44"},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/28 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae958e85570843efb478af8a810b74fd"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): GPTNeoXForCausalLM(\n","      (gpt_neox): GPTNeoXModel(\n","        (embed_in): Embedding(30080, 5120)\n","        (emb_dropout): Dropout(p=0.0, inplace=False)\n","        (layers): ModuleList(\n","          (0-39): 40 x GPTNeoXLayer(\n","            (input_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n","            (post_attention_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n","            (post_attention_dropout): Dropout(p=0.0, inplace=False)\n","            (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n","            (attention): GPTNeoXSdpaAttention(\n","              (rotary_emb): GPTNeoXRotaryEmbedding()\n","              (query_key_value): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=5120, out_features=15360, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=5120, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=15360, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (dense): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","              (attention_dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (mlp): GPTNeoXMLP(\n","              (dense_h_to_4h): Linear4bit(in_features=5120, out_features=20480, bias=True)\n","              (dense_4h_to_h): Linear4bit(in_features=20480, out_features=5120, bias=True)\n","              (act): GELUActivation()\n","            )\n","          )\n","        )\n","        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (embed_out): Linear(in_features=5120, out_features=30080, bias=False)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["next token generation"],"metadata":{"id":"ViGa1Tvyi8fI"}},{"cell_type":"code","source":["\n","prompt = '인간처럼 생각하고, 행동하는 \\'지능\\'을 통해 인류가 이제까지 풀지 못했던'\n","# 생성할 텍스트의 시작점으로 사용될 입력 문장 정의.\n","\n","with torch.no_grad(): # 자동 미분 비활성화 _ 모델을 평가 모드로 사용해서 메모리 절약, 계산속도 높임\n","  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n","  # 입력 문장을 토크나이저로 토큰화해서 토큰 ID로 변환, Pytorch 텐서로 반환\n","  # non-blocking=True : GPU와 CPU 간의 데이터 전송이 비동기적으로 이루어져서 처리속도가 빨라지도록 함.\n","\n","  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=64)\n","  #모델 사용해서 텍스트 생성\n","  # do_sample : 확률적으로 다음 단어를 선택해서 다양한 텍스트 출력을 생성하도록 한다.\n","  # temp : 샘플링의 랜덤성 조절, 값이 높을수록 더 무작위\n","  # max length : 생성할 최대 토큰 수\n","\n","\n","  generated = tokenizer.batch_decode(gen_tokens)[0]\n","  # 생성된 토큰 ID 배열(gen_tokens)을 원래의 텍스트로 디코딩해서 사람이 읽을 수 있는 문자열로 변환\n","  # batch_decode : 여러 샘플을 동시에 디코딩 할 수 있지만 우리는 단일 샘플만 생성하므로 첫번째 결과[0]만 추출.\n","\n","print(generated)"],"metadata":{"id":"sHacSUa3fgz0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723012283596,"user_tz":-540,"elapsed":6759,"user":{"displayName":"이유진","userId":"11630789315296928592"}},"outputId":"5da5e98e-1fe5-43a7-e43b-b8b42eafbdf0"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["인간처럼 생각하고, 행동하는 '지능'을 통해 인류가 이제까지 풀지 못했던 문제를 해결하고,또다른 차원의 삶을 열어갈 수 있다는 것이다.<|endoftext|>\n"]}]},{"cell_type":"markdown","source":["# QA"],"metadata":{"id":"zO4qeMpZi54_"}},{"cell_type":"code","source":["def gen(x):\n","    q = f\"### 질문: {x}\\n\\n### 답변:\"\n","    # print(q)\n","    gened = model.generate(\n","        **tokenizer(\n","            q,\n","            return_tensors='pt', #q를 토큰화하고 파이토치 텐서로 반환\n","            return_token_type_ids=False #토큰 타입 id 반환 x하도록 설정\n","        ).to('cuda'),\n","        max_new_tokens=50,\n","        early_stopping=True, #모델이 종료 토큰id(eos_token_id)를 생성하면 조기 종료. 더이상 토큰 생성x\n","        do_sample=True, #확률적으로 다음 토큰을 샘플링해서 출력을 다채롭게\n","        eos_token_id=2, #종료 토큰 id를 2로 설정.\n","    )\n","    print(tokenizer.decode(gened[0]))"],"metadata":{"id":"0JBJKYJcfLuX","executionInfo":{"status":"ok","timestamp":1723012283596,"user_tz":-540,"elapsed":4,"user":{"displayName":"이유진","userId":"11630789315296928592"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["gen('건강하게 살기 위한 세 가지 방법은?')"],"metadata":{"id":"55GdkP9afi6U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723012294285,"user_tz":-540,"elapsed":10692,"user":{"displayName":"이유진","userId":"11630789315296928592"}},"outputId":"7396bfc3-580f-4236-e977-c10e35df6f91"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["### 질문: 건강하게 살기 위한 세 가지 방법은?\n","\n","### 답변: (1) 몸을 움직이는 방법. 움직이지 않고 지내면 살찌기 쉽다. (2) 걷거나 달리거나 몸을 움직이는 기회를 많이 갖는 방법. (3) 생활습관의 개선. 즉 적당히 움직이\n"]}]},{"cell_type":"markdown","source":["# max_new_tokens = 100으로 변화."],"metadata":{"id":"5Lx957dMjvDF"}},{"cell_type":"code","source":["def gen(x):\n","    q = f\"### 질문: {x}\\n\\n### 답변:\"\n","    # print(q)\n","    gened = model.generate(\n","        **tokenizer(\n","            q,\n","            return_tensors='pt', #q를 토큰화하고 파이토치 텐서로 반환\n","            return_token_type_ids=False #토큰 타입 id 반환 x하도록 설정\n","        ).to('cuda'),\n","        max_new_tokens=100,\n","        early_stopping=True, #모델이 종료 토큰id(eos_token_id)를 생성하면 조기 종료. 더이상 토큰 생성x\n","        do_sample=True, #확률적으로 다음 토큰을 샘플링해서 출력을 다채롭게\n","        eos_token_id=2, #종료 토큰 id를 2로 설정.\n","    )\n","    print(tokenizer.decode(gened[0]))"],"metadata":{"id":"WT_s4jvvjq7C","executionInfo":{"status":"ok","timestamp":1723012971715,"user_tz":-540,"elapsed":1083,"user":{"displayName":"이유진","userId":"11630789315296928592"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["gen('건강하게 살기 위한 세 가지 방법은?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5VQ0Ec_8jtoU","executionInfo":{"status":"ok","timestamp":1723012990573,"user_tz":-540,"elapsed":13534,"user":{"displayName":"이유진","userId":"11630789315296928592"}},"outputId":"6a7539fd-b840-48f6-e38a-66202269de1a"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["### 질문: 건강하게 살기 위한 세 가지 방법은?\n","\n","### 답변: 안녕하세요. 대구피부과추천 세브란스피부과 입니다. 오늘 대구피부과추천 세브란스피부과에서 건강하게 살기위한 건강법  에 대해서 알려드리도록 하겠습니다. 먼저 세가지 건강법이란 첫번째는 올바른 식사법을 말합니다. 두번째는 적절한 운동법 세번째는 적절한 휴식법을 들 수 있습니다. 오늘 대구피부과추천 세브란스피부과에서 말하는 건강하게\n"]}]},{"cell_type":"markdown","source":["# top k 변화\n","- 텍스트 생성 과정에서 사용되는 샘플링 기법 중 하나\n","- 모델이 다음 토큰을 선택할 때 사용할 수 있는 후보 토큰의 개수를 제한하는 역할을 한다.\n","\n","- topk = 50 : 가장 확률이 높은 50개의 토큰 중에서 다음 토큰을 샘플링한다. 모델은 50개의 후보군 중에서 랜덤하게 토큰을 선택.\n","생성과정에서 확률이 높은 상위 50개의 토큰만 선택가능하고 이는 생성된 텍스트가 더 신뢰성 있게 한다.  "],"metadata":{"id":"VjacO_6njJWR"}},{"cell_type":"code","source":["prompt = '인간처럼 생각하고, 행동하는 \\'지능\\'을 통해 인류가 이제까지 풀지 못했던'\n","with torch.no_grad():\n","  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n","  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=64, top_k=50)\n","  generated = tokenizer.batch_decode(gen_tokens)[0]\n","\n","print(generated)"],"metadata":{"id":"8JaD2vpPfkuU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723012298444,"user_tz":-540,"elapsed":4173,"user":{"displayName":"이유진","userId":"11630789315296928592"}},"outputId":"0a7552e3-4d78-4533-97de-bd318fea751d"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["인간처럼 생각하고, 행동하는 '지능'을 통해 인류가 이제까지 풀지 못했던 어려운 난제들을 해결하고 있다.​​​​<|endoftext|>\n"]}]},{"cell_type":"code","source":["prompt = '인간처럼 생각하고, 행동하는 \\'지능\\'을 통해 인류가 이제까지 풀지 못했던'\n","with torch.no_grad():\n","  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n","  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=64, top_k=3)\n","  generated = tokenizer.batch_decode(gen_tokens)[0]\n","\n","print(generated)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FUQ7j8rkiw1j","executionInfo":{"status":"ok","timestamp":1723013041219,"user_tz":-540,"elapsed":9494,"user":{"displayName":"이유진","userId":"11630789315296928592"}},"outputId":"e3db2279-2c0b-4641-e2b1-f0f7f9d578d2"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["인간처럼 생각하고, 행동하는 '지능'을 통해 인류가 이제까지 풀지 못했던 '의식'이라는 문제를 해결하고자 한다. ​​그렇다면 과연 미래에는 인간의 의식에 어떤 변화가 일어날까?​​1. 인류는 두뇌의 한계\n"]}]},{"cell_type":"code","source":["prompt = '인간처럼 생각하고, 행동하는 \\'지능\\'을 통해 인류가 이제까지 풀지 못했던'\n","with torch.no_grad():\n","  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n","  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=64, top_k=1)\n","  generated = tokenizer.batch_decode(gen_tokens)[0]\n","\n","print(generated)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hAvUGChwlGTZ","executionInfo":{"status":"ok","timestamp":1723013116736,"user_tz":-540,"elapsed":6872,"user":{"displayName":"이유진","userId":"11630789315296928592"}},"outputId":"9b4efa91-d3df-4cf6-bfa7-d344bfb6558a"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["인간처럼 생각하고, 행동하는 '지능'을 통해 인류가 이제까지 풀지 못했던 문제들을 해결할 수 있을 것으로 기대하고 있다.​​​​​​​​​​​​​​\n"]}]},{"cell_type":"code","source":["prompt = '인간처럼 생각하고, 행동하는 \\'지능\\'을 통해 인류가 이제까지 풀지 못했던'\n","with torch.no_grad():\n","  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n","  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=64, top_k=2)\n","  generated = tokenizer.batch_decode(gen_tokens)[0]\n","\n","print(generated)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CJkSHYpBlbrV","executionInfo":{"status":"ok","timestamp":1723013149482,"user_tz":-540,"elapsed":8825,"user":{"displayName":"이유진","userId":"11630789315296928592"}},"outputId":"70fddb8b-68ac-4bd6-b1d1-d1cb40b6f132"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["인간처럼 생각하고, 행동하는 '지능'을 통해 인류가 이제까지 풀지 못했던 문제를 해결하는 것을 목표로 하고 있다.​​​​​​​​​​​​​​​\n"]}]}]}