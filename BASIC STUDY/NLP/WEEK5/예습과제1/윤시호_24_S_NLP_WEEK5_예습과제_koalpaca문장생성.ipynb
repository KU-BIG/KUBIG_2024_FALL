{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"236c50a72b1d429db6fad2f8bbc71b2e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f1b4bb7d498941c8aa00f4993588fb6a","IPY_MODEL_3e71491dbd2d4632aa9d32930a05971b","IPY_MODEL_d72280fc52d54c1fb9fd66b84c40c3d6"],"layout":"IPY_MODEL_f419db7154584c558e08bacf13a1602b"}},"f1b4bb7d498941c8aa00f4993588fb6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_14d8c02f1160402db78767f505022583","placeholder":"​","style":"IPY_MODEL_151dcad9d9a84ccd9bc96f8d5a962fb2","value":"Loading checkpoint shards: 100%"}},"3e71491dbd2d4632aa9d32930a05971b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dcbff46ab4924e83b078e78640e26b31","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e040257b91d4553a4785a217ce3b58d","value":28}},"d72280fc52d54c1fb9fd66b84c40c3d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a096a89f9d94c968e43b541e24092e6","placeholder":"​","style":"IPY_MODEL_f2b2fe6839634e93b5751d8996057499","value":" 28/28 [02:00&lt;00:00,  4.00s/it]"}},"f419db7154584c558e08bacf13a1602b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14d8c02f1160402db78767f505022583":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"151dcad9d9a84ccd9bc96f8d5a962fb2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dcbff46ab4924e83b078e78640e26b31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e040257b91d4553a4785a217ce3b58d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7a096a89f9d94c968e43b541e24092e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2b2fe6839634e93b5751d8996057499":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["5주차 예습과제에서는 6주차 내용인 GPT의 생성 능력을 맛보고자 합니다. 사용할 모델은 koalpaca라는 모델인데, 한국어 버전의 alpaca 모델입니다.\n","\n","alpaca는 llama라는 모델을 instruction fine-tuning 해서 사람의 지시를 잘 따르는 모델입니다.\n","\n","llama라는 모델은 LLaMA-13B의 경우 GPT-3보다 10배이상 작음에도 불구하고 대부분의 평가서 GPT-3보다 우수한 성능을 보이며, 더 나아가 LLaMA-65B의 경우 대부분의 벤치마크에서 Chinchilla, Gopher, GPT-3, PaLM와 유사하거나 더 뛰어난 결과를 보이는 훌륭한 모델입니다.\n","\n","그러면 지금부터 koalpaca로 생성 모델을 한번 경험해보도록 하겠습니다!"],"metadata":{"id":"fsnGgar7gx83"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"7crJC7X_eb1v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722679753892,"user_tz":-540,"elapsed":12,"user":{"displayName":"윤시호","userId":"07095870512662109004"}},"outputId":"e0b768ec-5814-40f6-c60e-4f3656a54f01"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Aug  3 10:09:13 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   54C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["# GPU 켜져 있는지 확인\n","\n","!nvidia-smi"]},{"cell_type":"code","source":["!pip install -q -U bitsandbytes\n","!pip install -q -U git+https://github.com/huggingface/transformers.git\n","!pip install -q -U git+https://github.com/huggingface/peft.git\n","!pip install -q -U git+https://github.com/huggingface/accelerate.git\n","!pip install -q datasets"],"metadata":{"id":"o186Sm7Uef5V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722679897982,"user_tz":-540,"elapsed":144094,"user":{"displayName":"윤시호","userId":"07095870512662109004"}},"outputId":"891d6b26-f2ec-4fc6-c2c4-9f6833992d1b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n","gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"],"metadata":{"id":"VgCKqAXMekYz","executionInfo":{"status":"ok","timestamp":1722679903880,"user_tz":-540,"elapsed":5918,"user":{"displayName":"윤시호","userId":"07095870512662109004"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["peft는 parameter efficient fine-tuning의 약자로, 큰 모델을 코랩에서 돌릴 수 있도록 쪼개서 만든 것입니다.\n","\n","오래 걸립니다!"],"metadata":{"id":"syg88baNin2b"}},{"cell_type":"code","source":["from peft import PeftModel, PeftConfig"],"metadata":{"id":"GlZv-twPerjy","executionInfo":{"status":"ok","timestamp":1722679905411,"user_tz":-540,"elapsed":1549,"user":{"displayName":"윤시호","userId":"07095870512662109004"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["peft_model_id = \"beomi/qlora-koalpaca-polyglot-12.8b-50step\"\n","config = PeftConfig.from_pretrained(peft_model_id)\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True, # 4비트 양자화(메모리 효율)\n","    bnb_4bit_use_double_quant=True, # 이중 양자화(정밀도)\n","    bnb_4bit_quant_type=\"nf4\", # 양자화된 형태: 정규화된 부동소수점 4비트\n","    bnb_4bit_compute_dtype=torch.bfloat16\n",")\n","model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, quantization_config=bnb_config, device_map={\"\":0})\n","# 기본 모델을 양자화 설정과 함께 불러옴\n","model = PeftModel.from_pretrained(model, peft_model_id)\n","# Peft모델을 로드해 model에 적용\n","tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n","# 해당 모델에 최적화된 토크나이저를 자동으로 불러옴\n","model.eval()"],"metadata":{"id":"gYf0UQ3fetzP","colab":{"base_uri":"https://localhost:8080/","height":890,"referenced_widgets":["236c50a72b1d429db6fad2f8bbc71b2e","f1b4bb7d498941c8aa00f4993588fb6a","3e71491dbd2d4632aa9d32930a05971b","d72280fc52d54c1fb9fd66b84c40c3d6","f419db7154584c558e08bacf13a1602b","14d8c02f1160402db78767f505022583","151dcad9d9a84ccd9bc96f8d5a962fb2","dcbff46ab4924e83b078e78640e26b31","8e040257b91d4553a4785a217ce3b58d","7a096a89f9d94c968e43b541e24092e6","f2b2fe6839634e93b5751d8996057499"]},"executionInfo":{"status":"ok","timestamp":1722683370865,"user_tz":-540,"elapsed":124190,"user":{"displayName":"윤시호","userId":"07095870512662109004"}},"outputId":"4e640ecb-7f93-478b-90a7-c656e77a8a06"},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/28 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"236c50a72b1d429db6fad2f8bbc71b2e"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): GPTNeoXForCausalLM(\n","      (gpt_neox): GPTNeoXModel(\n","        (embed_in): Embedding(30080, 5120)\n","        (emb_dropout): Dropout(p=0.0, inplace=False)\n","        (layers): ModuleList(\n","          (0-39): 40 x GPTNeoXLayer(\n","            (input_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n","            (post_attention_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n","            (post_attention_dropout): Dropout(p=0.0, inplace=False)\n","            (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n","            (attention): GPTNeoXSdpaAttention(\n","              (rotary_emb): GPTNeoXRotaryEmbedding()\n","              (query_key_value): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=5120, out_features=15360, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=5120, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=15360, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (dense): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","              (attention_dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (mlp): GPTNeoXMLP(\n","              (dense_h_to_4h): Linear4bit(in_features=5120, out_features=20480, bias=True)\n","              (dense_4h_to_h): Linear4bit(in_features=20480, out_features=5120, bias=True)\n","              (act): GELUActivation()\n","            )\n","          )\n","        )\n","        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (embed_out): Linear(in_features=5120, out_features=30080, bias=False)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["next token generation"],"metadata":{"id":"ViGa1Tvyi8fI"}},{"cell_type":"code","source":["prompt = '인간처럼 생각하고, 행동하는 \\'지능\\'을 통해 인류가 이제까지 풀지 못했던'\n","with torch.no_grad():\n","  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n","  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=64)\n","  generated = tokenizer.batch_decode(gen_tokens)[0]\n","\n","print(generated)"],"metadata":{"id":"sHacSUa3fgz0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722680268789,"user_tz":-540,"elapsed":13353,"user":{"displayName":"윤시호","userId":"07095870512662109004"}},"outputId":"d0b3f84d-daa4-430b-9e12-35642657cb98"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"output_type":"stream","name":"stdout","text":["인간처럼 생각하고, 행동하는 '지능'을 통해 인류가 이제까지 풀지 못했던 어려운 문제들을해결해 나가기를 기대해 본다.<|endoftext|>\n"]}]},{"cell_type":"markdown","source":[" QA"],"metadata":{"id":"zO4qeMpZi54_"}},{"cell_type":"code","source":["def gen(x):\n","    q = f\"### 질문: {x}\\n\\n### 답변:\"\n","    # print(q)\n","    gened = model.generate(\n","        **tokenizer(\n","            q,\n","            return_tensors='pt',\n","            return_token_type_ids=False\n","        ).to('cuda'), # 입력 텐서와 어텐션 마스크\n","        max_new_tokens=70, # 생성 시퀀스의 최대 길이\n","        early_stopping=True,\n","        do_sample=True, # 샘플링 방식, 아니면 그리디 서치 방식\n","        eos_token_id=2\n","    )\n","    print(tokenizer.decode(gened[0]))"],"metadata":{"id":"0JBJKYJcfLuX","executionInfo":{"status":"ok","timestamp":1722684690197,"user_tz":-540,"elapsed":411,"user":{"displayName":"윤시호","userId":"07095870512662109004"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["gen('건강하게 살기 위한 세 가지 방법은?')"],"metadata":{"id":"55GdkP9afi6U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722684644850,"user_tz":-540,"elapsed":7801,"user":{"displayName":"윤시호","userId":"07095870512662109004"}},"outputId":"c481a5bf-9467-46ad-edca-f813f227d312"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["### 질문: 건강하게 살기 위한 세 가지 방법은?\n","\n","### 답변: 1. 적당한 수면을 취한다. 2. 하루에 적어도 30분 정도 운동을 한다 3. 채소, 과일 등 신선한 것을 많이 먹는다. 오늘 말씀은 여기까지구요. 지금까지 진행에 Dr. Gu\n"]}]},{"cell_type":"markdown","source":["top_k와 같은 parameter를 조절하면서 실험해보세요!"],"metadata":{"id":"VjacO_6njJWR"}},{"cell_type":"code","source":["prompt = '공부에 집중이 안될 땐'\n","with torch.no_grad():\n","  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n","  gen_tokens = model.generate(tokens,\n","                              do_sample=True,\n","                              temperature=0.8, # 무작위성 ; 높을 수록 더 무작위적\n","                              max_length=64, # 입력 + 생성 시퀀스의 최대 길이\n","                              top_k=40) # 샘플링 후보 수,\n","  generated = tokenizer.batch_decode(gen_tokens)[0]\n","\n","print(generated)"],"metadata":{"id":"8JaD2vpPfkuU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722684664670,"user_tz":-540,"elapsed":8879,"user":{"displayName":"윤시호","userId":"07095870512662109004"}},"outputId":"d5b10e60-bc1d-41e5-f5ee-f7abad5e70ae"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["공부에 집중이 안될 땐 어떻게 하시나요?​저는 요즘 책만 보면 잠이 오기에공부를 못하고 있답니다...ㅎㅎ​​​ ​그래도 요즘 독서에 빠져 살고 있어서그 자체만으로 행복합니다!!(❁\n"]}]},{"cell_type":"code","source":["gen('인공지능은 인간을 능가하는가')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rDSo4pA_tuG-","executionInfo":{"status":"ok","timestamp":1722684711815,"user_tz":-540,"elapsed":10851,"user":{"displayName":"윤시호","userId":"07095870512662109004"}},"outputId":"1f909820-87fb-4e6b-d6fe-6b94697722c1"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["### 질문: 인공지능은 인간을 능가하는가\n","\n","### 답변: 이 질문에는 다소 애매한 면이 있습니다. 인공지능의 특징 및 장단점이 인간의 지능과 유사하기 때문입니다. 예를 들어, 어떤 사람이 어떤 분야에서 인간보다 월등하게 잘 한다는 것을 ‘지능이 높다’라고 우리는 말하지는 않습니다. 그렇다고 그 사람의 지능이\n"]}]}]}