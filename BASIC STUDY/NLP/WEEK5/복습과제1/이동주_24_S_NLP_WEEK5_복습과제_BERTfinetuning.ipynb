{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADkUGTqixRWo"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX_ZDhicpHkV"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSU7yERLP_66"
      },
      "source": [
        "## 1.1. Using Colab GPU for Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqG7FzRVFEIv"
      },
      "source": [
        "GPU 사용 가능 여부 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYsV4H8fCpZ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c69c3f6-cd46-407c-9f65-fcf17d6415dc"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ElsnSNUridI"
      },
      "source": [
        "## 1.2. Installing the Hugging Face Library\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_N2UDLevYWn"
      },
      "source": [
        "현재 Hugging Face 라이브러리는 BERT 작업을 위한 가장 널리 사용되는 인터페이스입니다. 사전 훈련된 다양한 transformer 모델을 지원하는 것 외에도 라이브러리에는 특정 작업에 적합한 이러한 모델의 사전 구축된 수정 사항도 포함되어 있습니다. 예를 들어, 본 과제에서는 `BertForSequenceClassification`을 사용합니다.\n",
        "\n",
        "라이브러리에는 토큰 분류, 질문 답변, 다음 문장 예측 등을 위한 작업별 클래스도 포함되어 있습니다. 이러한 사전 구축된 클래스를 사용하면 목적에 맞게 BERT를 수정하는 프로세스가 단순화됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NmMdkZO8R6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ff83b11-d491-4240-ddb3-8a36b4f08e29"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guw6ZNtaswKc"
      },
      "source": [
        "# 2. Loading CoLA Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9ZKxKc04Btk"
      },
      "source": [
        "단일 문장 분류에는 [CoLA(The Corpus of Linguistic Acceptability)](https://nyu-mll.github.io/CoLA/) 데이터세트를 사용하겠습니다. 문법적으로 정확하거나 틀린 것으로 표시된 문장 데이터셋입니다. 2018년 5월에 처음 공개되었으며 BERT와 같은 모델이 평가되는 \"GLUE 벤치마크\"에 포함된 테스트 중 하나입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JrUHXms16cn"
      },
      "source": [
        "## 2.1. Download & Extract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZNVW6xd0T0X"
      },
      "source": [
        "`wget` 패키지로 데이터를 다운로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m6AnuFv0QXQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57210482-d4fc-478f-d0b8-a750c9495b71"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMtmPMkBzrvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e65d5e8d-6426-463a-c3dc-45afd27ffda4"
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yv-tNv20dnH"
      },
      "source": [
        "# Unzip\n",
        "if not os.path.exists('./cola_public/'):\n",
        "    !unzip cola_public_1.1.zip"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQUy9Tat2EF_"
      },
      "source": [
        "## 2.2. Parse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UkeC7SG2krJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "01ade190-64ac-4669-f8c2-730cb41a11c7"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "df.sample(10)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     sentence_source  label label_notes  \\\n",
              "2389            l-93      1         NaN   \n",
              "5048            ks08      1         NaN   \n",
              "3133            l-93      0           *   \n",
              "5955            c_13      0           *   \n",
              "625             bc01      1         NaN   \n",
              "3542            ks08      0           *   \n",
              "6915            m_02      1         NaN   \n",
              "2908            l-93      1         NaN   \n",
              "5857            c_13      1         NaN   \n",
              "4191            ks08      1         NaN   \n",
              "\n",
              "                                               sentence  \n",
              "2389        Angela characterized Shelly as a lifesaver.  \n",
              "5048  They're not finding it a stress being in the s...  \n",
              "3133                              Paul exhaled on Mary.  \n",
              "5955                  I ordered if John drink his beer.  \n",
              "625         Press the stamp against the pad completely.  \n",
              "3542                                     They can very.  \n",
              "6915   This arch is supporting the weight of the tower.  \n",
              "2908                   That new handle detaches easily.  \n",
              "5857    The Brazilians pumped the oil across the river.  \n",
              "4191                               It is a wooden desk.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6577584e-d60d-4deb-a726-133bd48a15a2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2389</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Angela characterized Shelly as a lifesaver.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5048</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>They're not finding it a stress being in the s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3133</th>\n",
              "      <td>l-93</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>Paul exhaled on Mary.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5955</th>\n",
              "      <td>c_13</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>I ordered if John drink his beer.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>625</th>\n",
              "      <td>bc01</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Press the stamp against the pad completely.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3542</th>\n",
              "      <td>ks08</td>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>They can very.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6915</th>\n",
              "      <td>m_02</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>This arch is supporting the weight of the tower.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2908</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>That new handle detaches easily.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5857</th>\n",
              "      <td>c_13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The Brazilians pumped the oil across the river.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4191</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It is a wooden desk.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6577584e-d60d-4deb-a726-133bd48a15a2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6577584e-d60d-4deb-a726-133bd48a15a2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6577584e-d60d-4deb-a726-133bd48a15a2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9130299a-5511-493f-b0fe-d9e466e7eef8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9130299a-5511-493f-b0fe-d9e466e7eef8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9130299a-5511-493f-b0fe-d9e466e7eef8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"sentence_source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"ks08\",\n          \"m_02\",\n          \"c_13\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_notes\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"*\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"The Brazilians pumped the oil across the river.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfWzpPi92UAH"
      },
      "source": [
        "`sentence` 와 `label`\b만 남기겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blqIvQaQncdJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "52a2511a-f6c7-4f51-900e-9508c009af79"
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               sentence  label\n",
              "6770  We realised that Dr Jones died because he ate ...      0\n",
              "1652  Here's a pole for you to kiss the girl who tie...      0\n",
              "3258                    Jennifer baked at the potatoes.      0\n",
              "4651  Kim is resembled by the model in nearly every ...      0\n",
              "2672                            The book sent to Peter.      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e40c586c-9858-4e33-bfa2-91bf36323500\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6770</th>\n",
              "      <td>We realised that Dr Jones died because he ate ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1652</th>\n",
              "      <td>Here's a pole for you to kiss the girl who tie...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3258</th>\n",
              "      <td>Jennifer baked at the potatoes.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4651</th>\n",
              "      <td>Kim is resembled by the model in nearly every ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2672</th>\n",
              "      <td>The book sent to Peter.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e40c586c-9858-4e33-bfa2-91bf36323500')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e40c586c-9858-4e33-bfa2-91bf36323500 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e40c586c-9858-4e33-bfa2-91bf36323500');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-42311335-4fb3-4d84-9a5e-b5bc9e29a824\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-42311335-4fb3-4d84-9a5e-b5bc9e29a824')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-42311335-4fb3-4d84-9a5e-b5bc9e29a824 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Here's a pole for you to kiss the girl who tied the string around.\",\n          \"The book sent to Peter.\",\n          \"Jennifer baked at the potatoes.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuE5BqICAne2"
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snh5pKBmngq2",
        "outputId": "ff8b5e95-e2f1-4c26-b697-385406409045"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5O1eV-Pfct"
      },
      "source": [
        "# 3. Tokenization & Input Formatting\n",
        "\n",
        "이 섹션에서는 데이터 세트를 BERT가 학습할 수 있는 형식으로 변환합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8kEDRvShcU5"
      },
      "source": [
        "## 3.1. BERT Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWOPOyWghJp2"
      },
      "source": [
        "텍스트를 BERT에 공급하려면 텍스트를 토큰으로 분할한 다음 이러한 토큰을 토크나이저 어휘의 인덱스에 매핑해야 합니다.\n",
        "\n",
        "토큰화는 BERT에 포함된 토크나이저에 의해 수행되어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z474sSC6oe7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cde979e4-9234-426b-9e1f-03903cae6437"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLIbudgfh6F0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "357fecf6-9e5d-48f7-8ac7-0b70bdfc87b4"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeNIc4auFUdF"
      },
      "source": [
        "실제로 모든 문장을 변환할 때 `tokenize`와 `convert_tokens_to_ids`를 별도로 호출하는 대신 `tokenize.encode` 함수를 사용하여 두 단계를 모두 처리합니다.\n",
        "\n",
        "하지만 그렇게 하기 전에 BERT의 형식 요구 사항 중 일부에 대해 살펴보겠습니다다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viKGCCh8izww"
      },
      "source": [
        "## 3.2. Required Formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDcqNlvVhL5W"
      },
      "source": [
        "위의 코드에는 여기서 살펴볼 몇 가지 필수 형식 지정 단계가 생략되었습니다.\n",
        "\n",
        "\n",
        "우리는 다음을 수행해야 합니다.\n",
        "1. 각 문장의 시작과 끝 부분에 특수 토큰을 추가.\n",
        "2. 모든 문장을 하나의 일정한 길이로 채우고 자릅니다.\n",
        "3. \"attention mask\"를 사용하여 실제 토큰과 패딩 토큰을 명시적으로 구별합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6mceWWOjZnw"
      },
      "source": [
        "### Special Tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykk0P9JiKtVe"
      },
      "source": [
        "**`[SEP]`**\n",
        "\n",
        "모든 문장 끝에 특수 `[SEP]` 토큰을 추가해야 합니다.\n",
        "\n",
        "이 토큰은 BERT에 두 개의 별도 문장이 제공됨을 알립니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86C9objaKu8f"
      },
      "source": [
        "**`[CLS]`**\n",
        "\n",
        "분류 작업을 위해서는 모든 문장의 시작 부분에 특수 `[CLS]` 토큰을 추가해야 합니다.\n",
        "\n",
        "이 토큰은 특별한 의미를 갖습니다. BERT는 12개의 Transformer 레이어로 구성됩니다. 각 transformer는 토큰 임베딩 목록을 가져와 출력에 동일한 수의 임베딩을 생성합니다.\n",
        "\n",
        "![Illustration of CLS token purpose](http://www.mccormickml.com/assets/BERT/CLS_token_500x606.png)\n",
        "\n",
        "최종(12번째) transformer의 출력에서 *classifier는 *첫 번째 임베딩([CLS] 토큰에 해당)만 사용합니다*.\n",
        "\n",
        "또한 BERT는 분류를 위해 이 [CLS] 토큰만 사용하도록 훈련되었기 때문에 모델이 분류 단계에 필요한 모든 것을 단일 768 값 임베딩 벡터로 인코딩하도록 되었습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u51v0kFxeteu"
      },
      "source": [
        "### Sentence Length & Attention Mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPNuwqZVK3T6"
      },
      "source": [
        "BERT에는 두 가지 제약 조건이 있습니다.\n",
        "1. 모든 문장은 고정된 단일 길이로 채워지거나 잘려야 합니다.\n",
        "2. 최대 문장 길이는 512 토큰입니다.\n",
        "\n",
        "패딩은 BERT 어휘의 인덱스 0에 있는 특수 `[PAD]` 토큰을 사용하여 수행됩니다. 아래 그림은 8개 토큰의 \"MAX_LEN\"에 대한 패딩을 보여줍니다.\n",
        "\n",
        "<img src=\"http://www.mccormickml.com/assets/BERT/padding_and_mask.png\" width=\"600\">\n",
        "\n",
        "\"attention mask\"는 단순히 패딩되는 토큰과 패딩되지 않는 토큰을 나타내는 1과 0의 배열입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6w8elb-58GJ"
      },
      "source": [
        "## 3.2. Sentences to IDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M296yz577fV"
      },
      "source": [
        "`tokenizer.encode` 함수는 여러 단계를 결합합니다:\n",
        "1. 문장을 토큰으로 나눕니다.\n",
        "2. 특수 `[CLS]` 및 `[SEP]` 토큰을 추가합니다.\n",
        "3. 토큰을 해당 ID에 매핑합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bBdb3pt8LuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd571de6-5be2-4e8b-b510-79e716962e83"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Token IDs: [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhwCKszh6ych"
      },
      "source": [
        "## 3.3. Padding & Truncating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xytsw1oIfnX0"
      },
      "source": [
        "시퀀스를 모두 채우고 잘라서 길이가 모두 'MAX_LEN'이 되도록 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqiWTDrn_nGB"
      },
      "source": [
        "First, what's the maximum sentence length in our dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhUZO9vc_l6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47935d11-1f92-4121-e151-1b1e9baa0a36"
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length:  47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp9BPRd1tMIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89816aff-1cef-4fd0-86b5-340c47fadbff"
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\",\n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDs-MYtYH8sL"
      },
      "source": [
        "## 3.4. Attention Masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhGulL1pExCT"
      },
      "source": [
        "어텐션 마스크는 어떤 토큰이 실제 단어인지, 어떤 토큰이 패딩인지를 명확하게 보여줍니다.\n",
        "\n",
        "BERT 어휘는 ID 0을 사용하지 않으므로 토큰 ID가 0이면 패딩이고 그렇지 않으면 실제 토큰입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDoC24LeEv3N"
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "\n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "\n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRp4O7D295d_"
      },
      "source": [
        "## 3.5. Training & Validation Split\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu0ao7p8rb06"
      },
      "source": [
        "train/test를 분리합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFbE-UHvsb7-"
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LzSbTqW9_BR"
      },
      "source": [
        "## 3.6. Converting to PyTorch Data Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p1uXczp-Je4"
      },
      "source": [
        "우리 모델은 numpy.ndarrays 대신 PyTorch 텐서를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jw5K2A5Ko1RF"
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype\n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD9i6Z2pG-sN"
      },
      "source": [
        "또한 토치 DataLoader 클래스를 사용하여 데이터세트에 대한 반복자를 생성합니다. 이는 for 루프와 달리 반복자를 사용하면 전체 데이터세트를 메모리에 로드할 필요가 없기 때문에 훈련 중에 메모리를 절약하는 데 도움이 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEgLpFVlo1Z-"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it\n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bwa6Rts-02-"
      },
      "source": [
        "# 4. Train Our Classification Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6TKgyUzPIQc"
      },
      "source": [
        "## 4.1. BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sjzRT1V0zwm"
      },
      "source": [
        "이 작업을 위해 먼저 사전 훈련된 BERT 모델을 수정하여 분류를 위한 출력을 제공한 다음 전체 모델이 엔드투엔드에 적합할 때까지 데이터 세트에서 모델을 계속 훈련하려고 합니다.\n",
        "\n",
        "현재 미세 조정을 위해 Huggingface에서 제공되는 클래스 목록은 다음과 같습니다.\n",
        "* BertModel\n",
        "* BertForPreTraining\n",
        "* BertForMaskedLM\n",
        "* BertForNextSentence예측\n",
        "* **BertForSequenceClassification** -> 우리가 사용할 것입니다.\n",
        "* BertForTokenClassification\n",
        "* BertForQuestionAnswering\n",
        "\n",
        "이에 대한 문서는 [여기](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html)에서 찾을 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXYitPoE-cjH"
      },
      "source": [
        "우리는 [BertForSequenceClassification](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification)을 사용할 것입니다. 이것은 문장 분류기로 사용할 분류를 위해 상단에 단일 선형 레이어가 추가된 일반 BERT 모델입니다. 입력 데이터를 제공하면 사전 훈련된 전체 BERT 모델과 훈련되지 않은 추가 분류 계층이 특정 작업에 대해 훈련됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnQW9E-bBCRt"
      },
      "source": [
        "`from_pretrained` 에 대한 문서는 [여기](https://huggingface.co/transformers/v2.2.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained)에 있습니다.\n",
        "추가적인 parameter는 [여기](https://huggingface.co/transformers/v2.2.0/main_classes/configuration.html#transformers.PretrainedConfig)에 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFsCTp_mporB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b3c7f77-29fe-4bce-ace0-dae2cc882bf5"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single\n",
        "# linear classification layer on top.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PIiVlDYCtSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "359e1f32-4126-42f9-98a2-e7b933c61c0f"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRWT-D4U_Pvx"
      },
      "source": [
        "## 4.2. Optimizer & Learning Rate Scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o-VEBobKwHk"
      },
      "source": [
        "이제 모델을 로드했으므로 저장된 모델 내에서 훈련 하이퍼파라미터를 가져와야 합니다.\n",
        "\n",
        "미세 조정을 위해 저자는 다음 값 중에서 선택할 것을 권장합니다.\n",
        "- batch size: 16, 32(DataLoader를 생성할 때 32를 선택했습니다).\n",
        "- learning rate(Adam): 5e-5, 3e-5, 2e-5(여기에서는 2e-5를 사용하겠습니다).\n",
        "- epochs: 2, 3, 4(여기에서는 4를 사용합니다).\n",
        "\n",
        "엡실론 매개변수 `eps = 1e-8`은 \"구현 시 0으로 나누는 것을 방지하기 위한 매우 작은 숫자\"입니다([여기](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n",
        "\n",
        "'run_glue.py' [여기](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)에서 AdamW 최적화 프로그램 생성을 찾을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLs72DuMODJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec0fa28a-589b-49c0-a465-9741fe7d1803"
      },
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p0upAhhRiIx"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqfmWwUR_Sox"
      },
      "source": [
        "## 4.3. Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QXZhFb4LnV5"
      },
      "source": [
        "다음은 훈련 루프입니다. 많은 일이 진행되고 있지만 기본적으로 루프의 각 패스에는 트라이닝 단계와 검증 단계가 있습니다. 각 패스에서 우리는 다음을 수행해야 합니다.\n",
        "\n",
        "훈련 루프:\n",
        "- 데이터 입력 및 라벨 압축 풀기\n",
        "- 가속을 위해 GPU에 데이터 로드\n",
        "- 이전 단계에서 계산된 그래디언트를 지웁니다.\n",
        "     - pytorch에서는 명시적으로 지우지 않는 한 기본적으로 그래디언트가 누적됩니다(RNN과 같은 작업에 유용함).\n",
        "- 순방향 패스(네트워크를 통해 입력 데이터 공급)\n",
        "- 역방향 전달(역전파)\n",
        "- 네트워크에 Optimizer.step()을 사용하여 매개변수를 업데이트하도록 지시합니다.\n",
        "- 진행상황 모니터링을 위한 변수 추적\n",
        "\n",
        "평가 루프:\n",
        "- 데이터 입력 및 라벨 압축 풀기\n",
        "- 가속을 위해 GPU에 데이터 로드\n",
        "- 순방향 패스(네트워크를 통해 입력 데이터 공급)\n",
        "- 검증 데이터의 손실을 계산하고 진행 상황을 모니터링하기 위한 변수를 추적합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cQNvaZ9bnyy"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpt6tR83keZD"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-FYdx6nFE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "756c72f2-479f-44bd-cfa8-2d7c20c3d59e"
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_input_mask,\n",
        "                    labels=b_labels)\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the\n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:27.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:40.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:53.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:06.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:19.\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epcoh took: 0:01:19\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:26.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:39.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:53.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:06.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:19.\n",
            "\n",
            "  Average training loss: 0.30\n",
            "  Training epcoh took: 0:01:19\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:26.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:39.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:52.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:06.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:19.\n",
            "\n",
            "  Average training loss: 0.18\n",
            "  Training epcoh took: 0:01:19\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:13.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:26.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:39.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:52.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:05.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:19.\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:01:19\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.83\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68xreA9JAmG5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "f9453dfc-e291-4e0f-c5a4-d819d706d58f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkyubuJSOzg3"
      },
      "source": [
        "# 5. Performance On Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DosV94BYIYxg"
      },
      "source": [
        "test 용 데이터세트를 로드하고 [Matthew의 상관 계수](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)를 사용하여 예측을 평가합니다.\n",
        "\n",
        "이 지표를 사용하면 +1이 최고 점수이고 -1이 최저 점수입니다. 이런 식으로 우리는 이 특정 작업에 대한 최신 모델에 비해 우리가 얼마나 잘 수행하는지 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg42jJqqM68F"
      },
      "source": [
        "### 5.1. Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAN0LZBOOPVh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcee2e15-5dbe-4795-cc34-06a5f41c738b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN,\n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)\n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16lctEOyNFik"
      },
      "source": [
        "## 5.2. Evaluate on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hba10sXR7Xi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc59ee4c-97bc-4f0c-c873-10131ad9075b"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict\n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  # Telling the model not to compute or store gradients, saving memory and\n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "    DONE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWcy0X1hirdx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ee2075-7b06-4329-8707-d9320be2d561"
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRaZQ4XC7kLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bdd4175-71d7-4b29-c7e2-0427bd7d734e"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "  # Calculate and store the coef for this batch.\n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
        "  matthews_set.append(matthews)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xytAr_C48wnu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b64244a5-3bde-4225-c609-97f09e521524"
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.14856415213808927,\n",
              " -0.17407765595569785,\n",
              " 0.4040950971038548,\n",
              " 0.11365840154561273,\n",
              " 0.4133804997216296,\n",
              " 0.6777932975034471,\n",
              " 0.4152273992686999,\n",
              " 0.0,\n",
              " 0.9165151389911681,\n",
              " 0.6952687917708212,\n",
              " 0.8459051693633014,\n",
              " 0.7419408268023742,\n",
              " 0.8150678894028793,\n",
              " 0.7141684885491869,\n",
              " 0.1794871794871795,\n",
              " 0.5716350506349809,\n",
              " 0.0]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCYZa1lQ8Jn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1fc9773-0e64-40a9-c365-78599338f778"
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MCC: 0.508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- MCC는 -1~1 사이의 값으로, 0.508은 무작위 예측(0)보다 성능이 좋음을 뜻함!"
      ],
      "metadata": {
        "id": "a5_1Ht6u1QvI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-2EilVwKsNKT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}