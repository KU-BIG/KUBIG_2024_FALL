{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"hSfah6SLfhVG"},"source":["# 텍스트 생성 방법 : Transformers를 이용한 언어생성에 서로 다른 디코딩 방법 사용"]},{"cell_type":"markdown","metadata":{"id":"9l31dGxwgTMZ"},"source":["#소개\n","최근 몇 년 동안, OpenAI의 유명한 모델 GPT2처럼 수백만 개의 웹 페이지에서 훈련된 Transformer 기반 대형 언어 모델의 등장으로 개방형 언어 생성에 대한 관심이 증가하고 있습니다. 개선된 Transformer 아키텍처와 대규모 unsupervised training data 외에도, 더 나은 Decoding 방법도 중요한 역할을 했습니다.\n","\n","이 실습 자료는 다양한 Decoding 전략에 대한 간략한 개요를 제공합니다.\n","\n","다음 모든 기능은 auto-regressive 언어 생성에 사용할 수 있습니다. 요약하자면, auto-regressive 언어 생성은 word sequence의 확률 분포가 다음 던어 분포에 관한 P식의 결과로 분해될 수 있고\n","\n","$$ P(w_{1:T} | W_0 ) = \\prod_{t=1}^T P(w_{t} | w_{1: t-1}, W_0) \\text{ ,with }  w_{1: 0} = \\emptyset, $$\n","\n","W0가 초기 Context word sequence의 결과로 분해될 수 있다는 가정에 기초합니다.\n","\n","Word sequence의 길이 T는 보통 즉시 결정되며, P식에서 EOS 토큰이 생성된 timestep t=T와 부합합니다.\n","\n","이번 실습에서는 가장 두드러진 decoding 방법으로 Greedy search, Beam search, Top-K sampling, Top-p sampling을 주로 둘러볼 것입니다.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sz0J-hPCoZEl"},"source":["Transformers를 설치하고 Model을 load하겠습니다."]},{"cell_type":"code","metadata":{"id":"XbzZ_IVTtoQe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723615104594,"user_tz":-540,"elapsed":8677,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"86ce9833-d5b5-4d11-e06c-6c8747675f82"},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"]}]},{"cell_type":"markdown","metadata":{"id":"X4jS5zNKknHT"},"source":["이번 실습을 위해 SKT에서 공개한 KoGPT-2 모델을 사용해보도록 하겠습니다 :-)"]},{"cell_type":"code","metadata":{"id":"b1eVsFQQgdBk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723615738652,"user_tz":-540,"elapsed":49544,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"53c0f925-7692-4a1b-c1f9-0e49537caf11"},"source":["!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n","!apt-get install git-lfs\n","!git lfs install\n","!git clone https://huggingface.co/taeminlee/kogpt2"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Detected operating system as Ubuntu/jammy.\n","Checking for curl...\n","Detected curl...\n","Checking for gpg...\n","Detected gpg...\n","Detected apt version as 2.4.12\n","Running apt-get update... done.\n","Installing apt-transport-https... done.\n","Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n","Importing packagecloud gpg key... Packagecloud gpg key imported to /etc/apt/keyrings/github_git-lfs-archive-keyring.gpg\n","done.\n","Running apt-get update... done.\n","\n","The repository is setup! You can now install packages.\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following packages will be upgraded:\n","  git-lfs\n","1 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 7,420 kB of archives.\n","After this operation, 6,051 kB of additional disk space will be used.\n","Get:1 https://packagecloud.io/github/git-lfs/ubuntu jammy/main amd64 git-lfs amd64 3.5.1 [7,420 kB]\n","Fetched 7,420 kB in 0s (25.8 MB/s)\n","(Reading database ... 123598 files and directories currently installed.)\n","Preparing to unpack .../git-lfs_3.5.1_amd64.deb ...\n","Unpacking git-lfs (3.5.1) over (3.0.2-1ubuntu0.2) ...\n","Setting up git-lfs (3.5.1) ...\n","Git LFS initialized.\n","Processing triggers for man-db (2.10.2-1) ...\n","Git LFS initialized.\n","Cloning into 'kogpt2'...\n","remote: Enumerating objects: 56, done.\u001b[K\n","remote: Total 56 (delta 0), reused 0 (delta 0), pack-reused 56 (from 1)\u001b[K\n","Unpacking objects: 100% (56/56), 1.53 MiB | 4.41 MiB/s, done.\n","Filtering content: 100% (3/3), 1.41 GiB | 49.15 MiB/s, done.\n"]}]},{"cell_type":"code","metadata":{"id":"ckGerDmxgjOb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723615748765,"user_tz":-540,"elapsed":10117,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"e9cc6e61-725f-4169-f962-2d2e13bb2aa3"},"source":["import torch\n","from tokenizers import SentencePieceBPETokenizer\n","from transformers import GPT2Config, GPT2LMHeadModel\n","\n","tokenizer = SentencePieceBPETokenizer(\"/content/kogpt2/vocab.json\", \"/content/kogpt2/merges.txt\")\n","\n","config = GPT2Config(vocab_size=50000)\n","config.pad_token_id = tokenizer.token_to_id('<pad>')\n","model = GPT2LMHeadModel(config)\n","\n","model_dir = '/content/kogpt2/pytorch_model.bin'\n","\n","model.load_state_dict(torch.load(model_dir, map_location='cuda'), strict=False)\n","model.to('cuda')"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50000, 768)\n","    (wpe): Embedding(1024, 768)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-11): 12 x GPT2Block(\n","        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2SdpaAttention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n",")"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"BYdGXD4opfuk"},"source":["### **Greedy Search**\n","\n","Greedy search는 단순히 가장 높은 확률을 가진 단어를 다음 단어로 선택합니다.   \n","$w_t = argmax_{w}P(w | w_{1:t-1})$ 는 각각의 timestep $t$ 입니다. 아래 그림은 greedy search을 보여줍니다.   \n","\n","![Greedy Search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/greedy_search.png)\n","\n","알고리즘은 단어 \"The\"에서 시작하여 다음 단어로 가장 높은 확률의 단어인 \"nice\" 등을 선택하는 탐욕법입니다. 그러므로 최종적으로 생성된 Word sequence는 \"The\", \"nice\", \"woman\"이며 전반적인 확률은 0.5x0.4 = 0.2로 계산됩니다.\n","\n","다음 문맥 (\"I\", \"enjoy\", \"walking\", \"with\", \"my\", \"cute\", \"dog\")에서 GPT2를 사용하여 Word sequence를 생성할 수 있습니다.\n","\n","Transformers에서 다음과 같은 greedy search를 사용하는 방법을 살펴보겠습니다.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"OWLd_J6lXz_t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723616284134,"user_tz":-540,"elapsed":5123,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"deb6de32-44eb-49ea-b5af-d4c064696abb"},"source":["# encode context the generation is conditioned on\n","def tokenizing(text):\n","    return torch.tensor(tokenizer.encode(text, add_special_tokens=False).ids).unsqueeze(0).to('cuda')\n","\n","\n","input_ids = tokenizing(\"이순신은 조선 중기의 무신이다.\")\n","\n","# generate text until the output length (which includes the context length) reaches 50\n","# 생성 모델은 generate 함수를 통해 다음 token을 생성해낼 수 있습니다.\n","greedy_output = model.generate(input_ids, max_length=100)\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(greedy_output.tolist()[0], skip_special_tokens=True))\n"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","이순신은 조선 중기의 무신이다.</s><s> 이 목록은 다음과 같다.</s><s> 이 목록은 다음을 가리킨다.</s><s> 이 목록은 다음과 같다.</s><s> 이 목록은 다음과 같다.</s><s> 이 목록은 다음과 같다.</s><s> 이 목록은 다음과 같다.</s><s> 이 목록은 다음과 같다.</s><s> 이 목록은 다음과 같다.</s><s> 이 목록은 다음과 같다.</s><s> 이 목록은 다음과 같다.</s><s> 이 목록은 다음과 같다.</s><s> 이 목록은 다음과 같다.</s><s> 이 목록은 다음과 같다.</s><s>\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZNIDMgkds-VO"},"source":["GPT2로 짧은 텍스트를 생성했습니다.   \n","생성된 단어 문맥은 합리적이지만 모델은 비슷한 단어를 반복하는 수준입니다.   \n","이러한 현상은 일반적인 언어생성 모델에서 나타나는 공통된 문제이며 특히 Greedy search와 Beam search에서 훨씬 더 그런 현상이 두드러져 보입니다. (Vijayakumar et al., 2016 and Shao et al., 2017에서 관련 내용을 확인 할 수 있습니다.)   \n","\n","Greedy search의 주요 단점은 그림에서 볼수 있듯이 낮은 확률 단어 이후에 나올수 있는 더 높은 확률의 단어를 놓친다는 점입니다.\n","\n","예를 들면 단어 \"has\"는 0.9의 높은 조건부 확률을 가지고 있지만, 첫 검색단어중 두번째로 높은 조건부 확률 단어인 \"dog\" 이후에  숨어있는 형태입니다. 따라서 Greedy search는 \"The\",\"dog\",\"has\"라는 Word sequence를 놓치게 됩니다.\n","\n","이러한 문제는 Beam search에서 완화할 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"Y73RcrRPn-Bn"},"source":["### **Beam search**\n","\n","Beam search는 각 Time step에서 가장 확률이 높은 Hypotheses의 num_beams를 유지하고 결국 전체 확률이 가장 높은 hypothesis를 선택하는 것으로 숨겨진 높은 확률 Word sequence를 놓칠 위험을 줄입니다.\n","\n","`num_beams =2`라고 가정하고 Toy example을 설명하겠습니다.\n","\n","![Beam search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n","\n","Time step=1일때, Beam search는 가장 가능성 높은 Hypothesis \"The\",\"nice\"외에도 두번째로 가능성 높은 Hypothesis인 \"The\",\"dog\"를 추적합니다.\n","\n","Time step=2일때, Beam search는 Word sequence 확률 0.2를 가진 (\"The\",\"nice\",\"woman\") 보다 확률 0.36을 가진 (\"The\", \"dog\", \"has\")가 높다는 것을 찾습니다. 이것으로 Toy example에서 가장 가능성 높은 Word sequence를 발견 할 수 있다는 것을 보였습니다.\n","\n","Beam search는 항상 Greedy search보다 높은 확률의 결과 Sequence를 찾는 것이 가능합니다. 그러나 이것이 가장 가능성 높은 결과를 찾은 것이라고는 보장할 수 없습니다.\n","\n","`transformers`에서 Beam search를 사용하는 방법을 살펴볼 것입니다. 모든 Beam Hypotheses가 EOS토큰에 닿으면 생성이 완료되도록 `num_beams > 1` 과 `eqrly_stopping=True`로 파라미터를 설정합니다.\n"]},{"cell_type":"code","metadata":{"id":"R1R5kx30Ynej","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723616305707,"user_tz":-540,"elapsed":1181,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"6e6ccf98-8838-46b8-de50-d7ff05cc91dd"},"source":["# activate beam search and early_stopping\n","beam_output = model.generate(\n","    input_ids,\n","    max_length=50,\n","    num_beams=5,\n","    early_stopping=True\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))\n"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","이순신은 조선 중기의 무신이다.</s><s> 또한, 이 목록은 대한민국의 보물 제460호로 지정되었다.</s><s> 이 목록은 대한민국의 보물 제460호로 지정되었다.</s><s> 이 목록은 대한민국의 보물 제460호로 지정되었다.</s><s> 이 목록은 대한민국의\n"]}]},{"cell_type":"markdown","metadata":{"id":"UCeY_atbBMr7"},"source":["결과는 틀림없이 더 유창하게 보이지만 여전히 동일한 Word sequence를 반복하는 문제를 포함합니다.\n","\n","단순한 해결법은 Paulus et al. (2017)과 Klein et al. (2017)의 논문에서 제안된 n-grams 패널티를 도입하는 것입니다. 가장 일반적인 n-grams 패널티는 이미 나타난 n-gram에 대해 다음 단어로 생성될 확률을 0으로 설정하여 두번 나타나지 않도록 하는 방법입니다.\n","\n","`no_repeat_ngram_size=2`을 설정한다면 2-gram이 두번 나타나는 것을 막을 수 있습니다."]},{"cell_type":"code","metadata":{"id":"jy3iVJgfnkMi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723616324117,"user_tz":-540,"elapsed":837,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"4a03afff-c585-4fdc-9035-0bbfad0ea78d"},"source":["# set no_repeat_ngram_size to 2\n","beam_output = model.generate(\n","    input_ids,\n","    max_length=50,\n","    num_beams=5,\n","    no_repeat_ngram_size=2,\n","    early_stopping=True\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))\n"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","이순신은 조선 중기의 무신이다.</s><s> 이 목록은 대한민국의 보물 제224호로 지정되어 있는 보물들이다. ---------------------------------------------- ┌───────────────────┐│ │ │ ▶ 번 호 : 1002/07 ▶ 등록일 :\n"]}]},{"cell_type":"markdown","metadata":{"id":"qXTJqkIlFR4L"},"source":["더이상 반복이 나타나지 않는 다는 것을 볼 수 있습니다. 하지만 n-gram 패널티는 신중하게 사용되어야 합니다. 예를 들면 city New York에 대해 생성된 기사는 n-gram을 사용하지 않는 것이 좋습니다. 2-gram을 사용하게 될 경우 시의 이름이 전체 텍스트에서 한 번만 나타나기 때문입니다.\n","\n","Beam search의 또 다른 중요한 특징은 생성된 Top beam을 비교하여 목적에 가장 적합한 Beam을 선택할 수 있다는 것입니다.\n","\n","Transformer에서 num_return_sequences 파라미터를 return 해야 하는 최대 num_beams 보다 작거나 같도록 설정합니다. `num_return_sequences <= num_beams`로 설정된 코드를 확인할 수 있습니다."]},{"cell_type":"code","metadata":{"id":"5ClO3VphqGp6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723616343891,"user_tz":-540,"elapsed":918,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"36a0b057-f6f7-48db-e083-6a176b313f4c"},"source":["# set return_num_sequences > 1\n","beam_outputs = model.generate(\n","    input_ids,\n","    max_length=50,\n","    num_beams=5,\n","    no_repeat_ngram_size=2,\n","    num_return_sequences=5,\n","    early_stopping=True\n",")\n","\n","# now we have 3 output sequences\n","print(\"Output:\\n\" + 100 * '-')\n","for i, beam_output in enumerate(beam_outputs):\n","  print(\"{}: {}\".format(i, tokenizer.decode(beam_output.tolist(), skip_special_tokens=True)))"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","0: 이순신은 조선 중기의 무신이다.</s><s> 이 목록은 대한민국의 음악인 윤도현이 작사, 작곡한 곡들을 모은 음반으로, 2011년 10월 30일 발매되었다. &lt;윤도현&gt;은 작곡가 겸 음반제작사인\n","1: 이순신은 조선 중기의 무신이다.</s><s> 이 목록은 대한민국의 음악인 윤도현이 작사, 작곡한 곡들을 모은 음반으로, 2011년 10월 30일 발매되었다. &lt;윤도현&gt;은 작곡가 겸 음반 기획자인\n","2: 이순신은 조선 중기의 무신이다.</s><s> 이 목록은 대한민국의 음악인 윤도현이 작사, 작곡한 곡들을 모은 음반으로, 2011년 10월 30일 발매되었다. &lt;윤도현&gt;은 작곡가 겸 음반 기획자\n","3: 이순신은 조선 중기의 무신이다.</s><s> 이 목록은 대한민국의 음악인 윤도현이 작사, 작곡한 곡들을 모은 음반으로, 2011년 10월 30일 발매되었다. &lt;윤도현&gt;은 작곡가 겸 음반 제작자인\n","4: 이순신은 조선 중기의 무신이다.</s><s> 이 목록은 대한민국의 음악인 윤도현이 작사, 작곡한 곡들을 모은 음반으로, 2011년 10월 30일 발매되었다. &lt;윤도현&gt;은 작곡가 겸 음반제작자인\n"]}]},{"cell_type":"markdown","metadata":{"id":"FyRYm1OUI0zN"},"source":["코드 결과를 통해 볼수 있듯이 5개의 Beam hypotheses는 서로 약간 다를 뿐이며 5개만 사용했을 경우 놀랄만한 결과는 아닙니다.\n","\n","개방형 생성에서는 Beam search가 최선의 선택사항이 아닐수 있는 몇 가지 이유가 최근에 제시되었습니다.\n","\n","- Beam search는 Machine translation 또는 Text summarization처럼 원하는 문장 생성 길이가 예측 가능한 Task에서는 잘 작동할 수 있습니다. 하지만 Dialog 또는 Story Generation Task처럼 출력길이가 크게 달라질 수 있는 개방형 생성에서는 원활하게 작동하지 않습니다. (\n","[Murray et al. (2018)](https://arxiv.org/abs/1808.10006), [Yang et al. (2018)](https://arxiv.org/abs/1808.09582))\n","\n","- Beam search은 반복 생성 문제에 취약합니다. 특히 Story Generation Task에서 n-gram또는 기타 패널티를 통해 문장을 제어하는 것이 어렵습니다. 왜냐하면 \"반복이 없는 구문\"과 \"n=gram반복 주기\" 사이에서 적당한 trade-off를 찾기 위해 많은 finetuning이 필요하기 때문입니다.\n","\n","- [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751) 논문에 따르면 고품질 인간 언어는 높은 확률의 다음 단어 분포를 따르지 않는다고 주장합니다. 쉽게 말하자면 인간입장에서 우리는 지루하거나 예측 가능한 문장이 아니라 우리를 놀라게 할 수 있는 문장생성을 원한다고 합니다. 저자는 모델이 인간 텍스트 대비 beam search text를 그래프로 보여주면서 beam search text가 그다지 놀랍지 않은 문장이라는 것을 보여줬습니다.\n","\n","\n","![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)"]},{"cell_type":"markdown","metadata":{"id":"hWQ2EWfYRFJZ"},"source":["### **Sampling**\n","\n","가장 기본적인 형태의 Sampling은 조건부 확률 분포에 따라 다음 단어 $w_t$를 무작위로 선택하는 것을 의미합니다.\n","\n","\n","$$w_t \\sim P(w|w_{1:t-1})$$\n","\n","위의 예를 들어, 아래 사진은 Sampling할 때 언어 생성을 시각화한 형태입니다.\n","\n","\n","![vanilla_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/sampling_search.png)\n","\n","\n","Sampling을 이용한 언어생성은 더이상 결정론적이지 않습니다. 단어\n","$\\text{\"car\"}$ 는 조건부확률 $P(w | \\text{\"The\"})$에서 샘플링 된 후, $P(w | \\text{\"The\"}, \\text{\"car\"})$에서 $\\text{\"drives\"}$를 샘플링 합니다.\n","\n","\n","\n","`transformers`에서 `do_sample=True`를 설정하고 `top_k=0`을 통해 *Top-K* sampling을 비활성화 합니다."]},{"cell_type":"code","metadata":{"id":"aRAz4D-Ks0_4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723616424771,"user_tz":-540,"elapsed":854,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"ea724d25-b5bb-456c-af19-1eaeef465776"},"source":["# activate sampling and deactivate top_k by setting top_k sampling to 0\n","sample_output = model.generate(\n","    input_ids,\n","    do_sample=True, # 완전 random sampling\n","    max_length=50,\n","    top_k=0 # w/o top_k 추출\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","이순신은 조선 중기의 무신이다.</s><s> 관우로(關六郞) 홍명주(洪命周), 소옹(廣), 홍낙(弘) 등으로 10핍(十)으로 한산도를 함락하였다.</s><s>\n"]}]},{"cell_type":"markdown","metadata":{"id":"WWjxm9otWZgY"},"source":["흥미롭게도 본문은 괜찮은 것 같지만 자세히 보면 매우 일관성 없는 문장입니다. *3-grams*의 *new hand sense* 와*local batte harness* 라는 문장은 이상하고 사람이 쓴것처럼 보이지 않습니다. 이것은 sampling word sequences를 할때 모델이 일관성없이 횡설수설하는 문장을 발생시키는 큰 문제입니다. ([Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751)).\n","\n","한가지 트릭은 [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max). 의 이른바 `temperature`를 낮추어 분포 $P(w|w_{1:t-1})$를 더 선명하게 만드는 것입니다. 높은 확률의 단어의 가능성은 증가시키고 낮은 확률의 단어 가능성은 감소시키는 효과가 있습니다.\n","\n","temperature를 적용한다면 다음과 같은 그림을 보일 수 있습니다.\n","\n","![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/sampling_search_with_temp.png?raw=true)\n","\n","step=1의 다음 단어 분포는 더욱 선명해졌기 때문에 단어 $\\text{\"car\"}$를 선택할 확률이 거의 없습니다.\n","\n","\n","`temperature=0.7`를 설정하여 라이브러리에서 분포를 어떻게 변화시키는지 알아보겠습니다."]},{"cell_type":"code","metadata":{"id":"WgJredc-0j0Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723616439744,"user_tz":-540,"elapsed":847,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"d7f46ed9-1c46-430b-a71e-882e08484898"},"source":["# use temperature to decrease the sensitivity to low probability candidates\n","sample_output = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    top_k=0,\n","    temperature=0.7\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","이순신은 조선 중기의 무신이다.</s><s> 그 후, 1693년(숙종 3) 스승인 송시열의 추천으로 증직되었다.</s><s> 1696년(숙종 26)에 증직으로 좌의정, 병조참판,\n"]}]},{"cell_type":"markdown","metadata":{"id":"wn_k6Bykf_vg"},"source":["이제 이상한 n-gram이 적고 출력 문장이 조금 더 일관성 있게 생성됩니다. temperature를 적용하면 분포가 덜 랜덤하지만 `temperature` $ \\to 0$,을 설정한다면 temperature가 적용된 sampling은 greedy decoding과 같아지며 이전과 동일한 문제를 겪습니다."]},{"cell_type":"markdown","metadata":{"id":"KDhbxGrehxlz"},"source":["### **Top-K Sampling**\n","\n","[Fan et. al (2018)](https://arxiv.org/pdf/1805.04833.pdf)\n","\n","***Top-K*** sampling은 간단하지만 매우 강력한 생플링 방식을 도입했습니다. . *Top-K* sampling에서 가장 가능성 높은 다음 단어는 필터링 되고 확률 질량은 K 다음 단어에만 재분배됩니다. GPT2는 Top-K Sampling방식을 채택했는데, 이것이 Story Gerneration Task에 성공한 이유중 하나입니다.\n","\n","Top-K Sampling을 더 잘 설명하기 위해 위의 예제에서 두 Sampling step에 사용되는 범위를 3단어에서 10단어로 확장합니다.\n","\n","![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n","\n","\n","K=6을 설정하면 두 Sampling steps에서 Sampling pool을 6개의 단어로 제한합니다. $V_{\\text{top-K}}$로 정의되는 가장 높은 6개의 단어로  sampling pool을 제한합니다.\n","\n","첫 step에서 전체 확률 질량의 2/3인 0.68정도에 해당하는 단어에서 디코딩되지만, 두번째 step에서 거의 모든 확률질량인 0.99에서 디코딩합니다.\n","\n","그럼에도 불구하고 그것이 두번째 sampling step에서 $\\text{\"not\", \"the\", \"small\", \"told\"}$ 와 같은 다소 이상한 후보들을 성공적으로 제거가 가능했습니다."]},{"cell_type":"code","metadata":{"id":"HBtDOdD0wx3l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723616933941,"user_tz":-540,"elapsed":863,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"10912ddf-1ff6-4696-8a58-d3d31a31379b"},"source":["# set top_k to 50\n","sample_output = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    top_k=50\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","이순신은 조선 중기의 무신이다.</s><s> 한편, 이 의원은 \"새누리당은 지금 박근혜 대통령의 국정철학에 동정이 많이 가지 않는가\"라고 전제하고, \"새누리당은 박 대통령이 민생에 관한 인식이 바르게 되어 있다는 것을 보여줘야하고 그 결과로 정권에서\n"]}]},{"cell_type":"markdown","metadata":{"id":"dzeNQJNhr3EH"},"source":["지금까지 기법중 가장 인간적으로 보이는 텍스트를 생성했습니다. Top-K Sampling의 한 가지 우려되는 점은 다음 단어 확률 분포 $P(w|w_{1:t-1})$에서 필터링된 단어 수를 동적으로 조정하지 않는 점입니다. 예를들면 위 그림에서 첫번째 step의 단어들은 전반적으로 평평한 분포에서 Sampling 되지만, 두번째 step의 어떤 단어들은 매우 Sharp한 분포에서 Sampling 될 수 있기 때문에 문제가 될 수 있습니다.\n","\n","\n","Step $t=1$에서 Top-K은 꽤 합리적인 후보처럼 보이는 $\\text{\"people\", \"big\", \"house\", \"cat\"}$을 샘플링하는 가능성을 배제합니다. 반면에 Step $t=2$에서 단어 Sample pool에 단어 $\\text{\"down\", \"a\"}$와 같은 부적절한 단어를 포함합니다. 그러므로 Sample pool이 고정크기 K로 제한되면 모형이 Sharp한 분포에서 횡설수설한 단어를 고를 위험이있고 평평한 분포에서는 문장의 창의성이 제한될 수 있습니다. ([Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751))"]},{"cell_type":"markdown","metadata":{"id":"wV6AHjet0Slr"},"source":["### **Top-p (nucleus) sampling**\n","\n","Top-p sampling은 가장 가능성 높은 단어 K 개에서만 Sample을 추출하는 방법이 아니라 누적확률이 확률 p를 초과하는 최소한의 단어 집합에서 Sample을 추출합니다.\n","\n","그 후 확률 질량이 단어 집합 사이에 재분배 됩니다. 이 방법은 다음 단어의 확률 분포에 따라 단어 집합의 크기가 동적으로 증가하거나 감소할 수 있습니다.\n","\n","![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/top_p_sampling.png?raw=true)\n","\n","\n","\n","$p=0.92$을 설정할 경우, 상위 p Sample 추출은 $V_{\\text{top-p}}$로 정의된 확률 질량의 $p=92\\%$를 초과할 최소 단어 수를 선택합니다.\n","첫번째 예에서 가장 가능성 높은 9개의 단어 (\"nice\", \"dog\", \"car\" ...  house)가 포함된 반면, 두번째 예에서는 상위 3개의 단어(\"drives\", \"is\", \"turns\")만 선택해도 92%를 초과하게 됩니다. 즉 높은 확률의 단어에만 Sampling 하고 그렇지 않은 단어는 Sampling할 확률이 매우 적습니다."]},{"cell_type":"code","metadata":{"id":"EvwIc7YAx77F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723616995580,"user_tz":-540,"elapsed":907,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"dbf3f218-eeff-4ed0-dbe5-8f9c6f2b02ea"},"source":["# deactivate top_k sampling and sample only from 92% most likely words\n","sample_output = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    top_p=0.92,\n","    top_k=0\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","이순신은 조선 중기의 무신이다.</s><s> 경남 권역을 대표하는 대학인 창원산업과학고 국제고인 용연산중학교에서 2000년 개교하였다.</s><s> 이 학교에는 3,900여명의 학생과 3,100명의 내신성적이 각각 350명의\n"]}]},{"cell_type":"markdown","metadata":{"id":"894AH8E03pv7"},"source":["이론적으로는 Top-p가 Top-K보다 더 성능이 좋아 보이지만, 두 방법 모두 실제로 잘 작동합니다. Top-p는 또한 Top-K와 함께 사용될 수 있는데, 이것은 매우 낮은 순위의 단어를 피하면서도 일부 동적 선택을 허용할 수 있습니다.\n","\n","독립적으로 샘플링된 다중 출력을 얻기 위하여 파라미터를 다시 설정하도록 코드를 구성 할 수도 있습니다. `num_return_sequences > 1`:"]},{"cell_type":"code","metadata":{"id":"3kY8P9VG8Gi9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723617009739,"user_tz":-540,"elapsed":938,"user":{"displayName":"이승준","userId":"02916235139337086073"}},"outputId":"4348b8d9-5e5e-48b5-f134-fe0358076cf0"},"source":["# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n","sample_outputs = model.generate(\n","    input_ids,\n","    do_sample=True,\n","    max_length=50,\n","    top_k=20,\n","    top_p=0.90,\n","    num_return_sequences=3\n",")\n","\n","print(\"Output:\\n\" + 100 * '-')\n","for i, sample_output in enumerate(sample_outputs):\n","  print(\"{}: {}\".format(i, tokenizer.decode(sample_output.tolist(), skip_special_tokens=True)))"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n","----------------------------------------------------------------------------------------------------\n","0: 이순신은 조선 중기의 무신이다.</s><s> 하지만, 당시 그는 이미 그처럼 군주가 아닌 평범한 사람이었다.</s><s> 하지만, 그의 어머니, 그의 남동생, 그 누이동생, 그리고 그의 삼촌이었던 아버지, 그리고 그리고 그의 어머니까지\n","1: 이순신은 조선 중기의 무신이다.</s><s> 이후, 로키츠부르크에서, 그는 이 후, 리히텐베르크, 바이에른, 도르트문트, 베를린을 연고로 한 바이에른에서 활약하였다.</s><s> 그러나, 그의 FC 바이에른 뮌헨은\n","2: 이순신은 조선 중기의 무신이다.</s><s> 이에 앞서 지난 9일 오전 9시 45분쯤 서울 노원구 상계동에 있는 한 음식점에서 한 50대 남성이 흉기를 들고 나와 경찰과 대치 중이다.</s><s> 당시 이 남성은 흉기를 들고 있던 경찰관에게 “내가 죽\n"]}]},{"cell_type":"markdown","source":["### **Analysis**\n"],"metadata":{"id":"8sX8RzLHRrc2"}},{"cell_type":"markdown","source":["모델을 돌리고 분석한 점을 자유롭게 적어주세요!\n","\n","Your Opinion:\n","\n","여태까지 사용했던 디코딩 방법론들을 비교하며 볼 수 있어 유익했다.\n","\n","search 기반 방법론은 본문대로 다소 유동적이지 못한 결과가 나오며,\n","\n","샘플링 기반 방법론은 유동적이나 개연성을 찾기 어렵다는 생각이 들었다. 이를 목적에 기반해서 잘 선택 & 조절 해야함이 느껴졌다.\n","\n","\n","\n","현재 진행중인 프로젝트에서도 학습 후 inference 과정에서 단순단어 반복이 출력되거나 정성적으로 좋지 못한 결과가 나오는 경우가 많다.\n","\n","해당 과정에서 학습 파라미터 뿐만 아니라 디코딩 과정을 디테일하게 뜯어볼 필요가 있을 것 같다."],"metadata":{"id":"1PYW5ejORuMj"}},{"cell_type":"markdown","metadata":{"id":"etg-3zdz4jQK"},"source":["### **Conclusion**\n","\n","GPT를 포함한 생성형 언어모델을 활용하면서 중요하다 느낀 점들을 자유롭게 적어주세요!\n","\n","Your Opinion:\n","\n","목적에 따른 모델선택이 중요하다고 생각했다.\n","pretrain 된 데이터의 성질이나, 모델 자체의 특징, 제공하는 토크나이저 등을 살필 줄 알아야 한다.  \n","transformer 기반 모델들이 갈수록 무거워지는 경향이 있어 학습환경도 중요하다는 생각을 했다.\n","\n","또한 추가로 프로젝트 아키텍쳐를 익히는 과정도 중요하다고 생각했다.\n","대회참여나 레퍼런스를 볼 때 쥬피터 노트북 형식이 프로젝트 아키텍쳐로 구성된 경우도 많은 것 같다.    \n"]},{"cell_type":"markdown","source":["### **References**\n","\n","*   ([Welleck et al. (2020)](https://arxiv.org/abs/2002.02492))\n","*   ([Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf))"],"metadata":{"id":"U9GVxh6USEU1"}},{"cell_type":"code","source":[],"metadata":{"id":"dSG18AsDs-gU"},"execution_count":null,"outputs":[]}]}