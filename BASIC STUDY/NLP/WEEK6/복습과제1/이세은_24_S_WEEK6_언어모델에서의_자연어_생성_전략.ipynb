{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSfah6SLfhVG"
      },
      "source": [
        "# 텍스트 생성 방법 : Transformers를 이용한 언어생성에 서로 다른 디코딩 방법 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l31dGxwgTMZ"
      },
      "source": [
        "#소개\n",
        "최근 몇 년 동안, OpenAI의 유명한 모델 GPT2처럼 수백만 개의 웹 페이지에서 훈련된 Transformer 기반 대형 언어 모델의 등장으로 개방형 언어 생성에 대한 관심이 증가하고 있습니다. 개선된 Transformer 아키텍처와 대규모 unsupervised training data 외에도, 더 나은 Decoding 방법도 중요한 역할을 했습니다.\n",
        "\n",
        "이 실습 자료는 다양한 Decoding 전략에 대한 간략한 개요를 제공합니다.\n",
        "\n",
        "다음 모든 기능은 auto-regressive 언어 생성에 사용할 수 있습니다. 요약하자면, auto-regressive 언어 생성은 word sequence의 확률 분포가 다음 단어 분포에 관한 P식의 결과로 분해될 수 있고\n",
        "\n",
        "$$ P(w_{1:T} | W_0 ) = \\prod_{t=1}^T P(w_{t} | w_{1: t-1}, W_0) \\text{ ,with }  w_{1: 0} = \\emptyset, $$\n",
        "\n",
        "W0가 초기 Context word sequence의 결과로 분해될 수 있다는 가정에 기초합니다.\n",
        "\n",
        "Word sequence의 길이 T는 보통 즉시 결정되며, P식에서 EOS 토큰이 생성된 timestep t=T와 부합합니다.\n",
        "\n",
        "이번 실습에서는 가장 두드러진 decoding 방법으로 Greedy search, Beam search, Top-K sampling, Top-p sampling을 주로 둘러볼 것입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz0J-hPCoZEl"
      },
      "source": [
        "Transformers를 설치하고 Model을 load하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XbzZ_IVTtoQe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "786fe9a2-695e-4f43-be6e-088d90594fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4jS5zNKknHT"
      },
      "source": [
        "이번 실습을 위해 SKT에서 공개한 KoGPT-2 모델을 사용해보도록 하겠습니다 :-)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b1eVsFQQgdBk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe60152-516b-4486-9237-21eb5f84d4b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected operating system as Ubuntu/jammy.\n",
            "Checking for curl...\n",
            "Detected curl...\n",
            "Checking for gpg...\n",
            "Detected gpg...\n",
            "Detected apt version as 2.4.12\n",
            "Running apt-get update... done.\n",
            "Installing apt-transport-https... done.\n",
            "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
            "Importing packagecloud gpg key... Packagecloud gpg key imported to /etc/apt/keyrings/github_git-lfs-archive-keyring.gpg\n",
            "done.\n",
            "Running apt-get update... done.\n",
            "\n",
            "The repository is setup! You can now install packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following packages will be upgraded:\n",
            "  git-lfs\n",
            "1 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 7,420 kB of archives.\n",
            "After this operation, 6,051 kB of additional disk space will be used.\n",
            "Get:1 https://packagecloud.io/github/git-lfs/ubuntu jammy/main amd64 git-lfs amd64 3.5.1 [7,420 kB]\n",
            "Fetched 7,420 kB in 0s (18.3 MB/s)\n",
            "(Reading database ... 123598 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_3.5.1_amd64.deb ...\n",
            "Unpacking git-lfs (3.5.1) over (3.0.2-1ubuntu0.2) ...\n",
            "Setting up git-lfs (3.5.1) ...\n",
            "Git LFS initialized.\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Git LFS initialized.\n",
            "Cloning into 'kogpt2'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Total 56 (delta 0), reused 0 (delta 0), pack-reused 56 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (56/56), 1.53 MiB | 4.05 MiB/s, done.\n",
            "Filtering content: 100% (3/3), 1.41 GiB | 61.96 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!apt-get install git-lfs\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/taeminlee/kogpt2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.token_to_id('<pad>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KTUB4_iywZc",
        "outputId": "2a6975b6-da3d-4efd-9fd3-eb410d3cd728"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ckGerDmxgjOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8a66b9f-fe3b-4d7f-8088-0b02e80b3e33"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50000, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import torch\n",
        "from tokenizers import SentencePieceBPETokenizer\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "\n",
        "# Byte Pair Encoding (BPE) 알고리즘 사용하는 토크나이저\n",
        "# vocab.json: 각 토큰을 고유한 ID로 매핑\n",
        "# merges.txt: BPE 병합 규칙.\n",
        "tokenizer = SentencePieceBPETokenizer(\"/content/kogpt2/vocab.json\", \"/content/kogpt2/merges.txt\")\n",
        "\n",
        "# GPT2Config: GPT2 모델 설정을 위한 클래스.\n",
        "# vocab_size=50000: 모델이 최대 50000개의 고유한 토큰을 처리할 수 있다\n",
        "config = GPT2Config(vocab_size=50000)\n",
        "# 패딩 토큰의 ID 설정\n",
        "config.pad_token_id = tokenizer.token_to_id('<pad>')\n",
        "# GPT2 Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n",
        "model = GPT2LMHeadModel(config)\n",
        "# 사전 학습된 모델의 가중치가 저장된 파일 경로\n",
        "model_dir = '/content/kogpt2/pytorch_model.bin'\n",
        "# load_state_dict: 저장된 모델의 가중치를 GPU로 불러와 모델에 로드\n",
        "# strict=True: state_dict에 있는 모든 키가 모델의 파라미터 이름과 정확히 일치해야 함\n",
        "# strict=False: 일부 키가 모델에 없더라도 에러를 발생시키지 않는다\n",
        "model.load_state_dict(torch.load(model_dir, map_location='cuda'), strict=False)\n",
        "model.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUKD7GJlyyxq",
        "outputId": "13986447-6d5b-4c41-c463-aa0831a70ecc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Config {\n",
              "  \"activation_function\": \"gelu_new\",\n",
              "  \"attn_pdrop\": 0.1,\n",
              "  \"bos_token_id\": 50256,\n",
              "  \"embd_pdrop\": 0.1,\n",
              "  \"eos_token_id\": 50256,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"layer_norm_epsilon\": 1e-05,\n",
              "  \"model_type\": \"gpt2\",\n",
              "  \"n_embd\": 768,\n",
              "  \"n_head\": 12,\n",
              "  \"n_inner\": null,\n",
              "  \"n_layer\": 12,\n",
              "  \"n_positions\": 1024,\n",
              "  \"pad_token_id\": 3,\n",
              "  \"reorder_and_upcast_attn\": false,\n",
              "  \"resid_pdrop\": 0.1,\n",
              "  \"scale_attn_by_inverse_layer_idx\": false,\n",
              "  \"scale_attn_weights\": true,\n",
              "  \"summary_activation\": null,\n",
              "  \"summary_first_dropout\": 0.1,\n",
              "  \"summary_proj_to_labels\": true,\n",
              "  \"summary_type\": \"cls_index\",\n",
              "  \"summary_use_proj\": true,\n",
              "  \"transformers_version\": \"4.42.4\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 50000\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYdGXD4opfuk"
      },
      "source": [
        "### **Greedy Search**\n",
        "\n",
        "Greedy search는 단순히 가장 높은 확률을 가진 단어를 다음 단어로 선택합니다.   \n",
        "$w_t = argmax_{w}P(w | w_{1:t-1})$ 는 각각의 timestep $t$ 입니다. 아래 그림은 greedy search을 보여줍니다.   \n",
        "\n",
        "![Greedy Search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/greedy_search.png)\n",
        "\n",
        "알고리즘은 단어 \"The\"에서 시작하여 다음 단어로 가장 높은 확률의 단어인 \"nice\" 등을 선택하는 탐욕법입니다. 그러므로 최종적으로 생성된 Word sequence는 \"The\", \"nice\", \"woman\"이며 전반적인 확률은 0.5x0.4 = 0.2로 계산됩니다.\n",
        "\n",
        "다음 문맥 (\"I\", \"enjoy\", \"walking\", \"with\", \"my\", \"cute\", \"dog\")에서 GPT2를 사용하여 Word sequence를 생성할 수 있습니다.\n",
        "\n",
        "Transformers에서 다음과 같은 greedy search를 사용하는 방법을 살펴보겠습니다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 코드 쪼개서 보기"
      ],
      "metadata": {
        "id": "XfH2bwkW1P6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"이순신은 조선 중기의 무신이다.\", add_special_tokens=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwq-PFCX0-Ve",
        "outputId": "e9dd8384-f419-4a59-abed-a9dcb49a4423"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"이순신은 조선 중기의 무신이다.\", add_special_tokens=False).ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8olCNL2N1H7B",
        "outputId": "de2e7500-1d54-4f9b-ccae-586b635dc94d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10925, 6647, 1117, 40249, 39793, 128, 47440]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"이순신은 조선 중기의 무신이다.\", add_special_tokens=False).tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXRv4D-m1LAE",
        "outputId": "7c425de6-c703-442e-be45-ee8c3021bcf0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁이순', '신은', '▁조선', '▁중기의', '▁무신', '이다', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\", add_special_tokens=False).ids).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTePu8UU1ppo",
        "outputId": "839a95c9-49a6-448d-a96c-fd094cc93a10"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([7])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\", add_special_tokens=False).ids).unsqueeze(0).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7zGhxCZ12-g",
        "outputId": "10e76851-15e9-4b71-8f56-572f9c98968a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "greedy_output.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Scf0IxSz3Scn",
        "outputId": "c5b1f32c-8411-4cff-a17c-87bd9411632e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(greedy_output.tolist()[0], skip_special_tokens=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "n0p4yHNz3cZp",
        "outputId": "426055bf-9858-4583-fce4-4caba0f97942"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'이순신은 조선 중기의 무신이다.</s><s> 이 때, 그는 자신의 스승이자 스승인 고경명(高敬命)을 따라, 그 후, 그의 제자였던 고경명(高敬命)을 따라, 그 후, 그의 제자였던 고경명(高敬命)을 따라, 그 후, 그의 제자였던 고경명(高敬命)을 따라, 그 후, 그의 제자였던 고경명(高敬命命)을 따라, 그 후'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "OWLd_J6lXz_t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "7bdddefa-7b61-4f9b-f16c-67f47ffd424a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'이순신은 조선 중기의 무신이다.</s><s> 이 목록은 대한민국의 가수 목록이다.</s><s> 이 목록은 대한민국의 가수 목록이다.</s><s> 이 목록은 대한민국의 가수 목록이다.</s><s> 이 목록은 대한민국의 가수 목록이다.</s><s> 이 목록은 대한민국의 가수 목록'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# encode context the generation is conditioned on\n",
        "def tokenizing(text):\n",
        "    return torch.tensor(tokenizer.encode(text, add_special_tokens=False).ids).unsqueeze(0).to('cuda')\n",
        "\n",
        "input_ids = tokenizing(\"이순신은 조선 중기의 무신이다.\")\n",
        "\n",
        "# generate text until the output length (which includes the context length) reaches 50\n",
        "# 생성 모델은 generate 함수를 통해 다음 token을 생성해낼 수 있습니다.\n",
        "greedy_output = model.generate(input_ids, max_length=50)\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "tokenizer.decode(greedy_output.tolist()[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNIDMgkds-VO"
      },
      "source": [
        "GPT2로 짧은 텍스트를 생성했습니다.   \n",
        "생성된 단어 문맥은 합리적이지만 모델은 비슷한 단어를 반복하는 수준입니다.   \n",
        "이러한 현상은 일반적인 언어생성 모델에서 나타나는 공통된 문제이며 특히 Greedy search와 Beam search에서 훨씬 더 그런 현상이 두드러져 보입니다. (Vijayakumar et al., 2016 and Shao et al., 2017에서 관련 내용을 확인 할 수 있습니다.)   \n",
        "\n",
        "Greedy search의 주요 단점은 그림에서 볼수 있듯이 낮은 확률 단어 이후에 나올수 있는 더 높은 확률의 단어를 놓친다는 점입니다.\n",
        "\n",
        "예를 들면 단어 \"has\"는 0.9의 높은 조건부 확률을 가지고 있지만, 첫 검색단어중 두번째로 높은 조건부 확률 단어인 \"dog\" 이후에  숨어있는 형태입니다. 따라서 Greedy search는 \"The\",\"dog\",\"has\"라는 Word sequence를 놓치게 됩니다.\n",
        "\n",
        "이러한 문제는 Beam search에서 완화할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y73RcrRPn-Bn"
      },
      "source": [
        "### **Beam search**\n",
        "\n",
        "Beam search는 각 Time step에서 가장 확률이 높은 Hypotheses의 num_beams를 유지하고 결국 전체 확률이 가장 높은 hypothesis를 선택하는 것으로 숨겨진 높은 확률 Word sequence를 놓칠 위험을 줄입니다.\n",
        "\n",
        "`num_beams =2`라고 가정하고 Toy example을 설명하겠습니다.\n",
        "\n",
        "![Beam search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n",
        "\n",
        "Time step=1일때, Beam search는 가장 가능성 높은 Hypothesis \"The\",\"nice\"외에도 두번째로 가능성 높은 Hypothesis인 \"The\",\"dog\"를 추적합니다.\n",
        "\n",
        "Time step=2일때, Beam search는 Word sequence 확률 0.2를 가진 (\"The\",\"nice\",\"woman\") 보다 확률 0.36을 가진 (\"The\", \"dog\", \"has\")가 높다는 것을 찾습니다. 이것으로 Toy example에서 가장 가능성 높은 Word sequence를 발견 할 수 있다는 것을 보였습니다.\n",
        "\n",
        "Beam search는 항상 Greedy search보다 높은 확률의 결과 Sequence를 찾는 것이 가능합니다. 그러나 이것이 가장 가능성 높은 결과를 찾은 것이라고는 보장할 수 없습니다.\n",
        "\n",
        "`transformers`에서 Beam search를 사용하는 방법을 살펴볼 것입니다. 모든 Beam Hypotheses가 EOS토큰에 닿으면 생성이 완료되도록 `num_beams > 1` 과 `eqrly_stopping=True`로 파라미터를 설정합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "R1R5kx30Ynej",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "be8ba6ec-5990-4111-80d9-d333bcbd6262"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'이순신은 조선 중기의 무신이다.</s><s> 그 후, 이조참의(吏曹參議)를 거쳐 이조참판(吏曹參判)에 이르렀다.</s><s> 이조참판(吏曹參判)을 거쳐 이조참'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# activate beam search and early_stopping\n",
        "beam_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    num_beams=5,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "display(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCeY_atbBMr7"
      },
      "source": [
        "결과는 틀림없이 더 유창하게 보이지만 여전히 동일한 Word sequence를 반복하는 문제를 포함합니다.\n",
        "\n",
        "단순한 해결법은 Paulus et al. (2017)과 Klein et al. (2017)의 논문에서 제안된 n-grams 패널티를 도입하는 것입니다. 가장 일반적인 n-grams 패널티는 이미 나타난 n-gram에 대해 다음 단어로 생성될 확률을 0으로 설정하여 두번 나타나지 않도록 하는 방법입니다.\n",
        "\n",
        "`no_repeat_ngram_size=2`을 설정한다면 2-gram이 두번 나타나는 것을 막을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "jy3iVJgfnkMi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "21916107-c4d4-4d0c-9679-d3f0b39172b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'이순신은 조선 중기의 무신이다.</s><s> 그 후, 그 해 11월 15일(음력 1월 16일)에 향년 60세의 나이로 세상을 떠나자, 향년 57세(당시 향년 64세)의 나이에 세상을 떠났다. &lt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# set no_repeat_ngram_size to 2\n",
        "beam_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,  # 연속된 두 개의 단어 조합이 반복되는 것을 방지\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "display(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXTJqkIlFR4L"
      },
      "source": [
        "더이상 반복이 나타나지 않는 다는 것을 볼 수 있습니다. 하지만 n-gram 패널티는 신중하게 사용되어야 합니다. 예를 들면 city New York에 대해 생성된 기사는 n-gram을 사용하지 않는 것이 좋습니다. 2-gram을 사용하게 될 경우 시의 이름이 전체 텍스트에서 한 번만 나타나기 때문입니다.\n",
        "\n",
        "Beam search의 또 다른 중요한 특징은 생성된 Top beam을 비교하여 목적에 가장 적합한 Beam을 선택할 수 있다는 것입니다.\n",
        "\n",
        "Transformer에서 num_return_sequences 파라미터를 return 해야 하는 최대 num_beams 보다 작거나 같도록 설정합니다. `num_return_sequences <= num_beams`로 설정된 코드를 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "5ClO3VphqGp6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "76d56d38-c565-4464-df4c-fd553eb80dfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'0: 이순신은 조선 중기의 무신이다.</s><s> 이 목록은 대한민국의 보물 제194호로 지정되어 있는 조선시대의 보물급 문화재를 정리한 것으로, 현재 국립중앙박물관에 소장되어 있는 문화재 중에서 보물 1점, 보물 14점 등 총 30점의 문화재가 포함되어 있다'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'1: 이순신은 조선 중기의 무신이다.</s><s> 이 목록은 대한민국의 보물 제194호로 지정되어 있는 조선시대의 보물급 문화재를 정리한 것으로, 현재 국립중앙박물관에 소장되어 있는 문화재 중에서 보물 1점, 보물 2점 등 총 3점의 문화재가 포함되어 있다'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'2: 이순신은 조선 중기의 무신이다.</s><s> 이 목록은 대한민국의 보물 제194호로 지정되어 있는 조선시대의 보물급 문화재를 정리한 것으로, 현재 국립중앙박물관에 소장되어 있는 문화재 중에서 보물 1점, 보물 14점 등 총 30점의 문화재가 있다.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'3: 이순신은 조선 중기의 무신이다.</s><s> 이 목록은 대한민국의 보물 제194호로 지정되어 있는 조선시대의 보물급 문화재를 정리한 것으로, 현재 국립중앙박물관에 소장되어 있는 문화재 중에서 보물 1점, 보물 2점 등 총 3점의 문화재가 지정되어 있다'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'4: 이순신은 조선 중기의 무신이다.</s><s> 이 목록은 대한민국의 보물 제194호로 지정되어 있는 조선시대의 보물급 문화재를 정리한 것으로, 현재 국립중앙박물관에 소장되어 있는 문화재 중에서 보물 1점, 보물 14점 등 총 30점의 문화재가 지정되어 있다'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# set num_return_sequences > 1\n",
        "beam_outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_return_sequences=5,  # 5개의 서로 다른 출력을 생성\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# now we have 5 output sequences\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "  display(\"{}: {}\".format(i, tokenizer.decode(beam_output.tolist(), skip_special_tokens=True)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyRYm1OUI0zN"
      },
      "source": [
        "코드 결과를 통해 볼수 있듯이 5개의 Beam hypotheses는 서로 약간 다를 뿐이며 5개만 사용했을 경우 놀랄만한 결과는 아닙니다.\n",
        "\n",
        "개방형 생성에서는 Beam search가 최선의 선택사항이 아닐수 있는 몇 가지 이유가 최근에 제시되었습니다.\n",
        "\n",
        "- Beam search는 Machine translation 또는 Text summarization처럼 원하는 문장 생성 길이가 예측 가능한 Task에서는 잘 작동할 수 있습니다. 하지만 Dialog 또는 Story Generation Task처럼 출력길이가 크게 달라질 수 있는 개방형 생성에서는 원활하게 작동하지 않습니다. (\n",
        "[Murray et al. (2018)](https://arxiv.org/abs/1808.10006), [Yang et al. (2018)](https://arxiv.org/abs/1808.09582))\n",
        "\n",
        "- Beam search은 반복 생성 문제에 취약합니다. 특히 Story Generation Task에서 n-gram또는 기타 패널티를 통해 문장을 제어하는 것이 어렵습니다. 왜냐하면 \"반복이 없는 구문\"과 \"n=gram반복 주기\" 사이에서 적당한 trade-off를 찾기 위해 많은 finetuning이 필요하기 때문입니다.\n",
        "\n",
        "- [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751) 논문에 따르면 고품질 인간 언어는 높은 확률의 다음 단어 분포를 따르지 않는다고 주장합니다. 쉽게 말하자면 인간입장에서 우리는 지루하거나 예측 가능한 문장이 아니라 우리를 놀라게 할 수 있는 문장생성을 원한다고 합니다. 저자는 모델이 인간 텍스트 대비 beam search text를 그래프로 보여주면서 beam search text가 그다지 놀랍지 않은 문장이라는 것을 보여줬습니다.\n",
        "\n",
        "\n",
        "![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWQ2EWfYRFJZ"
      },
      "source": [
        "### **Sampling**\n",
        "\n",
        "가장 기본적인 형태의 Sampling은 조건부 확률 분포에 따라 다음 단어 $w_t$를 무작위로 선택하는 것을 의미합니다.\n",
        "\n",
        "\n",
        "$$w_t \\sim P(w|w_{1:t-1})$$\n",
        "\n",
        "위의 예를 들어, 아래 사진은 Sampling할 때 언어 생성을 시각화한 형태입니다.\n",
        "\n",
        "\n",
        "![vanilla_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/sampling_search.png)\n",
        "\n",
        "\n",
        "Sampling을 이용한 언어생성은 더이상 결정론적이지 않습니다. 단어\n",
        "$\\text{\"car\"}$ 는 조건부확률 $P(w | \\text{\"The\"})$에서 샘플링 된 후, $P(w | \\text{\"The\"}, \\text{\"car\"})$에서 $\\text{\"drives\"}$를 샘플링 합니다.\n",
        "\n",
        "\n",
        "\n",
        "`transformers`에서 `do_sample=True`를 설정하고 `top_k=0`을 통해 *Top-K* sampling을 비활성화 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "aRAz4D-Ks0_4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "0bc93049-aca8-482a-b359-4f07391b8c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'이순신은 조선 중기의 무신이다.</s><s> 그러나 외교적 측면에서는 문제 없이 지속되어야 한다.</s><s> 대표적 접인성도국은 바로 태국의 국토지화와 태국의 나라인, 그리고샨이다.</s><s> 이를 명백하게 인계하는 물통의 역할이'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,  # 완전 random sampling\n",
        "    max_length=50,\n",
        "    top_k=0  # w/o top_k 추출\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "display(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWjxm9otWZgY"
      },
      "source": [
        "흥미롭게도 본문은 괜찮은 것 같지만 자세히 보면 매우 일관성 없는 문장입니다. *3-grams*의 *new hand sense* 와*local batte harness* 라는 문장은 이상하고 사람이 쓴것처럼 보이지 않습니다. 이것은 sampling word sequences를 할때 모델이 일관성없이 횡설수설하는 문장을 발생시키는 큰 문제입니다. ([Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751)).\n",
        "\n",
        "한가지 트릭은 [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max). 의 이른바 `temperature`를 낮추어 분포 $P(w|w_{1:t-1})$를 더 선명하게 만드는 것입니다. 높은 확률의 단어의 가능성은 증가시키고 낮은 확률의 단어 가능성은 감소시키는 효과가 있습니다.\n",
        "\n",
        "temperature를 적용한다면 다음과 같은 그림을 보일 수 있습니다.\n",
        "\n",
        "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/sampling_search_with_temp.png?raw=true)\n",
        "\n",
        "step=1의 다음 단어 분포는 더욱 선명해졌기 때문에 단어 $\\text{\"car\"}$를 선택할 확률이 거의 없습니다.\n",
        "\n",
        "\n",
        "`temperature=0.7`를 설정하여 라이브러리에서 분포를 어떻게 변화시키는지 알아보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "WgJredc-0j0Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "936ceef9-fd34-4d0d-98b2-5610cea94d3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'이순신은 조선 중기의 무신이다.</s><s> 그러나 그 아이가 죽자, 그가 어머니에게 죽은 후, 곧이어 아들 장수(長壽)가 되어 다시 군대를 정비하여, 성을 포기하고 자신의 주군으로 삼았다.</s><s> 장수(長壽)는'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=50,\n",
        "    top_k=0,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "display(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn_k6Bykf_vg"
      },
      "source": [
        "이제 이상한 n-gram이 적고 출력 문장이 조금 더 일관성 있게 생성됩니다. temperature를 적용하면 분포가 덜 랜덤하지만 `temperature` $ \\to 0$을 설정한다면 temperature가 적용된 sampling은 greedy decoding과 같아지며 이전과 동일한 문제를 겪습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDhbxGrehxlz"
      },
      "source": [
        "### **Top-K Sampling**\n",
        "\n",
        "[Fan et. al (2018)](https://arxiv.org/pdf/1805.04833.pdf)\n",
        "\n",
        "***Top-K*** sampling은 간단하지만 매우 강력한 생플링 방식을 도입했습니다. *Top-K* sampling에서 가장 가능성 높은 다음 단어는 필터링 되고 확률 질량은 K 다음 단어에만 재분배됩니다. GPT2는 Top-K Sampling방식을 채택했는데, 이것이 Story Gerneration Task에 성공한 이유중 하나입니다.\n",
        "\n",
        "Top-K Sampling을 더 잘 설명하기 위해 위의 예제에서 두 Sampling step에 사용되는 범위를 3단어에서 10단어로 확장합니다.\n",
        "\n",
        "![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n",
        "\n",
        "\n",
        "K=6을 설정하면 두 Sampling steps에서 Sampling pool을 6개의 단어로 제한합니다. $V_{\\text{top-K}}$로 정의되는 가장 높은 6개의 단어로  sampling pool을 제한합니다.\n",
        "\n",
        "첫 step에서 전체 확률 질량의 2/3인 0.68정도에 해당하는 단어에서 디코딩되지만, 두번째 step에서 거의 모든 확률질량인 0.99에서 디코딩합니다.\n",
        "\n",
        "그럼에도 불구하고 그것이 두번째 sampling step에서 $\\text{\"not\", \"the\", \"small\", \"told\"}$ 와 같은 다소 이상한 후보들을 성공적으로 제거가 가능했습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "HBtDOdD0wx3l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "befbbfbf-33a8-4e33-98bc-d21e4a9ff3f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'이순신은 조선 중기의 무신이다.</s><s> 한편 이 전 대통령은 임기중 한 차례도 현직 대통령을 해먹은 적이 없으며, 다른 전직 대통령들도 퇴임 후 거취를 두고 저울질 하는 것으로 알려졌다.</s><s> 이 전 대통령이 퇴임 후 귀국 후에는 자신의'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# set top_k to 50\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=50,\n",
        "    top_k=50\n",
        ")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "display(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzeNQJNhr3EH"
      },
      "source": [
        "지금까지 기법중 가장 인간적으로 보이는 텍스트를 생성했습니다. Top-K Sampling의 한 가지 우려되는 점은 다음 단어 확률 분포 $P(w|w_{1:t-1})$에서 필터링된 단어 수를 동적으로 조정하지 않는 점입니다. 예를들면 위 그림에서 첫번째 step의 단어들은 전반적으로 평평한 분포에서 Sampling 되지만, 두번째 step의 어떤 단어들은 매우 Sharp한 분포에서 Sampling 될 수 있기 때문에 문제가 될 수 있습니다.\n",
        "\n",
        "\n",
        "Step $t=1$에서 Top-K은 꽤 합리적인 후보처럼 보이는 $\\text{\"people\", \"big\", \"house\", \"cat\"}$을 샘플링하는 가능성을 배제합니다. 반면에 Step $t=2$에서 단어 Sample pool에 단어 $\\text{\"down\", \"a\"}$와 같은 부적절한 단어를 포함합니다. 그러므로 Sample pool이 고정크기 K로 제한되면 모형이 Sharp한 분포에서 횡설수설한 단어를 고를 위험이있고 평평한 분포에서는 문장의 창의성이 제한될 수 있습니다. ([Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV6AHjet0Slr"
      },
      "source": [
        "### **Top-p (nucleus) sampling**\n",
        "\n",
        "Top-p sampling은 가장 가능성 높은 단어 K 개에서만 Sample을 추출하는 방법이 아니라 누적확률이 확률 p를 초과하는 최소한의 단어 집합에서 Sample을 추출합니다.\n",
        "\n",
        "그 후 확률 질량이 단어 집합 사이에 재분배 됩니다. 이 방법은 다음 단어의 확률 분포에 따라 단어 집합의 크기가 동적으로 증가하거나 감소할 수 있습니다.\n",
        "\n",
        "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/top_p_sampling.png?raw=true)\n",
        "\n",
        "\n",
        "\n",
        "$p=0.92$을 설정할 경우, 상위 p Sample 추출은 $V_{\\text{top-p}}$로 정의된 확률 질량의 $p=92\\%$를 초과할 최소 단어 수를 선택합니다.\n",
        "첫번째 예에서 가장 가능성 높은 9개의 단어 (\"nice\", \"dog\", \"car\" ...  house)가 포함된 반면, 두번째 예에서는 상위 3개의 단어(\"drives\", \"is\", \"turns\")만 선택해도 92%를 초과하게 됩니다. 즉 높은 확률의 단어에만 Sampling 하고 그렇지 않은 단어는 Sampling할 확률이 매우 적습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "EvwIc7YAx77F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "977f20a2-de03-4291-ab14-1253cab07d1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'이순신은 조선 중기의 무신이다.</s><s> 사대부에서 감찰관을 거쳐 현재의 중앙보훈요원으로 재직하였다.</s><s> 인천 화성을 수색하던 중 한 적이 군사분계선을 넘어서 한강에서 침몰하는 북한 잠수정의 시신을 발견하였는데,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# deactivate top_k sampling and sample only from 92% most likely words\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=50,\n",
        "    top_p=0.92,\n",
        "    top_k=0\n",
        ")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "display(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "894AH8E03pv7"
      },
      "source": [
        "이론적으로는 Top-p가 Top-K보다 더 성능이 좋아 보이지만, 두 방법 모두 실제로 잘 작동합니다. Top-p는 또한 Top-K와 함께 사용될 수 있는데, 이것은 매우 낮은 순위의 단어를 피하면서도 일부 동적 선택을 허용할 수 있습니다.\n",
        "\n",
        "독립적으로 샘플링된 다중 출력을 얻기 위하여 파라미터를 다시 설정하도록 코드를 구성 할 수도 있습니다. `num_return_sequences > 1`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "3kY8P9VG8Gi9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "e9694348-22e1-4c11-abf6-48d0a9c0b966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'0: 이순신은 조선 중기의 무신이다.</s><s> 특히 “현재는 그 동안 진행해온 여러 차례의 실험이 성공적으로 마무리되어 이제 더 많은 연구 및 연구 활동을 통해 그 효과를 극대화 시킬 수 있게 되어 매우 기쁘게 생각한다”고 덧붙였다.</s><s> ‘제'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'1: 이순신은 조선 중기의 무신이다.</s><s> 이 목록은 대한민국 대통령 취임식이 열린 2008년 7월 15일부터 2007년 3월 21일 사이에 태어난 대한민국의 대통령을 그린 그림들이다.</s><s> 이 목록은 대통령 취임식 당일인 2008년 4월 15일부터 2009'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'2: 이순신은 조선 중기의 무신이다.</s><s> 그리고 그의 아들 노암 송시열은 그의 뒤를 이어 대하국왕에 봉했고, 그 후 여러 번 왕과 함께 정사를 통일하였다.</s><s> (동국대)</s><s> 동국대('"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# set top_k = 20 and set top_p = 0.95 and num_return_sequences = 3\n",
        "sample_outputs = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=50,\n",
        "    top_k=20,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=3\n",
        ")\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  display(\"{}: {}\".format(i, tokenizer.decode(sample_output.tolist(), skip_special_tokens=True)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sX8RzLHRrc2"
      },
      "source": [
        "### **Analysis**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PYW5ejORuMj"
      },
      "source": [
        "모델을 돌리고 분석한 점을 자유롭게 적어주세요!\n",
        "\n",
        "Your Opinion: greedy search는 가장 높은 확률을 가진 단어를 다음 단어로 선택하고, beam search는 각 Time step에서 가장 확률이 높은 num_beams개의 단어들을 선택해 최종적으로 전체 확률이 가장 높은 단어들의 조합을 출력해낸다. Sampling은 단어의 확률 분포에 따라 단어를 샘플링하는 방법이며, top-k, top-p, temperature와 같은 하이퍼파라미터를 적절하게 조합해야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etg-3zdz4jQK"
      },
      "source": [
        "### **Conclusion**\n",
        "\n",
        "GPT를 포함한 생성형 언어모델을 활용하면서 중요하다 느낀 점들을 자유롭게 적어주세요!\n",
        "\n",
        "Your Opinion: 언어모델은 크기가 크다보니 다른 모델에 비해 학습 시간이 오래 걸린다. 그래서 언어모델을 본격적으로 학습시키기 전, 학습에 사용할 데이터 중 일부 데이터로만 모델을 학습시켜 코드에 이상이 없는지, 모델의 학습, 추론 과정에 이상이 없는지를 확인하는 것이 매우 중요하다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9GVxh6USEU1"
      },
      "source": [
        "### **References**\n",
        "\n",
        "*   ([Welleck et al. (2020)](https://arxiv.org/abs/2002.02492))\n",
        "*   ([Welleck et al. (2019)](https://arxiv.org/pdf/1908.04319.pdf))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}