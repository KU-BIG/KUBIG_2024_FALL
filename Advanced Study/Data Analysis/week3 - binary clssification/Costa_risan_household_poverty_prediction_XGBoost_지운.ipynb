{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51abbb01-8bb7-478b-9240-43ace0bfc1ee",
      "metadata": {
        "id": "51abbb01-8bb7-478b-9240-43ace0bfc1ee"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f4c9b06-3026-4760-9802-5d2ff4aabb28",
      "metadata": {
        "id": "9f4c9b06-3026-4760-9802-5d2ff4aabb28"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a90ca995-eb61-4532-98ce-5e8216da81fd",
      "metadata": {
        "id": "a90ca995-eb61-4532-98ce-5e8216da81fd"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c471552b-818a-4409-8b88-9ff83b46c74f",
      "metadata": {
        "id": "c471552b-818a-4409-8b88-9ff83b46c74f"
      },
      "outputs": [],
      "source": [
        "pip install -U scikit-learn==0.23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bedce204-289e-42fa-8ed1-473e46908efd",
      "metadata": {
        "scrolled": true,
        "id": "bedce204-289e-42fa-8ed1-473e46908efd"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import sklearn\n",
        "import joblib\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.externals.joblib import Parallel, delayed\n",
        "from sklearn.base import clone\n",
        "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
        "from sklearn.utils import class_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba418bd5-f1b0-4333-95e7-1ec25c3bfe37",
      "metadata": {
        "scrolled": true,
        "id": "ba418bd5-f1b0-4333-95e7-1ec25c3bfe37"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b842da6-14c1-48aa-91f7-a889cc1948e3",
      "metadata": {
        "id": "3b842da6-14c1-48aa-91f7-a889cc1948e3"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def encode_Data(df): # 라벨 인코딩으로 df의 idhogar 열을 categorical data -> numerical data로 변환\n",
        "    df['idhogar']=LabelEncoder().fit_transform(df['idhogar'])\n",
        "\n",
        "def feature_importance(forest, X_train, display_results=True): # forest 모델의 feature 중요도 계산, 정렬\n",
        "    ranked_list = [] # 피쳐 중요도 저장\n",
        "    zero_features = [] # 중요도 0인 피쳐 저장\n",
        "\n",
        "    importances = forest.feature_importances_\n",
        "    imdicies = np.argsort(importances)[::-1] # 중요도 가져와서 내림차순 정렬\n",
        "\n",
        "    if display_results:\n",
        "        print(\"Feature ranking:\")\n",
        "\n",
        "    for f in range(X_train.shape[1]): # 피쳐 수만큼 반복하면서 각 피쳐의 순위, 인덱스, 중요도, 이름 출력\n",
        "        if display_results:\n",
        "            print(\"%d. feature %d (%f)\" % (f+1, indices[f], importances[indices[f]])+\"-\"+X_train.columns[indices[f]])\n",
        "\n",
        "        ranked_list.append(X_train.columns[indices[f]]) # 중요도 높으면 ranked_list에 추가\n",
        "\n",
        "        if importances[indices[f]] == 0.0: # 중요도 0이면 zero_features에 추가\n",
        "            zero_features.append(X_trian.columns[indices[f]])\n",
        "\n",
        "    return ranked_list, zero_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3205c08d-2169-4e1e-8978-bc26e2061abd",
      "metadata": {
        "id": "3205c08d-2169-4e1e-8978-bc26e2061abd"
      },
      "outputs": [],
      "source": [
        "def do_features(df): # 새로운 feature 추가하고 집계\n",
        "\n",
        "    feats_div = [('children_fraction', 'r4t1', 'r4t3'),\n",
        "                 ('working_man_fraction', 'r4h2', 'r4t3'),\n",
        "                 ('all_man_fraction', 'r4h3', 'r4t3'),\n",
        "                 ('human_density', 'tamviv', 'rooms'),\n",
        "                 ('human_bed_density', 'tamviv', 'bedrooms'),\n",
        "                 ('rent_per_person', 'v2a1', 'r4t3'),\n",
        "                 ('rent_per_room', 'v2a1', 'rooms'),\n",
        "                 ('mobile_density', 'qmobilephone', 'r4t3'),\n",
        "                 ('tablet_density', 'v18q1', 'r4t3'),\n",
        "                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n",
        "                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n",
        "                ]\n",
        "\n",
        "    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n",
        "                 ('people_weird_stat', 'tamhog', 'r4t3')]\n",
        "    # 두 개의 칼럼으로 나누어 새로운 특성 생성\n",
        "    for f_new, f1, f2 in feats_div:\n",
        "        df['fe_'+f_new] = (df[f1]/df[f2]).astype(np.float32)\n",
        "    # 두 개의 칼럼의 차이를 계산하여 새로운 특성 생성\n",
        "    for f_new, f1, f2 in feats_sub:\n",
        "        df['fe_'+f_new] = (df[f1]-df[f2]).astype(np.float32)\n",
        "\n",
        "    # 'estadocivil', 'parentesco', 'intlevel'로 시작하는 모든 컬럼에 대한 집계\n",
        "    aggs_cat = {'dis':['mean']}\n",
        "    for s_ in ['estadocivil', 'parentesco', 'intlevel']:\n",
        "        for f_ in [f_ for f_ in df.columns if f_.startwith(s_)]:\n",
        "            aggs_cat[f_] = ['mean','count']\n",
        "\n",
        "    # age가 18이상인 데이터를 idhogar 기준으로 집계, 새로운 이름으로 df에 합침\n",
        "    for name_, df_ in [('18', df.query('age>=18'))]:\n",
        "        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n",
        "        df_agg.columns = pd.Index(['agg' + name_+'_'+e[0]+\"_\"+e[1].upper() for e in df_agg.columns.tolist()])\n",
        "        df = df.join(df_Agg, how='left', on='idhogar')\n",
        "        del df_agg\n",
        "\n",
        "    #id 컬럼은 df에서 삭제\n",
        "    df.drop(['id'], axis=1, inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "042e247b-ac53-4352-bbaf-1dac2a89dfcd",
      "metadata": {
        "id": "042e247b-ac53-4352-bbaf-1dac2a89dfcd"
      },
      "outputs": [],
      "source": [
        "def convert_OHE2LE(df): # 원핫인코딩된 dataframe을 레이블 인코딩으로 변환\n",
        "    tmp_df = df.copy(deep=True)\n",
        "    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu',\n",
        "               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco',\n",
        "               'instlevel', 'lugar', 'tipovivi',\n",
        "               'manual_elec']: # 레이블 인코딩할 컬럼 리스트\n",
        "        # 각 컬럼에 대해 원핫 인코딩된 컬럼 선택, 컬럼의 합계 계산.\n",
        "        if 'manual_' not in s_:\n",
        "            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n",
        "        elif 'elec' in s_:\n",
        "            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n",
        "        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
        "\n",
        "        # 원핫인코딩된 컬럼의 합이 0인 경우 결측치가 있는 것으로 간주, dummy columns 추가\n",
        "        if 0 in sum_ohe:\n",
        "            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n",
        "                  .format(s_))\n",
        "            # dummy colmn name to be added\n",
        "            col_dummy = s_+'_dummy'\n",
        "            # add the column to the dataframe\n",
        "            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n",
        "            # add the name to the list of columns to be label-encoded\n",
        "            cols_s_.append(col_dummy)\n",
        "            # proof-check, that now the category is complete\n",
        "            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n",
        "            if 0 in sum_ohe:\n",
        "                 print(\"The category completion did not work\")\n",
        "\n",
        "        # idxmax를 사용하여 가장 큰 값 가지는 인덱스 찾고 레이블 인코딩\n",
        "        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n",
        "        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n",
        "        # 원핫인코딩된 컬럼은 삭제\n",
        "        if 'parentesco1' in cols_s_:\n",
        "            cols_s_.remove('parentesco1')\n",
        "        tmp_df.drop(cols_s_, axis=1, inplace=True)\n",
        "    return tmp_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d10ef212-7005-4d6e-828a-66bd7d3cec80",
      "metadata": {
        "id": "d10ef212-7005-4d6e-828a-66bd7d3cec80"
      },
      "source": [
        "Read in the data and clean it up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8a38044-bacf-4487-9ad2-bc3158f212c9",
      "metadata": {
        "id": "e8a38044-bacf-4487-9ad2-bc3158f212c9"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('../input/train.csv')\n",
        "test = pd.read_csv('../input/test.csv')\n",
        "\n",
        "test_ids = test.Id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc8e13d6-4574-44c5-a5fd-a2dd68c0d2e1",
      "metadata": {
        "id": "fc8e13d6-4574-44c5-a5fd-a2dd68c0d2e1"
      },
      "outputs": [],
      "source": [
        "def process_df(df_): # 데이터프레임 처리하는 변수\n",
        "    # encode the idhogar\n",
        "    encode_data(df_)\n",
        "\n",
        "    # create aggregate features\n",
        "    return do_features(df_)\n",
        "\n",
        "train = process_df(train)\n",
        "test = process_df(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16c63157-993d-4fce-8cf7-aaf83fa8dc1d",
      "metadata": {
        "id": "16c63157-993d-4fce-8cf7-aaf83fa8dc1d"
      },
      "outputs": [],
      "source": [
        "# 의존성 처리 -> 제곱근 계산하여 dependency 칼럼 생성\n",
        "train['dependency'] = np.sqrt(train['SQBdependency'])\n",
        "test['dependency'] = np.sqrt(test['SQBdependency'])\n",
        "\n",
        "# no 값을 0으로 변환\n",
        "train.loc[train['edjefa'] == \"no\", \"edjefa\"] = 0\n",
        "train.loc[train['edjefe'] == \"no\", \"edjefe\"] = 0\n",
        "test.loc[test['edjefa'] == \"no\", \"edjefa\"] = 0\n",
        "test.loc[test['edjefe'] == \"no\", \"edjefe\"] = 0\n",
        "\n",
        "# 교육이 yes, 가구주인 경우 escolarj로 채우기\n",
        "train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"edjefa\"] = train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
        "train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"edjefe\"] = train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
        "\n",
        "test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"edjefa\"] = test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
        "test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"edjefe\"] = test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
        "\n",
        "# 교육상태가 yes인 경우 4로 변환\n",
        "train.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
        "train.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
        "\n",
        "test.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
        "test.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
        "\n",
        "# 데이터 타입을 int로 변환\n",
        "train['edjefe'] = train['edjefe'].astype(\"int\")\n",
        "train['edjefa'] = train['edjefa'].astype(\"int\")\n",
        "test['edjefe'] = test['edjefe'].astype(\"int\")\n",
        "test['edjefa'] = test['edjefa'].astype(\"int\")\n",
        "\n",
        "# 가구주 중 최대 교육 수준 계산\n",
        "train['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\n",
        "test['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)\n",
        "\n",
        "# 결측치 0으로 대체\n",
        "train['v2a1']=train['v2a1'].fillna(0)\n",
        "test['v2a1']=test['v2a1'].fillna(0)\n",
        "\n",
        "test['v18q1']=test['v18q1'].fillna(0)\n",
        "train['v18q1']=train['v18q1'].fillna(0)\n",
        "\n",
        "train['rez_esc']=train['rez_esc'].fillna(0)\n",
        "test['rez_esc']=test['rez_esc'].fillna(0)\n",
        "\n",
        "train.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\n",
        "train.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
        "무 의존성 처리 -> 제곱근 계산하여 dependency 칼럼 생성\n",
        "train['dependency'] = np.sqrt(train['SQBdependency'])\n",
        "test['dependency'] = np.sqrt(test['SQBdependency'])\n",
        "\n",
        "# no 값을 0으로 변환\n",
        "train.loc[train['edjefa'] == \"no\", \"edjefa\"] = 0\n",
        "train.loc[train['edjefe'] == \"no\", \"edjefe\"] = 0\n",
        "test.loc[test['edjefa'] == \"no\", \"edjefa\"] = 0\n",
        "test.loc[test['edjefe'] == \"no\", \"edjefe\"] = 0\n",
        "\n",
        "# 교육이 yes, 가구주인 경우 escolarj로 채우기\n",
        "train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"edjefa\"] = train.loc[(train['edjefa'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
        "train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"edjefe\"] = train.loc[(train['edjefe'] == \"yes\") & (train['parentesco1'] == 1), \"escolari\"]\n",
        "\n",
        "test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"edjefa\"] = test.loc[(test['edjefa'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
        "test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"edjefe\"] = test.loc[(test['edjefe'] == \"yes\") & (test['parentesco1'] == 1), \"escolari\"]\n",
        "\n",
        "# 교육상태가 yes인 경우 4로 변환\n",
        "train.loc[train['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
        "train.loc[train['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
        "\n",
        "test.loc[test['edjefa'] == \"yes\", \"edjefa\"] = 4\n",
        "test.loc[test['edjefe'] == \"yes\", \"edjefe\"] = 4\n",
        "\n",
        "# 데이터 타입을 int로 변환\n",
        "train['edjefe'] = train['edjefe'].astype(\"int\")\n",
        "train['edjefa'] = train['edjefa'].astype(\"int\")\n",
        "test['edjefe'] = test['edjefe'].astype(\"int\")\n",
        "test['edjefa'] = test['edjefa'].astype(\"int\")\n",
        "\n",
        "# 가구주 중 최대 교육 수준 계산\n",
        "train['edjef'] = np.max(train[['edjefa','edjefe']], axis=1)\n",
        "test['edjef'] = np.max(test[['edjefa','edjefe']], axis=1)\n",
        "\n",
        "# 결측치 0으로 대체\n",
        "train['v2a1']=train['v2a1'].fillna(0)\n",
        "test['v2a1']=test['v2a1'].fillna(0)\n",
        "\n",
        "test['v18q1']=test['v18q1'].fillna(0)\n",
        "train['v18q1']=train['v18q1'].fillna(0)\n",
        "\n",
        "train['rez_esc']=train['rez_esc'].fillna(0)\n",
        "test['rez_esc']=test['rez_esc'].fillna(0)\n",
        "\n",
        "train.loc[train.meaneduc.isnull(), \"meaneduc\"] = 0\n",
        "train.loc[train.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
        "무\n",
        "test.loc[test.meaneduc.isnull(), \"meaneduc\"] = 0\n",
        "test.loc[test.SQBmeaned.isnull(), \"SQBmeaned\"] = 0\n",
        "\n",
        "# 화장실 및 급수 관련 데이터의 불일치 처리\n",
        "# if there is no water we'll assume they do not\n",
        "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"v14a\"] = 0\n",
        "train.loc[(train.v14a ==  1) & (train.sanitario1 ==  1) & (train.abastaguano == 0), \"sanitario1\"] = 0\n",
        "\n",
        "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"v14a\"] = 0\n",
        "test.loc[(test.v14a ==  1) & (test.sanitario1 ==  1) & (test.abastaguano == 0), \"sanitario1\"] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "888ed39d-dfb1-4f67-8717-98e870c7f42b",
      "metadata": {
        "id": "888ed39d-dfb1-4f67-8717-98e870c7f42b"
      },
      "outputs": [],
      "source": [
        "def train_test_apply_func(train_, test_, func_): # 주어진 train_, test_ 데이터프레임에 대해 특정 함수 적용\n",
        "    test_['Target'] = 0\n",
        "    xx = pd.concat([train_, test_]) # 두 데이터셋을 수직으로 결합\n",
        "\n",
        "    xx_func = func_(xx) # 함수에 적용하여 변환\n",
        "    train_ = xx_func.iloc[:train_.shape[0], :] # 원래 train_ 데이터프레임 크기만큼의 행을 가져와 train_으로 설정, 나머지를 test로 설정\n",
        "    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1) # target컬럼은 삭제\n",
        "\n",
        "    del xx, xx_func\n",
        "    return train_, test_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03291bf5-157f-4485-a267-45ea13383d53",
      "metadata": {
        "id": "03291bf5-157f-4485-a267-45ea13383d53"
      },
      "outputs": [],
      "source": [
        "# convert the one hot fields into label encoded\n",
        "train, test = train_test_apply_func(train, test, convert_OHE2LE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34186317-1ea5-4d83-a4fb-e3c63217be0f",
      "metadata": {
        "id": "34186317-1ea5-4d83-a4fb-e3c63217be0f"
      },
      "source": [
        "Geo aggregates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "465a9744-6e85-4f25-800e-00f77e417754",
      "metadata": {
        "id": "465a9744-6e85-4f25-800e-00f77e417754"
      },
      "outputs": [],
      "source": [
        "cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE',\n",
        "              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n",
        "              'pared_LE'] # 원핫인코딩을 적용할 범주형 변수 리스트\n",
        "cols_nums = ['age', 'meaneduc', 'dependency',\n",
        "             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n",
        "             'bedrooms', 'overcrowding'] # 숫자형\n",
        "\n",
        "def convert_geo2aggs(df_): # 지리적 특성에 따른 집계 값 계산\n",
        "    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)], # df에서 이런 변수들 선택, 원핫인코딩\n",
        "                        pd.get_dummies(df_[cols_2_ohe],\n",
        "                                       columns=cols_2_ohe)],axis=1)\n",
        "    # lugar_LE, idhogar로 그룹화하여 평균값 계산, 다시 그룹화하여 각 지역의 평균 집계 값 계산. float32 형태로 변환\n",
        "    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n",
        "    geo_agg.columns = pd.Index(['geo_' + e for e in geo_agg.columns.tolist()]) # 컬럼 이름에 geo_ 추가\n",
        "\n",
        "    del tmp_df\n",
        "    return df_.join(geo_agg, how='left', on='lugar_LE') # 원본 df에 왼쪽에 geo_agg 조인해줌\n",
        "\n",
        "# add some aggregates by geography\n",
        "train, test = train_test_apply_func(train, test, convert_geo2aggs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e85653d0-ab15-47b6-bf79-cee12e325119",
      "metadata": {
        "id": "e85653d0-ab15-47b6-bf79-cee12e325119"
      },
      "outputs": [],
      "source": [
        "# train, test 데이터프레임에 추가적인 피쳐 생성, 각 가구의 18세 이상 인원 수 계산\n",
        "train['num_over_18'] = 0 # 초기화\n",
        "train['num_over_18'] = train[train.age >= 18].groupby('idhogar').transform(\"count\") # 18세 이상 사람 수 카운트, num-over_18에 할당\n",
        "train['num_over_18'] = train.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\") # 각가구의 최대값 계산하여 저장\n",
        "train['num_over_18'] = train['num_over_18'].fillna(0) # 결측값은 0으로\n",
        "\n",
        "test['num_over_18'] = 0\n",
        "test['num_over_18'] = test[test.age >= 18].groupby('idhogar').transform(\"count\")\n",
        "test['num_over_18'] = test.groupby(\"idhogar\")[\"num_over_18\"].transform(\"max\")\n",
        "test['num_over_18'] = test['num_over_18'].fillna(0)\n",
        "\n",
        "# 추가로 특성을 생성해냄.\n",
        "def extract_features(df):\n",
        "    df['bedrooms_to_rooms'] = df['bedrooms']/df['rooms'] # 침실 수를 방 수로 나눔\n",
        "    df['rent_to_rooms'] = df['v2a1']/df['rooms'] # 임대료를 방 수로 나눔\n",
        "    df['tamhog_to_rooms'] = df['tamhog']/df['rooms'] # 가구 크기를 방 수로 나눔\n",
        "    df['r4t3_to_tamhog'] = df['r4t3']/df['tamhog'] # r4t3 - 총 인ㅇㅝㄴ을 가구 크기로 나눔\n",
        "    df['r4t3_to_rooms'] = df['r4t3']/df['rooms'] # r4t3 - 총 인원을 방 수로 나눔\n",
        "    df['v2a1_to_r4t3'] = df['v2a1']/df['r4t3'] # 임대료를 총 인원으로 나눔\n",
        "    df['v2a1_to_r4t3'] = df['v2a1']/(df['r4t3'] - df['r4t1']) # 임대료를 12세 미만으로 나눔\n",
        "    df['hhsize_to_rooms'] = df['hhsize']/df['rooms'] # 가구크기를 방 수로 나눔\n",
        "    df['rent_to_hhsize'] = df['v2a1']/df['hhsize'] # 임대료를 가구 크기로 나눔\n",
        "    df['rent_to_over_18'] = df['v2a1']/df['num_over_18'] # 임대료를 18세 이상 인원으로 나눔, 없는 경우에는 전체 임대료로 대체\n",
        "    #\n",
        "    df.loc[df.num_over_18 == 0, \"rent_to_over_18\"] = df[df.num_over_18 == 0].v2a1\n",
        "\n",
        "# 데이터프레임에 위에서 추출한 feature 포함시킨다.\n",
        "extract_features(train)\n",
        "extract_features(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbba11c5-e566-4f78-b483-88df6db0c7b1",
      "metadata": {
        "id": "bbba11c5-e566-4f78-b483-88df6db0c7b1"
      },
      "outputs": [],
      "source": [
        "# drop duplicated columns\n",
        "needless_cols = ['r4t3', 'tamhog', 'tamviv', 'hhsize', 'v18q', 'v14a', 'agesq',\n",
        "                 'mobilephone', 'female', ] # 제거할 컬럼들 (중복되거나 필요하지 않음 )\n",
        "\n",
        "instlevel_cols = [s for s in train.columns.tolist() if 'instlevel' in s] # 컬럼 중에서 instlevel이 포함된 컬럼 저장 -> 교육 수준과 관련\n",
        "\n",
        "needless_cols.extend(instlevel_cols) # 제거할 컬럼 리스트에 교육 관련 컬럼들 추가\n",
        "\n",
        "train = train.drop(needless_cols, axis=1) # needless 칼럼 제거\n",
        "test = test.drop(needless_cols, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd382adf-2320-4d45-b4b0-90241af05238",
      "metadata": {
        "id": "bd382adf-2320-4d45-b4b0-90241af05238"
      },
      "source": [
        "Split the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff9afc69-2cb8-438e-a8e1-05206355bd04",
      "metadata": {
        "id": "ff9afc69-2cb8-438e-a8e1-05206355bd04"
      },
      "outputs": [],
      "source": [
        "# train, test 0.8:0.2로 분할\n",
        "def split_data(train, y, sample_weight=None, households=None, test_percentage=0.20, seed=None):\n",
        "    # uncomment for extra randomness\n",
        "#     np.random.seed(seed=seed)\n",
        "\n",
        "    train2 = train.copy()\n",
        "\n",
        "    # pick some random households to use for the test data\n",
        "    cv_hhs = np.random.choice(households, size=int(len(households) * test_percentage), replace=False)\n",
        "\n",
        "    # select households which are in the random selection\n",
        "    cv_idx = np.isin(households, cv_hhs)\n",
        "    X_test = train2[cv_idx] # 테스트 데이터, 훈련 데이터 분리\n",
        "    y_test = y[cv_idx]\n",
        "\n",
        "    X_train = train2[~cv_idx]\n",
        "    y_train = y[~cv_idx]\n",
        "\n",
        "    if sample_weight is not None:\n",
        "        y_train_weights = sample_weight[~cv_idx]\n",
        "        return X_train, y_train, X_test, y_test, y_train_weights\n",
        "\n",
        "    return X_train, y_train, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a06bd346-fcdb-4458-8f84-5202717053b9",
      "metadata": {
        "id": "a06bd346-fcdb-4458-8f84-5202717053b9"
      },
      "outputs": [],
      "source": [
        "# parentesco가 1인 데이터만 선택하여 X에 저장\n",
        "X = train.query('parentesco1==1')\n",
        "# X = train.copy()\n",
        "\n",
        "# Target 컬럼에서 y만 빼서 저장, X에서 제거 -> 입력할 데이터 / 레이블 구분\n",
        "y = X['Target'] - 1\n",
        "X = X.drop(['Target'], axis=1)\n",
        "\n",
        "np.random.seed(seed=None)\n",
        "\n",
        "train2 = X.copy() # X는 바뀌지 않도록 train2로 데이터 복사\n",
        "\n",
        "train_hhs = train2.idhogar # train에서 가구 ID 추출하여 households 리스트에 고유한 가구ID 저장\n",
        "\n",
        "households = train2.idhogar.unique()\n",
        "cv_hhs = np.random.choice(households, size=int(len(households) * 0.15), replace=False) # 15% 무작위 선택하여 저장, 테스트 데이터\n",
        "\n",
        "cv_idx = np.isin(train2.idhogar, cv_hhs) # 선택한 가구 ID가 train2에 포함되어 있는지 확인\n",
        "\n",
        "X_test = train2[cv_idx]\n",
        "y_test = y[cv_idx]\n",
        "\n",
        "X_train = train2[~cv_idx]\n",
        "y_train = y[~cv_idx]\n",
        "\n",
        "# train on entire dataset\n",
        "X_train = train2\n",
        "y_train = y\n",
        "\n",
        "train_households = X_train.idhogar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7a28574-6305-4794-a2b5-ad5707419e37",
      "metadata": {
        "id": "d7a28574-6305-4794-a2b5-ad5707419e37"
      },
      "outputs": [],
      "source": [
        "# 클래스 불균형 문제 해결을 위해 샘플 가중치 계산\n",
        "# 'balanced' : 클래스 샘플수와 반비례하는 가중치 부여\n",
        "#y_train : 타겟 변ㅅㅜ, 클래스 레이블 포함\n",
        "y_train_weights = class_weight.compute_sample_weight('balanced', y_train, indices=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53132206-4302-4347-99b2-3a837a13ccbf",
      "metadata": {
        "id": "53132206-4302-4347-99b2-3a837a13ccbf"
      },
      "outputs": [],
      "source": [
        "# 모델 성능 최적화, 과적합 방지를 위해 특성 제거\n",
        "extra_drop_features = [\n",
        " 'agg18_estadocivil1_MEAN',\n",
        " 'agg18_estadocivil6_COUNT',\n",
        " 'agg18_estadocivil7_COUNT',\n",
        " 'agg18_parentesco10_COUNT',\n",
        " 'agg18_parentesco11_COUNT',\n",
        " 'agg18_parentesco12_COUNT',\n",
        " 'agg18_parentesco1_COUNT',\n",
        " 'agg18_parentesco2_COUNT',\n",
        " 'agg18_parentesco3_COUNT',\n",
        " 'agg18_parentesco4_COUNT',\n",
        " 'agg18_parentesco5_COUNT',\n",
        " 'agg18_parentesco6_COUNT',\n",
        " 'agg18_parentesco7_COUNT',\n",
        " 'agg18_parentesco8_COUNT',\n",
        " 'agg18_parentesco9_COUNT',\n",
        " 'geo_elimbasu_LE_4',\n",
        " 'geo_energcocinar_LE_1',\n",
        " 'geo_energcocinar_LE_2',\n",
        " 'geo_epared_LE_0',\n",
        " 'geo_hogar_mayor',\n",
        " 'geo_manual_elec_LE_2',\n",
        " 'geo_pared_LE_3',\n",
        " 'geo_pared_LE_4',\n",
        " 'geo_pared_LE_5',\n",
        " 'geo_pared_LE_6',\n",
        " 'num_over_18',\n",
        " 'parentesco_LE',\n",
        " 'rez_esc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9dff10f-79c5-4008-86a7-d02a99590a80",
      "metadata": {
        "id": "e9dff10f-79c5-4008-86a7-d02a99590a80"
      },
      "outputs": [],
      "source": [
        "xgb_drop_cols = extra_drop_features + [\"idhogar\",  'parentesco1'] # 제거할 컬럼"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19dec5aa-b612-48c4-b440-ba4b171ab3a7",
      "metadata": {
        "id": "19dec5aa-b612-48c4-b440-ba4b171ab3a7"
      },
      "source": [
        "Fit a voting classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3008518c-ed98-4554-a746-9b2b9ae9a5dd",
      "metadata": {
        "id": "3008518c-ed98-4554-a746-9b2b9ae9a5dd"
      },
      "outputs": [],
      "source": [
        "# 여러셋의 하이퍼파라미터 정의 XGBoost 모델의 성능 최적화, 모델의 복잡성, 학습률, 정규화 등 조절\n",
        "#4\n",
        "opt_parameters = {'max_depth':35, 'eta':0.1, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 1, 'num_class': 4, 'gamma': 2.0, 'colsample_bylevel': 0.9, 'subsample': 0.84, 'colsample_bytree': 0.88, 'reg_lambda': 0.40 }\n",
        "#5\n",
        "opt_parameters = {'max_depth':35, 'eta':0.15, 'silent':1, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.5, 'colsample_bylevel': 1, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n",
        "#6\n",
        "# opt_parameters = {'max_depth':35, 'eta':0.15, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 2.75, 'colsample_bylevel': 0.95, 'subsample': 0.95, 'colsample_bytree': 0.85, 'reg_lambda': 0.35 }\n",
        "# 7\n",
        "# opt_parameters = {'max_depth':35, 'eta':0.12, 'silent':0, 'objective':'multi:softmax', 'min_child_weight': 2, 'num_class': 4, 'gamma': 3.25, 'colsample_bylevel': 0.95, 'subsample': 0.88, 'colsample_bytree': 0.88, 'reg_lambda': 0.35 }\n",
        "\n",
        "def evaluate_macroF1_lgb(predictions, truth):  # LightGBM 모델의 예측 결과 평가 예측-실제 레이블 비교\n",
        "    # this follows the discussion in https://github.com/Microsoft/LightGBM/issues/1483\n",
        "    pred_labels = predictions.argmax(axis=1)\n",
        "    truth = truth.get_label()\n",
        "    f1 = f1_score(truth, pred_labels, average='macro')\n",
        "    return ('macroF1', 1-f1)\n",
        "\n",
        "# 룬련 파라미터 설정\n",
        "fit_params={\"early_stopping_rounds\":500,\n",
        "            \"eval_metric\" : evaluate_macroF1_lgb,\n",
        "            \"eval_set\" : [(X_train,y_train), (X_test,y_test)],\n",
        "            'verbose': False,\n",
        "           }\n",
        "\n",
        "# 학습률을 반복에 따라 조정, 50회마다 로그 출력\n",
        "def learning_rate_power_0997(current_iter):\n",
        "    base_learning_rate = 0.1\n",
        "    min_learning_rate = 0.02\n",
        "    lr = base_learning_rate  * np.power(.995, current_iter)\n",
        "    return max(lr, min_learning_rate)\n",
        "\n",
        "fit_params['verbose'] = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a696b5c-c3c6-48e7-a819-e91cf76d6ab9",
      "metadata": {
        "id": "3a696b5c-c3c6-48e7-a819-e91cf76d6ab9"
      },
      "outputs": [],
      "source": [
        "np.random.seed(100)\n",
        "\n",
        "# estimator을 데이터에 맞추기 위해 필요한 절차 정의 (split, 모델 학습,  )\n",
        "def _parallel_fit_estimator(estimator1, X, y, sample_weight=None, threshold=True, **fit_params):\n",
        "    estimator = clone(estimator1)\n",
        "\n",
        "    # randomly split the data so we have a test set for early stopping\n",
        "    if sample_weight is not None:\n",
        "        X_train, y_train, X_test, y_test, y_train_weight = split_data(X, y, sample_weight, households=train_households)\n",
        "    else:\n",
        "        X_train, y_train, X_test, y_test = split_data(X, y, None, households=train_households)\n",
        "\n",
        "    # update the fit params with our new split\n",
        "    fit_params[\"eval_set\"] = [(X_test,y_test)]\n",
        "\n",
        "    # fit the estimator\n",
        "    if sample_weight is not None:\n",
        "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
        "            estimator.fit(X_train, y_train)\n",
        "        else:\n",
        "            _ = estimator.fit(X_train, y_train, sample_weight=y_train_weight, **fit_params)\n",
        "    else:\n",
        "        if isinstance(estimator1, ExtraTreesClassifier) or isinstance(estimator1, RandomForestClassifier):\n",
        "            estimator.fit(X_train, y_train)\n",
        "        else:\n",
        "            _ = estimator.fit(X_train, y_train, **fit_params)\n",
        "\n",
        "    if not isinstance(estimator1, ExtraTreesClassifier) and not isinstance(estimator1, RandomForestClassifier) and not isinstance(estimator1, xgb.XGBClassifier):\n",
        "        best_cv_round = np.argmax(estimator.evals_result_['validation_0']['mlogloss'])\n",
        "        best_cv = np.max(estimator.evals_result_['validation_0']['mlogloss'])\n",
        "        best_train = estimator.evals_result_['train']['macroF1'][best_cv_round]\n",
        "    else:\n",
        "        best_train = f1_score(y_train, estimator.predict(X_train), average=\"macro\")\n",
        "        best_cv = f1_score(y_test, estimator.predict(X_test), average=\"macro\")\n",
        "        print(\"Train F1:\", best_train)\n",
        "        print(\"Test F1:\", best_cv)\n",
        "\n",
        "    # reject some estimators based on their performance on train and test sets\n",
        "    if threshold:\n",
        "        # if the valid score is very high we'll allow a little more leeway with the train scores\n",
        "        if ((best_cv > 0.37) and (best_train > 0.75)) or ((best_cv > 0.44) and (best_train > 0.65)):\n",
        "            return estimator\n",
        "\n",
        "        # else recurse until we get a better one\n",
        "        else:\n",
        "            print(\"Unacceptable!!! Trying again...\")\n",
        "            return _parallel_fit_estimator(estimator1, X, y, sample_weight=sample_weight, **fit_params)\n",
        "\n",
        "    else:\n",
        "        return estimator\n",
        "\n",
        "class VotingClassifierLGBM(VotingClassifier):\n",
        "    '''\n",
        "    This implements the fit method of the VotingClassifier propagating fit_params\n",
        "    '''\n",
        "    # 레이블 y가 다차원 배열일 경우 예외. voting 매개변수가 soft인지 hard인지, estimators가 비어있는지 확인\n",
        "    def fit(self, X, y, sample_weight=None, threshold=True, **fit_params):\n",
        "\n",
        "        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n",
        "            raise NotImplementedError('Multilabel and multi-output'\n",
        "                                      ' classification is not supported.')\n",
        "\n",
        "        if self.voting not in ('soft', 'hard'):\n",
        "            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n",
        "                             % self.voting)\n",
        "\n",
        "        if self.estimators is None or len(self.estimators) == 0:\n",
        "            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n",
        "                                 ' should be a list of (string, estimator)'\n",
        "                                 ' tuples')\n",
        "\n",
        "        if (self.weights is not None and\n",
        "                len(self.weights) != len(self.estimators)):\n",
        "            raise ValueError('Number of classifiers and weights must be equal'\n",
        "                             '; got %d weights, %d estimators'\n",
        "                             % (len(self.weights), len(self.estimators)))\n",
        "\n",
        "        names, clfs = zip(*self.estimators)\n",
        "        self._validate_names(names)\n",
        "\n",
        "        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n",
        "        if n_isnone == len(self.estimators):\n",
        "            raise ValueError('All estimators are None. At least one is '\n",
        "                             'required to be a classifier!')\n",
        "\n",
        "        self.le_ = LabelEncoder().fit(y)\n",
        "        self.classes_ = self.le_.classes_\n",
        "        self.estimators_ = []\n",
        "\n",
        "        transformed_y = self.le_.transform(y)\n",
        "\n",
        "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
        "                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n",
        "                                                 sample_weight=sample_weight, threshold=threshold, **fit_params)\n",
        "                for clf in clfs if clf is not None)\n",
        "\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e470bc2-a745-4214-bf1e-1fc83d7b0b38",
      "metadata": {
        "id": "6e470bc2-a745-4214-bf1e-1fc83d7b0b38"
      },
      "outputs": [],
      "source": [
        "clfs = []\n",
        "for i in range(15): # 15개의 XGBOost 분류기를 만들어 리스트 clfs에 추가. 랜덤시드는 모두 다른 조건, 트리는 300개, 학습률 0.15\n",
        "    clf = xgb.XGBClassifier(random_state=217+i, n_estimators=300, learning_rate=0.15, n_jobs=4, **opt_parameters)\n",
        "\n",
        "    clfs.append(('xgb{}'.format(i), clf))\n",
        "\n",
        "# 생성한 XGBoost 분류기들은 소프트 보팅 방식으로 결합, 각 모델의 예측 확률을 평균내어 최종 예측 결정.\n",
        "vc = VotingClassifierLGBM(clfs, voting='soft')\n",
        "del(clfs)\n",
        "\n",
        "#최종학습\n",
        "_ = vc.fit(X_train.drop(xgb_drop_cols, axis=1), y_train, sample_weight=y_train_weights, threshold=False, **fit_params)\n",
        "\n",
        "clf_final = vc.estimators_[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b16bddc5-7cf4-4184-8f48-1538c9f800a7",
      "metadata": {
        "id": "b16bddc5-7cf4-4184-8f48-1538c9f800a7"
      },
      "outputs": [],
      "source": [
        "# clf_final을 사용하여 테스트 데이터에 대한 F1 점수 계산\n",
        "global_score = f1_score(y_test, clf_final.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
        "# 소프트 보팅 방식으로 설정하고, F1 점수 계산\n",
        "vc.voting = 'soft'\n",
        "global_score_soft = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
        "# 하드 보팅 점수계산\n",
        "vc.voting = 'hard'\n",
        "global_score_hard = f1_score(y_test, vc.predict(X_test.drop(xgb_drop_cols, axis=1)), average='macro')\n",
        "\n",
        "print('Validation score of a single LGBM Classifier: {:.4f}'.format(global_score))\n",
        "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_score_soft))\n",
        "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_score_hard))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "943e8bea-6c6d-488d-8cb9-3a60e4803d8d",
      "metadata": {
        "id": "943e8bea-6c6d-488d-8cb9-3a60e4803d8d"
      },
      "outputs": [],
      "source": [
        "# see which features are not used by ANY models\n",
        "useless_features = [] # 특성 저장할 list\n",
        "drop_features = set() # 모든 모델에서 사용되지 않는 피쳐 저장할 set\n",
        "counter = 0 # 반복 횟수\n",
        "for est in vc.estimators_:\n",
        "    # 각 모데에서 사용된 특성과 사용되지 않는 특성 평가\n",
        "    ranked_features, unused_features = feature_importance(est, X_train.drop(xgb_drop_cols, axis=1), display_results=False)\n",
        "    useless_features.append(unused_features)\n",
        "    # 모델에서 사용되지 않는 특성과의 교집합을 구하여 모든 모델에서 사용되지 않는 특성만 남김\n",
        "    if counter == 0:\n",
        "        drop_features = set(unused_features)\n",
        "    else:\n",
        "        drop_features = drop_features.intersection(set(unused_features))\n",
        "    counter += 1\n",
        "\n",
        "drop_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e77621c-6d76-45de-b795-4976ed09eedc",
      "metadata": {
        "id": "4e77621c-6d76-45de-b795-4976ed09eedc"
      },
      "outputs": [],
      "source": [
        "# 특성 중요도 평가하는 과정. 중요도를 높은 순으로 정렬하여 반환\n",
        "ranked_features = feature_importance(clf_final, X_train.drop(xgb_drop_cols, axis=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed9e8fea-2cfd-47a2-b759-c0e2395fd178",
      "metadata": {
        "id": "ed9e8fea-2cfd-47a2-b759-c0e2395fd178"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "411baefb-6051-4bce-a7e8-ab08cd656b09",
      "metadata": {
        "id": "411baefb-6051-4bce-a7e8-ab08cd656b09"
      },
      "outputs": [],
      "source": [
        "# 리스트에 제거할 특성의 이름 초기화. 모델 학습에 필요 없거나 중복된 정보\n",
        "et_drop_cols = ['agg18_age_MAX', 'agg18_age_MEAN', 'agg18_age_MIN', 'agg18_dis_MEAN',\n",
        "       'agg18_escolari_MAX', 'agg18_escolari_MEAN', 'agg18_escolari_MIN',\n",
        "       'agg18_estadocivil1_COUNT', 'agg18_estadocivil1_MEAN',\n",
        "       'agg18_estadocivil2_COUNT', 'agg18_estadocivil2_MEAN',\n",
        "       'agg18_estadocivil3_COUNT', 'agg18_estadocivil3_MEAN',\n",
        "       'agg18_estadocivil4_COUNT', 'agg18_estadocivil4_MEAN',\n",
        "       'agg18_estadocivil5_COUNT', 'agg18_estadocivil5_MEAN',\n",
        "       'agg18_estadocivil6_COUNT', 'agg18_estadocivil6_MEAN',\n",
        "       'agg18_estadocivil7_COUNT', 'agg18_estadocivil7_MEAN',\n",
        "       'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN',\n",
        "       'agg18_parentesco11_COUNT', 'agg18_parentesco11_MEAN',\n",
        "       'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN',\n",
        "       'agg18_parentesco1_COUNT', 'agg18_parentesco1_MEAN',\n",
        "       'agg18_parentesco2_COUNT', 'agg18_parentesco2_MEAN',\n",
        "       'agg18_parentesco3_COUNT', 'agg18_parentesco3_MEAN',\n",
        "       'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN',\n",
        "       'agg18_parentesco5_COUNT', 'agg18_parentesco5_MEAN',\n",
        "       'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN',\n",
        "       'agg18_parentesco7_COUNT', 'agg18_parentesco7_MEAN',\n",
        "       'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN',\n",
        "       'agg18_parentesco9_COUNT', 'agg18_parentesco9_MEAN'] #+ ['parentesco_LE', 'rez_esc']\n",
        "\n",
        "et_drop_cols.extend([\"idhogar\", \"parentesco1\", 'fe_rent_per_person', 'fe_rent_per_room',\n",
        "       'fe_tablet_adult_density', 'fe_tablet_density'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "809f9c5b-a262-43be-8277-256fa0204d50",
      "metadata": {
        "id": "809f9c5b-a262-43be-8277-256fa0204d50"
      },
      "outputs": [],
      "source": [
        "# 10개의 랜덤 포레스트 분류기를 생성하여 리스트 ets에 추가\n",
        "ets = []\n",
        "for i in range(10):\n",
        "    rf = RandomForestClassifier(max_depth=None, random_state=217+i, n_jobs=4, n_estimators=700, min_impurity_decrease=1e-3, min_samples_leaf=2, verbose=0, class_weight=\"balanced\")\n",
        "    ets.append(('rf{}'.format(i), rf))\n",
        "\n",
        "# 생성한 랜덤 포레스트 분류기들을 소프트 보팅 방식으로 결합하는 객체 생성.\n",
        "vc2 = VotingClassifierLGBM(ets, voting='soft')\n",
        "_ = vc2.fit(X_train.drop(et_drop_cols, axis=1), y_train, threshold=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "743c0230-454d-4246-abc7-3d59a48381bd",
      "metadata": {
        "id": "743c0230-454d-4246-abc7-3d59a48381bd"
      },
      "outputs": [],
      "source": [
        "# soft/hard 방식으로 예측한 결과에 대해 F1 점수 출력\n",
        "vc2.voting = 'soft'\n",
        "global_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
        "vc2.voting = 'hard'\n",
        "global_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
        "\n",
        "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\n",
        "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_rf_score_hard))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85b97fb4-678f-4abe-9433-fc9512d37948",
      "metadata": {
        "id": "85b97fb4-678f-4abe-9433-fc9512d37948"
      },
      "outputs": [],
      "source": [
        "# w/o threshold, extra drop cols\n",
        "vc2.voting = 'soft'\n",
        "global_rf_score_soft = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
        "vc2.voting = 'hard'\n",
        "global_rf_score_hard = f1_score(y_test, vc2.predict(X_test.drop(et_drop_cols, axis=1)), average='macro')\n",
        "\n",
        "print('Validation score of a VotingClassifier on 3 LGBMs with soft voting strategy: {:.4f}'.format(global_rf_score_soft))\n",
        "print('Validation score of a VotingClassifier on 3 LGBMs with hard voting strategy: {:.4f}'.format(global_rf_score_hard))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17f24cef-d443-47e4-a12b-7984201346e9",
      "metadata": {
        "id": "17f24cef-d443-47e4-a12b-7984201346e9"
      },
      "outputs": [],
      "source": [
        "# 사용되지 않는 특성 평가, 제거\n",
        "useless_features = [\n",
        "drop_features = set()\n",
        "counter = 0\n",
        "for est in vc2.estimators_:\n",
        "    ranked_features, unused_features = feature_importance(est, X_train.drop(et_drop_cols, axis=1), display_results=False)\n",
        "    useless_features.append(unused_features)\n",
        "    if counter == 0:\n",
        "        drop_features = set(unused_features)\n",
        "    else:\n",
        "        drop_features = drop_features.intersection(set(unused_features))\n",
        "    counter += 1\n",
        "\n",
        "drop_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e5ee7f3-6926-4a99-8a8d-65fff91c4cd8",
      "metadata": {
        "id": "3e5ee7f3-6926-4a99-8a8d-65fff91c4cd8"
      },
      "outputs": [],
      "source": [
        "# vc, vc2의 예측 결과를 결합하여 최종 예측을 생성하는 함수를 정의, F1 점수를 계산\n",
        "def combine_voters(data, weights=[0.5, 0.5]):\n",
        "    # do soft voting with both classifiers\n",
        "    vc.voting=\"soft\"\n",
        "    vc1_probs = vc.predict_proba(data.drop(xgb_drop_cols, axis=1))\n",
        "    vc2.voting=\"soft\"\n",
        "    vc2_probs = vc2.predict_proba(data.drop(et_drop_cols, axis=1))\n",
        "\n",
        "    # 각 모델의 예측확률을 가중치 따라 결합, 곱한 후 합\n",
        "    final_vote = (vc1_probs * weights[0]) + (vc2_probs * weights[1])\n",
        "    predictions = np.argmax(final_vote, axis=1)\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb07d037-e09a-43ab-9a24-f80004fb8bf7",
      "metadata": {
        "id": "fb07d037-e09a-43ab-9a24-f80004fb8bf7"
      },
      "outputs": [],
      "source": [
        "# 각 분류기의 예측 확률 계산\n",
        "combo_preds = combine_voters(X_test, weights=[0.5, 0.5])\n",
        "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
        "global_combo_score_soft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69f7e5fb-c5aa-427e-863b-0e2f6b1604ff",
      "metadata": {
        "id": "69f7e5fb-c5aa-427e-863b-0e2f6b1604ff"
      },
      "outputs": [],
      "source": [
        "combo_preds = combine_voters(X_test, weights=[0.4, 0.6])\n",
        "global_combo_score_soft= f1_score(y_test, combo_preds, average='macro')\n",
        "global_combo_score_soft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "002fcd6f-967b-4139-ad19-16be54389288",
      "metadata": {
        "id": "002fcd6f-967b-4139-ad19-16be54389288"
      },
      "outputs": [],
      "source": [
        "combo_preds = combine_voters(X_test, weights=[0.6, 0.4])\n",
        "global_combo_score_soft = f1_score(y_test, combo_preds, average='macro')\n",
        "global_combo_score_soft"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bca27b5-be98-4db5-99c7-fb51c8e6b48c",
      "metadata": {
        "id": "2bca27b5-be98-4db5-99c7-fb51c8e6b48c"
      },
      "source": [
        "Prepare submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97d2aa78-2909-4814-8f6a-abdf72bcf7e2",
      "metadata": {
        "id": "97d2aa78-2909-4814-8f6a-abdf72bcf7e2"
      },
      "outputs": [],
      "source": [
        "y_subm = pd.DataFrame()\n",
        "y_subm['Id'] = test_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db658c77-7876-4660-8244-399e6b5a7664",
      "metadata": {
        "id": "db658c77-7876-4660-8244-399e6b5a7664"
      },
      "outputs": [],
      "source": [
        "vc.voting = 'soft'\n",
        "y_subm_lgb = y_subm.copy(deep=True)\n",
        "y_subm_lgb['Target'] = vc.predict(test.drop(xgb_drop_cols, axis=1)) + 1\n",
        "\n",
        "vc2.voting = 'soft'\n",
        "y_subm_rf = y_subm.copy(deep=True)\n",
        "y_subm_rf['Target'] = vc2.predict(test.drop(et_drop_cols, axis=1)) + 1\n",
        "\n",
        "y_subm_ens = y_subm.copy(deep=True)\n",
        "y_subm_ens['Target'] = combine_voters(test) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9f46af6-4fc6-4746-8257-dadfac9766a3",
      "metadata": {
        "id": "a9f46af6-4fc6-4746-8257-dadfac9766a3"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "\n",
        "sub_file_lgb = 'submission_soft_XGB_{:.4f}_{}.csv'.format(global_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
        "sub_file_rf = 'submission_soft_RF_{:.4f}_{}.csv'.format(global_rf_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
        "sub_file_ens = 'submission_ens_{:.4f}_{}.csv'.format(global_combo_score_soft, str(now.strftime('%Y-%m-%d-%H-%M')))\n",
        "\n",
        "y_subm_lgb.to_csv(sub_file_lgb, index=False)\n",
        "y_subm_rf.to_csv(sub_file_rf, index=False)\n",
        "y_subm_ens.to_csv(sub_file_ens, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf46a1a0-f12d-4f63-8823-292a9afd1c01",
      "metadata": {
        "id": "cf46a1a0-f12d-4f63-8823-292a9afd1c01"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "068ddd62-3e7c-493c-8da4-6e30a1179420",
      "metadata": {
        "id": "068ddd62-3e7c-493c-8da4-6e30a1179420"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}