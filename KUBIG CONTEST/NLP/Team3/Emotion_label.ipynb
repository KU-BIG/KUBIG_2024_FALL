{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMUkT9dR7w26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a17d5414-1764-42e7-d5a4-5e326dbc061c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AAGlDMp9HI0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XQ4WDGAu-2_i"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "# 1. 최상위 폴더 경로 설정 (하위 폴더들이 있는 폴더)\n",
        "folder_path = '/content/drive/MyDrive/NLP project/감정 라벨링데이터'\n",
        "\n",
        "# 2. 빈 리스트 생성 (여기에 각 파일에서 추출된 데이터를 저장할 것)\n",
        "all_data = []\n",
        "\n",
        "# 3. os.walk()를 사용하여 하위 폴더까지 모든 JSON 파일 찾기\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    for file_name in files:\n",
        "        if file_name.endswith('.json'):  # JSON 파일만 처리\n",
        "            file_path = os.path.join(root, file_name)\n",
        "\n",
        "            # 4. JSON 파일 읽기\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # 5. 필요한 데이터 추출 (utterances에서 listener_empathy가 있는 부분만 추출)\n",
        "            utterances = data.get('utterances', [])\n",
        "\n",
        "            for utterance in utterances:\n",
        "                if utterance.get('listener_empathy') is not None:\n",
        "                    all_data.append({\n",
        "                        \"text\": utterance['text'],\n",
        "                        \"listener_empathy\": utterance['listener_empathy']\n",
        "                    })\n",
        "\n",
        "# 6. pandas 데이터프레임으로 변환\n",
        "df = pd.DataFrame(all_data)\n",
        "\n",
        "# 7. listener_empathy 리스트를 콤마로 연결된 문자열로 변환\n",
        "df['emotion_label'] = df['listener_empathy'].apply(lambda x: ', '.join(x))\n",
        "\n",
        "# 8. 최종 데이터프레임 출력 (문장과 감정 라벨)\n",
        "df_final = df[['text', 'emotion_label']]\n",
        "\n",
        "# 9. 데이터프레임 확인\n",
        "df_final.head()\n",
        "\n",
        "# 데이터프레임 크기 확인 (몇 개의 문장이 처리되었는지)\n",
        "print(f\"총 {len(df_final)}개의 문장이 처리되었습니다.\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "siVTfbrrKAUM"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# 데이터프레임 저장\n",
        "df_final.to_csv('/content/drive/MyDrive/NLP project/final_dataframe.csv', index=False, encoding='utf-8-sig')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "5jhGYj6HwYxm",
        "outputId": "38bb5bc4-7519-44c2-9eae-e63463b49175"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/KUBIG CONTEST NLP-Team3/19_최지우/emotion_classifier.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-23d808bef00a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/KUBIG CONTEST NLP-Team3/19_최지우/emotion_classifier.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#  emotion_classifier csv 파일 불러옴\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/KUBIG CONTEST NLP-Team3/19_최지우/emotion_classifier.csv'"
          ]
        }
      ],
      "source": [
        "df_final = pd.read_csv('/content/drive/MyDrive/KUBIG CONTEST NLP-Team3/19_최지우/emotion_classifier.csv') #  emotion_classifier csv 파일 불러옴"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "syBV8vmfnKhH"
      },
      "outputs": [],
      "source": [
        "df_final.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "id": "QWX5xAyvnMAP",
        "outputId": "470ee615-40c4-4937-ce97-f9e297430c9d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>동조</th>\n",
              "      <td>24513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>위로, 동조</th>\n",
              "      <td>18339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>격려</th>\n",
              "      <td>17702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>위로</th>\n",
              "      <td>16320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>격려, 동조</th>\n",
              "      <td>14975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>조언</th>\n",
              "      <td>12956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>조언, 격려</th>\n",
              "      <td>10120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>조언, 동조</th>\n",
              "      <td>9152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>조언, 위로</th>\n",
              "      <td>8002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>격려, 위로</th>\n",
              "      <td>4037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>조언, 격려, 동조</th>\n",
              "      <td>2526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>조언, 위로, 동조</th>\n",
              "      <td>2504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>조언, 격려, 위로</th>\n",
              "      <td>1923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>격려, 위로, 동조</th>\n",
              "      <td>1570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>동조, 격려</th>\n",
              "      <td>502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>격려, 조언</th>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>동조, 조언</th>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>동조, 위로</th>\n",
              "      <td>61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>조언, 격려, 위로, 동조</th>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>동조, 격려, 위로</th>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>격려, 동조, 위로</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>동조, 조언, 위로</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>동조, 격려, 조언</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>격려, 동조, 조언</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>격려, 조언, 위로</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ],
            "text/plain": [
              "emotion_label\n",
              "동조                24513\n",
              "위로, 동조            18339\n",
              "격려                17702\n",
              "위로                16320\n",
              "격려, 동조            14975\n",
              "조언                12956\n",
              "조언, 격려            10120\n",
              "조언, 동조             9152\n",
              "조언, 위로             8002\n",
              "격려, 위로             4037\n",
              "조언, 격려, 동조         2526\n",
              "조언, 위로, 동조         2504\n",
              "조언, 격려, 위로         1923\n",
              "격려, 위로, 동조         1570\n",
              "동조, 격려              502\n",
              "격려, 조언               77\n",
              "동조, 조언               69\n",
              "동조, 위로               61\n",
              "조언, 격려, 위로, 동조       18\n",
              "동조, 격려, 위로           11\n",
              "격려, 동조, 위로            7\n",
              "동조, 조언, 위로            6\n",
              "동조, 격려, 조언            5\n",
              "격려, 동조, 조언            5\n",
              "격려, 조언, 위로            4\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_final.value_counts('emotion_label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9KkHeRmKitR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSzvKiuXMSOD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 가능한 모든 감정 레이블 정의\n",
        "possible_labels = [\"격려\", \"동조\", \"위로\", \"조언\"]\n",
        "\n",
        "#  라벨을 다중 라벨 벡터로 변환\n",
        "# 예: [\"격려\", \"위로\"] -> [1, 0, 1, 0] (격려와 위로는 해당, 동조와 조언은 해당하지 않음)\n",
        "def encode_labels(labels, possible_labels):\n",
        "    label_vector = [1 if label in labels else 0 for label in possible_labels]\n",
        "    return label_vector\n",
        "\n",
        "df_final['encoded_label'] = df_final['emotion_label'].apply(lambda x: encode_labels(x, possible_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Vq_tgqwMj17"
      },
      "outputs": [],
      "source": [
        "# 학습/테스트 데이터 분리\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(df_final['text'], df_final['encoded_label'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHCDK0XizjBl"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# ALBERT 토크나이저 및 모델 로드\n",
        "\n",
        "from transformers import AlbertTokenizer, AlbertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "model = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJOMUZipz4Xd"
      },
      "outputs": [],
      "source": [
        "''''\n",
        "\n",
        "# Roberta 토크나이저 모델 및 로드\n",
        "\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtqnP_PiMlzS",
        "outputId": "018a49a2-3223-4842-9550-4bab7ef458d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#  BERT 토크나이저 및 모델 로드\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(possible_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5i4WY1ZTl9eL",
        "outputId": "597ba061-262f-4940-808c-46d0c6647eb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 6. CUDA 사용 여부 확인 및 모델 이동\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)  # 모델을 GPU로 이동"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZidSCa8cnhaI",
        "outputId": "8ba56773-8be0-48bd-c4c0-f663798710c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWuMA7KvMnol"
      },
      "outputs": [],
      "source": [
        "#  데이터셋 준비\n",
        "class MultiLabelDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=64, return_tensors=\"pt\")\n",
        "        self.labels = torch.tensor(labels.tolist(), dtype=torch.float32)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-3nSk2TMo_E"
      },
      "outputs": [],
      "source": [
        "train_dataset = MultiLabelDataset(train_texts, train_labels)\n",
        "val_dataset = MultiLabelDataset(val_texts, val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkj7Uew3Mqm3",
        "outputId": "64fc6348-13ec-431a-c7ee-2c2d71192297"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#  학습 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/KUBIG CONTEST NLP-Team3/19_최지우/results',\n",
        "    num_train_epochs=3,\n",
        "    save_steps=2_000,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='/content/drive/MyDrive/KUBIG CONTEST NLP-Team3/19_최지우/logs',\n",
        "    logging_steps=1000,\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3XTkWCQMtKV"
      },
      "outputs": [],
      "source": [
        "#  Trainer 설정\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "qE3RfZmQMvC6",
        "outputId": "3541f42a-b90a-43ea-fe66-0f87fb4a6786"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10908' max='10908' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10908/10908 1:10:43, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.554500</td>\n",
              "      <td>0.528382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.506600</td>\n",
              "      <td>0.502544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.479300</td>\n",
              "      <td>0.494819</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10908, training_loss=0.5246419875141811, metrics={'train_runtime': 4243.4472, 'train_samples_per_second': 82.237, 'train_steps_per_second': 2.571, 'total_flos': 1.1477406333040128e+16, 'train_loss': 0.5246419875141811, 'epoch': 3.0})"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#  모델 학습\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "NHBrLwLMNReb",
        "outputId": "3268622e-50aa-4c66-cf7c-567c5f3c822f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='909' max='909' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [909/909 01:43]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.4948192238807678,\n",
              " 'eval_runtime': 103.3002,\n",
              " 'eval_samples_per_second': 281.519,\n",
              " 'eval_steps_per_second': 8.8,\n",
              " 'epoch': 3.0}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#  모델 평가\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84GBYuE6pL79"
      },
      "outputs": [],
      "source": [
        "# 모델과 토크나이저 저장 (경로 따로 설정 필요)\n",
        "model.save_pretrained('/content/drive/MyDrive/KUBIG CONTEST NLP-Team3/19_최지우/saved_model')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/KUBIG CONTEST NLP-Team3/19_최지우/saved_model')\n",
        "\n",
        "# 저장된 모델과 토크나이저 불러오기 (경로 따로 설정 필요)\n",
        "model = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/KUBIG CONTEST NLP-Team3/19_최지우/saved_model')\n",
        "tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/KUBIG CONTEST NLP-Team3/19_최지우/saved_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "Bx-o8chdhejD",
        "outputId": "e7406f01-0023-4eaf-ca8a-64e2cb247ea3"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Incorrect path_or_model_id: '/content/drive/MyDrive/KUBIG CONTEST NLP-Team3/19_최지우/saved_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/KUBIG CONTEST NLP-Team3/19_최지우/saved_model'. Use `repo_type` argument if needed.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a3fcc1ba53cb>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 저장된 모델과 토크나이저 불러오기 (경로 따로 설정 필요)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/KUBIG CONTEST NLP-Team3/19_최지우/saved_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/KUBIG CONTEST NLP-Team3/19_최지우/saved_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3127\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3128\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m   3129\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3130\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{err}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHFValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;34mf\"Incorrect path_or_model_id: '{path_or_repo_id}'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         ) from e\n",
            "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/content/drive/MyDrive/KUBIG CONTEST NLP-Team3/19_최지우/saved_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# 저장된 모델과 토크나이저 불러오기 (경로 따로 설정 필요)\n",
        "model = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/KUBIG CONTEST NLP-Team3/19_최지우/saved_model')\n",
        "tokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/KUBIG CONTEST NLP-Team3/19_최지우/saved_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaM7CV0CokgA"
      },
      "outputs": [],
      "source": [
        "def predict_empathy_label(model, tokenizer, text, possible_labels, device='cpu'):\n",
        "    \"\"\"\n",
        "    입력된 문장에 대한 공감 라벨을 예측하는 함수\n",
        "\n",
        "    Args:\n",
        "        model: 학습된 BERT 모델 (BertForSequenceClassification)\n",
        "        tokenizer: BERT 토크나이저 (BertTokenizer)\n",
        "        text: 예측할 문장 (str)\n",
        "        possible_labels: 가능한 모든 공감 라벨 리스트 (예: [\"격려\", \"동조\", \"위로\", \"조언\"])\n",
        "        device: 사용할 장치 (GPU 또는 CPU)\n",
        "\n",
        "    Returns:\n",
        "        predicted_emotions: 예측된 공감 라벨 리스트 (예: [\"격려\", \"위로\"])\n",
        "    \"\"\"\n",
        "\n",
        "    # 모델을 평가 모드로 전환\n",
        "    model.eval()\n",
        "\n",
        "    # 입력 문장을 BERT 입력 형식으로 변환하고 지정된 장치로 이동\n",
        "    inputs = tokenizer(text, truncation=True, padding=True, max_length=64, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # 모델 예측 수행\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # 시그모이드를 통해 각 감정에 대한 확률로 변환\n",
        "    predictions = torch.sigmoid(logits)\n",
        "\n",
        "    # 임계값(threshold)을 설정해 해당 감정에 속하는지 여부를 결정 (0.5 이상일 경우 감정이 해당한다고 판단)\n",
        "    threshold = 0.5\n",
        "    predicted_labels = (predictions > threshold).int().tolist()[0]\n",
        "\n",
        "    # 예측된 라벨을 실제 감정으로 변환\n",
        "    predicted_emotions = [possible_labels[i] for i, label in enumerate(predicted_labels) if label == 1]\n",
        "    print(predictions)\n",
        "    return predicted_emotions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRpSZ6oDouy0",
        "outputId": "137ffffe-44c1-470c-f729-cc43a4a7aa2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2547, 0.5680, 0.0708, 0.3061]], device='cuda:0')\n",
            "예측된 공감 라벨: ['동조', '조언']\n"
          ]
        }
      ],
      "source": [
        "# 가능한 공감 라벨 정의 (이 예시에서는 [\"격려\", \"동조\", \"위로\", \"조언\"])\n",
        "possible_labels = [\"격려\", \"동조\", \"위로\", \"조언\"]\n",
        "\n",
        "# 예시 문장\n",
        "new_sentence = \"위로가 나오려면 어떻게 해야 하지?\"\n",
        "\n",
        "# 예측 함수 호출 (학습된 모델, 토크나이저, 예측할 문장, 가능한 공감 라벨, GPU 장치)\n",
        "predicted_empathy = predict_empathy_label(model, tokenizer, new_sentence, possible_labels, device)\n",
        "\n",
        "print(f\"예측된 공감 라벨: {predicted_empathy}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}