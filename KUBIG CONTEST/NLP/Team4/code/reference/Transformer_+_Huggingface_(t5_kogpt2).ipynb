{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### 0. 라이브러리 import 및 설치"
      ],
      "metadata": {
        "id": "awTA7vSsRqsa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mncmHGBRgvH",
        "outputId": "a6ec79fd-56c2-4a99-ce70-8e5292726a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "9c73TqLHR2qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kiwipiepy"
      ],
      "metadata": {
        "id": "OBrZE4SlR4mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y mecab libmecab-dev mecab-ipadic-utf8\n",
        "!git clone --depth 1 https://github.com/gogumaUno/mecab-ko-dic.git\n",
        "!bash mecab-ko-dic/tools/add-userdic.sh\n",
        "!sudo cp -r mecab-ko-dic /usr/local/lib/mecab/dic/mecab-ko-dic\n",
        "!pip install mecab-python3"
      ],
      "metadata": {
        "id": "mJr-2Xk-R8xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext"
      ],
      "metadata": {
        "id": "lQvO_Q_SSAx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from konlpy.tag import Okt, Kkma, Hannanum, Komoran\n",
        "from kiwipiepy import Kiwi\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# from torchtext.data import Field, TabularDataset, BucketIterator\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# from torchtext.vocab import Vocab\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "7c2lVUtXSB4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A. 데이터 전처리"
      ],
      "metadata": {
        "id": "OCOz3wIgSGkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "standard = pd.read_parquet(\"/content/drive/MyDrive/2024-2 KUBIG/24S NLP Basic Team 4/data/경상도_학습데이터_대화_표준버전.parquet\")\n",
        "dialect = pd.read_parquet(\"/content/drive/MyDrive/2024-2 KUBIG/24S NLP Basic Team 4/data/경상도_학습데이터_대화_방언버전_방언개수추가.parquet\")"
      ],
      "metadata": {
        "id": "VezK0l1dSFA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(columns=[\"speaker_1\", \"speaker_2\", \"speaker_2_eojeol_sum\"])\n",
        "data[\"speaker_1\"] = list(standard[\"speaker_1\"])\n",
        "data[\"speaker_2\"] = list(dialect[\"speaker_2\"])\n",
        "data[\"speaker_2_eojeol_sum\"] = list(dialect[\"speaker_2_eojeol_sum\"])"
      ],
      "metadata": {
        "id": "sJH0YqPhSM0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "HxoZwmH7SOJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 구두점, 기호를 없애주고 소문자로 바꿔주는 함수\n",
        "def remove_punc(string):\n",
        "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    no_punct = \"\"\n",
        "    for char in string:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char  # space is also a character\n",
        "    return no_punct\n",
        "\n",
        "# 길이가 하나인 문자를 없애주는 함수\n",
        "def del_one_char(string):\n",
        "    result = []\n",
        "    string_seq = string.split(\" \")\n",
        "    for s in string_seq:\n",
        "        if len(s) != 1:\n",
        "            result.append(s)\n",
        "    return ' '.join(result)\n",
        "\n",
        "def is_len_over_three(string):\n",
        "    string_seq = string.split(\" \")\n",
        "    if len(string_seq) > 3:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "metadata": {
        "id": "bAr8m9k5SQOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 영어도 지울지는 잠시 보류\n",
        "data[\"speaker_1\"] = data[\"speaker_1\"].apply(lambda x : remove_punc(x))\n",
        "data[\"speaker_2\"] = data[\"speaker_2\"].apply(lambda x : remove_punc(x))\n",
        "\n",
        "# 이 가설이 정확할지는 모르겠으나, \"챗봇\" 구현에 중요한 요소 중 하나는 얼마나 사람처럼 응답하는가 인 것 같아\n",
        "# 한 글자짜리 단어를 모두 지울 시 어색해지지 않을까 하는 우려에서 우선 배제\n",
        "# data[\"speaker_1\"] = data[\"speaker_1\"].apply(lambda x : del_one_char(x))\n",
        "# data[\"speaker_2\"] = data[\"speaker_2\"].apply(lambda x : del_one_char(x))\n",
        "\n",
        "# 질의의 단어 수가 충분치 않은 경우, 질문과 응답의 영양가가 없어보여 느낌상 3개 초과의 단어로 구성된 문장만 반영하도록\n",
        "data = data[data[\"speaker_1\"].apply(lambda x : is_len_over_three(x))]\n",
        "data = data[data[\"speaker_2\"].apply(lambda x : is_len_over_three(x))]"
      ],
      "metadata": {
        "id": "VPDmmHr9SRco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basic EDA\n",
        "\n",
        "data_2 = data.copy()\n",
        "\n",
        "data_2[\"speaker_1_length\"] = data_2[\"speaker_1\"].apply(lambda x : len(x.split(\" \")))\n",
        "data_2[\"speaker_2_length\"] = data_2[\"speaker_2\"].apply(lambda x : len(x.split(\" \")))\n",
        "\n",
        "speaker_1_length = list(data_2[\"speaker_1_length\"])\n",
        "speaker_2_length = list(data_2[\"speaker_2_length\"])\n",
        "\n",
        "quartiles_1 = np.percentile(speaker_1_length, [25, 50, 75])\n",
        "quartiles_2 = np.percentile(speaker_2_length, [25, 50, 75])\n",
        "quartiles_3 = np.percentile(data_2[\"speaker_2_eojeol_sum\"], [25, 50, 75])\n",
        "\n",
        "# 사분위수\n",
        "print(quartiles_1)\n",
        "print(quartiles_2)\n",
        "print(quartiles_3)\n",
        "\n",
        "print(len(data_2[data_2[\"speaker_2_eojeol_sum\"] == 0]))\n",
        "\n",
        "data_2 = data_2[data_2[\"speaker_2_eojeol_sum\"] != 0]"
      ],
      "metadata": {
        "id": "O_S1WMYUSTZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### B. Transformer"
      ],
      "metadata": {
        "id": "0Q7m1Z5LShMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# kiwi / okt / kkma / komoran / split (띄워쓰기)\n",
        "# 총 5가지 종류의 tokenizer 를 만들어 테스트 진행\n",
        "\n",
        "okt = Okt()\n",
        "kkma = Kkma()\n",
        "kiwi = Kiwi()\n",
        "komoran = Komoran()\n",
        "\n",
        "def tokenizer_kiwi(text):\n",
        "  result = kiwi.analyze(text)\n",
        "  kiwi_result = []\n",
        "\n",
        "  for sentence in result:\n",
        "    for word in sentence[0]:\n",
        "      kiwi_result.append(word.form)\n",
        "\n",
        "  return kiwi_result\n",
        "\n",
        "def tokenizer_okt(text):\n",
        "  return okt.morphs(text)\n",
        "\n",
        "def tokenizer_kkma(text):\n",
        "  return kkma.morphs(text)\n",
        "\n",
        "def tokenizer_komoran(text):\n",
        "  return komoran.morphs(text)\n",
        "\n",
        "def tokenizer_split(text):\n",
        "  return text.split(\" \")\n"
      ],
      "metadata": {
        "id": "SIyLPDD6SWSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab (단어와 index 를 매핑해주는 사전) 생성 메서드\n",
        "def build_vocab(dataframe, target_column, tokenizer, min_freq):\n",
        "  tokenized_texts = []\n",
        "\n",
        "  for idx, row in dataframe.iterrows():\n",
        "    tokens = tokenizer(row[target_column])\n",
        "    tokenized_texts.append(tokens)\n",
        "\n",
        "  # counter = Counter(token for tokens in tokenized_texts for token in tokens)\n",
        "  def yield_tokens():\n",
        "    for tokens in tokenized_texts:\n",
        "      yield tokens\n",
        "\n",
        "  vocab = build_vocab_from_iterator(yield_tokens(), specials=['<unk>', '<pad>', '<sos>', '<eos>'], min_freq=min_freq)\n",
        "  # unk 토큰 설정\n",
        "  vocab.set_default_index(vocab['<unk>'])\n",
        "  # vocab = Vocab(counter, specials=['<unk>', '<pad>', '<sos>', '<eos>'], min_freq=min_freq)\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "4R9j1KCvSWKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train / test data split\n",
        "\n",
        "train, test = train_test_split(data_2, test_size=0.2, random_state=1097)"
      ],
      "metadata": {
        "id": "7v0trPVSSaNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch 의 dataset / dataloader\n",
        "class CustomTextDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, vocab_src, vocab_trg, max_len=30):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab_src = vocab_src\n",
        "        self.vocab_trg = vocab_trg\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = self.data.iloc[idx, 0]\n",
        "        trg_text = self.data.iloc[idx, 1]\n",
        "        src_tokens = self.tokenizer(src_text)\n",
        "        trg_tokens = self.tokenizer(trg_text)\n",
        "\n",
        "        src_indexes = [self.vocab_src['<sos>']]\n",
        "        trg_indexes = [self.vocab_trg['<sos>']]\n",
        "\n",
        "        # 소스 토큰을 인덱스로 변환 (존재하지 않는 키는 <unk>으로 처리)\n",
        "        for token in src_tokens[:self.max_len - 2]:\n",
        "            try:\n",
        "                src_indexes.append(self.vocab_src[token])\n",
        "            except KeyError:\n",
        "                src_indexes.append(self.vocab_src['<unk>'])\n",
        "\n",
        "        # 타겟 토큰을 인덱스로 변환 (존재하지 않는 키는 <unk>으로 처리)\n",
        "        for token in trg_tokens[:self.max_len - 2]:\n",
        "            try:\n",
        "                trg_indexes.append(self.vocab_trg[token])\n",
        "            except KeyError:\n",
        "                trg_indexes.append(self.vocab_trg['<unk>'])\n",
        "\n",
        "        # 종료 토큰 추가\n",
        "        src_indexes.append(self.vocab_src['<eos>'])\n",
        "        trg_indexes.append(self.vocab_trg['<eos>'])\n",
        "\n",
        "        if len(src_indexes) < self.max_len:\n",
        "            src_indexes += [self.vocab_src['<pad>']] * (self.max_len - len(src_indexes))\n",
        "        if len(trg_indexes) < self.max_len:\n",
        "            trg_indexes += [self.vocab_trg['<pad>']] * (self.max_len - len(trg_indexes))\n",
        "\n",
        "        return torch.tensor(src_indexes), torch.tensor(trg_indexes)\n",
        "# collate_fn 이 max_len torch 길이를 맞춰주기 위한 역할인데,\n",
        "# 여기서 두 가지 아이디어가 존재한다.\n",
        "#   1. 단순 길이 기준으로 자르기\n",
        "#   2. vocab 의 우선도 기준으로 자르기\n",
        "# 만약 단순히 토큰의 최대 길이에 맞춰 padding 을 넣어주면 쓸데없이 크기만 커지는 걸 방지하기 위함\n",
        "\n",
        "# 가변 배치 학습\n",
        "# sampler 를 이용하면 되는데 이 부분은 우선 pass"
      ],
      "metadata": {
        "id": "yAffSb_zSqvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_src = build_vocab(train, \"speaker_1\", tokenizer_okt, min_freq=2)\n",
        "vocab_trg = build_vocab(train, \"speaker_2\", tokenizer_okt, min_freq=2)"
      ],
      "metadata": {
        "id": "NasihbwBSsfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(vocab_src))\n",
        "print(len(vocab_trg))"
      ],
      "metadata": {
        "id": "QFG5ciOaStQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset 및 DataLoader\n",
        "train_dataset = CustomTextDataset(train, tokenizer_okt, vocab_src, vocab_trg)\n",
        "test_dataset = CustomTextDataset(test, tokenizer_okt, vocab_src, vocab_trg)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "XheU_BHhSuLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader 예시 확인\n",
        "for i, (src_batch, trg_batch) in enumerate(train_loader):\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    print(f\"Source Batch:\\n{src_batch}\")\n",
        "    print(f\"Target Batch:\\n{trg_batch}\")\n",
        "    print(\"\\n\")\n",
        "    if i == 2:  # 3개의 배치만 출력\n",
        "        break"
      ],
      "metadata": {
        "id": "fBJUUrYvSvPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, dropout_ratio, device):\n",
        "        super().__init__()\n",
        "\n",
        "        assert hidden_dim % n_heads == 0\n",
        "\n",
        "        self.hidden_dim = hidden_dim # 임베딩 차원\n",
        "        self.n_heads = n_heads # 헤드(head)의 개수: 서로 다른 어텐션(attention) 컨셉의 수\n",
        "        self.head_dim = hidden_dim // n_heads # 각 헤드(head)에서의 임베딩 차원\n",
        "\n",
        "        self.fc_q = nn.Linear(hidden_dim, hidden_dim) # Query 값에 적용될 FC 레이어\n",
        "        self.fc_k = nn.Linear(hidden_dim, hidden_dim) # Key 값에 적용될 FC 레이어\n",
        "        self.fc_v = nn.Linear(hidden_dim, hidden_dim) # Value 값에 적용될 FC 레이어\n",
        "\n",
        "        self.fc_o = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        # query: [batch_size, query_len, hidden_dim]\n",
        "        # key: [batch_size, key_len, hidden_dim]\n",
        "        # value: [batch_size, value_len, hidden_dim]\n",
        "\n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "\n",
        "        # Q: [batch_size, query_len, hidden_dim]\n",
        "        # K: [batch_size, key_len, hidden_dim]\n",
        "        # V: [batch_size, value_len, hidden_dim]\n",
        "\n",
        "        # hidden_dim → n_heads X head_dim 형태로 변형\n",
        "        # n_heads(h)개의 서로 다른 어텐션(attention) 컨셉을 학습하도록 유도\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Q: [batch_size, n_heads, query_len, head_dim]\n",
        "        # K: [batch_size, n_heads, key_len, head_dim]\n",
        "        # V: [batch_size, n_heads, value_len, head_dim]\n",
        "\n",
        "        # Attention Energy 계산\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "        # energy: [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        # 마스크(mask)를 사용하는 경우\n",
        "        if mask is not None:\n",
        "            # 마스크(mask) 값이 0인 부분을 -1e10으로 채우기\n",
        "            energy = energy.masked_fill(mask==0, -1e10)\n",
        "\n",
        "        # 어텐션(attention) 스코어 계산: 각 단어에 대한 확률 값\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "\n",
        "        # attention: [batch_size, n_heads, query_len, key_len]\n",
        "\n",
        "        # 여기에서 Scaled Dot-Product Attention을 계산\n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "\n",
        "        # x: [batch_size, n_heads, query_len, head_dim]\n",
        "\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "        # x: [batch_size, query_len, n_heads, head_dim]\n",
        "\n",
        "        x = x.view(batch_size, -1, self.hidden_dim)\n",
        "\n",
        "        # x: [batch_size, query_len, hidden_dim]\n",
        "\n",
        "        x = self.fc_o(x)\n",
        "\n",
        "        # x: [batch_size, query_len, hidden_dim]\n",
        "\n",
        "        return x, attention"
      ],
      "metadata": {
        "id": "eqH0C8fnSxBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 포워드 레이어 (선형변환)\n",
        "\n",
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, pf_dim, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc_1 = nn.Linear(hidden_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # x: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "\n",
        "        # x: [batch_size, seq_len, pf_dim]\n",
        "\n",
        "        x = self.fc_2(x)\n",
        "\n",
        "        # x: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "fga_7V7hSyIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더 레이어\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 하나의 임베딩이 복제되어 Query, Key, Value로 입력되는 방식\n",
        "    def forward(self, src, src_mask):\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        # self attention\n",
        "        # 필요한 경우 마스크(mask) 행렬을 이용하여 어텐션(attention)할 단어를 조절 가능\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "\n",
        "        # DH : forward 메서드를 호출할 필요가 없음\n",
        "\n",
        "        # dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        # position-wise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "\n",
        "        # dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        return src"
      ],
      "metadata": {
        "id": "LGbya3f8SzHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인코더 모델 생성\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hidden_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
        "\n",
        "        # nn.Embedding : https://wikidocs.net/64779\n",
        "        # 임베딩은 결국 룩업 테이블을 생성하는 것...!\n",
        "\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
        "\n",
        "        # nn.ModuleList : nn.Module 클래스를 확장하는 데에 사용하는 유틸리티\n",
        "        # 여러 층으로 구성되어있는 Encoder 부분을 관리하기 쉽도록 활용함\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "\n",
        "        # src: [batch_size, src_len]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        # positional encoding\n",
        "\n",
        "        # pos: [batch_size, src_len]\n",
        "\n",
        "        # 소스 문장의 임베딩과 위치 임베딩을 더한 것을 사용\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "        # note : 으음 이게 잘 되나?\n",
        "        # 위치 정보가 충분히 반영되지 않을 것 같은 느낌.\n",
        "        # nn.Emdedding 의 임베딩 테이블은 랜덤한 숫자들로 생성될텐데,,,\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        # 모든 인코더 레이어를 차례대로 거치면서 순전파(forward) 수행\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "\n",
        "        # src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        return src # 마지막 레이어의 출력을 반환"
      ],
      "metadata": {
        "id": "5E_3mbjlS0JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더 레이어\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dim, n_heads, pf_dim, dropout_ratio, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hidden_dim, n_heads, dropout_ratio, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dim, pf_dim, dropout_ratio)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "    # 인코더의 출력 값(enc_src)을 어텐션(attention)하는 구조\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "        # enc_src: [batch_size, src_len, hidden_dim]\n",
        "        # trg_mask: [batch_size, trg_len]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        # self attention\n",
        "        # 자기 자신에 대하여 어텐션(attention)\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "\n",
        "        # dropout, residual connection and layer norm\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "        # encoder attention\n",
        "        # 디코더의 쿼리(Query)를 이용해 인코더를 어텐션(attention)\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "\n",
        "        # dropout, residual connection and layer norm\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "        # positionwise feedforward\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "\n",
        "        # dropout, residual and layer norm\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "        return trg, attention"
      ],
      "metadata": {
        "id": "Rd53AwGOS1Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더 모델 생성\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, n_layers, n_heads, pf_dim, dropout_ratio, device, max_length=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hidden_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hidden_dim)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hidden_dim, n_heads, pf_dim, dropout_ratio, device) for _ in range(n_layers)])\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
        "\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "\n",
        "        # trg: [batch_size, trg_len]\n",
        "        # enc_src: [batch_size, src_len, hidden_dim]\n",
        "        # trg_mask: [batch_size, trg_len]\n",
        "        # src_mask: [batch_size, src_len]\n",
        "\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "\n",
        "        # pos: [batch_size, trg_len]\n",
        "\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            # 소스 마스크와 타겟 마스크 모두 사용\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        # trg: [batch_size, trg_len, hidden_dim]\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "        output = self.fc_out(trg)\n",
        "\n",
        "        # output: [batch_size, trg_len, output_dim]\n",
        "\n",
        "        return output, attention"
      ],
      "metadata": {
        "id": "CoDiHvbPS2Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 트랜스포머 모델\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    # 소스 문장의 <pad> 토큰에 대하여 마스크(mask) 값을 0으로 설정\n",
        "    def make_src_mask(self, src):\n",
        "\n",
        "        # src: [batch_size, src_len]\n",
        "\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # src_mask: [batch_size, 1, 1, src_len]\n",
        "\n",
        "        return src_mask\n",
        "\n",
        "    # 타겟 문장에서 각 단어는 다음 단어가 무엇인지 알 수 없도록(이전 단어만 보도록) 만들기 위해 마스크를 사용\n",
        "    def make_trg_mask(self, trg):\n",
        "\n",
        "        # trg: [batch_size, trg_len]\n",
        "\n",
        "        \"\"\" (마스크 예시)\n",
        "        1 0 0 0 0\n",
        "        1 1 0 0 0\n",
        "        1 1 1 0 0\n",
        "        1 1 1 0 0\n",
        "        1 1 1 0 0\n",
        "        \"\"\"\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # trg_pad_mask: [batch_size, 1, 1, trg_len]\n",
        "\n",
        "        trg_len = trg.shape[1]\n",
        "\n",
        "        \"\"\" (마스크 예시)\n",
        "        1 0 0 0 0\n",
        "        1 1 0 0 0\n",
        "        1 1 1 0 0\n",
        "        1 1 1 1 0\n",
        "        1 1 1 1 1\n",
        "        \"\"\"\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "\n",
        "        # trg_sub_mask: [trg_len, trg_len]\n",
        "\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "\n",
        "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "\n",
        "        # src: [batch_size, src_len]\n",
        "        # trg: [batch_size, trg_len]\n",
        "\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "\n",
        "        # src_mask: [batch_size, 1, 1, src_len]\n",
        "        # trg_mask: [batch_size, 1, trg_len, trg_len]\n",
        "\n",
        "        enc_src = self.encoder(src, src_mask)\n",
        "\n",
        "        # enc_src: [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "        # output: [batch_size, trg_len, output_dim]\n",
        "        # attention: [batch_size, n_heads, trg_len, src_len]\n",
        "\n",
        "        return output, attention"
      ],
      "metadata": {
        "id": "OOEOpbUpS3Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 튜닝\n",
        "\n",
        "INPUT_DIM = len(vocab_src)\n",
        "OUTPUT_DIM = len(vocab_trg)\n",
        "HIDDEN_DIM = 256\n",
        "ENC_LAYERS = 3\n",
        "DEC_LAYERS = 3\n",
        "ENC_HEADS = 8\n",
        "DEC_HEADS = 8\n",
        "ENC_PF_DIM = 512\n",
        "DEC_PF_DIM = 512\n",
        "ENC_DROPOUT = 0.1\n",
        "DEC_DROPOUT = 0.1"
      ],
      "metadata": {
        "id": "3s296Q0ZS4Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SRC_PAD_IDX = vocab_src.get_stoi[vocab_src.pad_token]\n",
        "# TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "SRC_PAD_IDX = vocab_src[\"<pad>\"]\n",
        "TRG_PAD_IDX = vocab_trg[\"<pad>\"]\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 인코더(encoder)와 디코더(decoder) 객체 선언\n",
        "enc = Encoder(INPUT_DIM, HIDDEN_DIM, ENC_LAYERS, ENC_HEADS, ENC_PF_DIM, ENC_DROPOUT, device)\n",
        "dec = Decoder(OUTPUT_DIM, HIDDEN_DIM, DEC_LAYERS, DEC_HEADS, DEC_PF_DIM, DEC_DROPOUT, device)\n",
        "\n",
        "# Transformer 객체 선언\n",
        "model = Transformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "metadata": {
        "id": "S20C-hBvS58J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파라미터 개수\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "id": "jgok-NZSS66a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model.apply(initialize_weights)"
      ],
      "metadata": {
        "id": "vz5ExYncS8DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Adam optimizer로 학습 최적화\n",
        "LEARNING_RATE = 0.0005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "metadata": {
        "id": "haEU6Sj3S9Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습(train) 함수\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train() # 학습 모드\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # 전체 학습 데이터를 확인하며\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch[0].to(device)\n",
        "        trg = batch[1].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 출력 단어의 마지막 인덱스(<eos>)는 제외\n",
        "        # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "\n",
        "        # output: [배치 크기, trg_len - 1, output_dim]\n",
        "        # trg: [배치 크기, trg_len]\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        # 출력 단어의 인덱스 0(<sos>)은 제외\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "        # output: [배치 크기 * trg_len - 1, output_dim]\n",
        "        # trg: [배치 크기 * trg len - 1]\n",
        "\n",
        "        # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward() # 기울기(gradient) 계산\n",
        "\n",
        "        # 기울기(gradient) clipping 진행\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 전체 손실 값 계산\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "CEBWDj4NS95i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 모델 평가(evaluate) 함수\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval() # 평가 모드\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 전체 평가 데이터를 확인하며\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch[0]\n",
        "            trg = batch[1]\n",
        "\n",
        "            # 출력 단어의 마지F막 인덱스(<eos>)는 제외\n",
        "            # 입력을 할 때는 <sos>부터 시작하도록 처리\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "\n",
        "            # output: [배치 크기, trg_len - 1, output_dim]\n",
        "            # trg: [배치 크기, trg_len]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            # 출력 단어의 인덱스 0(<sos>)은 제외\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "\n",
        "            # output: [배치 크기 * trg_len - 1, output_dim]\n",
        "            # trg: [배치 크기 * trg len - 1]\n",
        "\n",
        "            # 모델의 출력 결과와 타겟 문장을 비교하여 손실 계산\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            # 전체 손실 값 계산\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "kOumQlJkS_AW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time() # 시작 시간 기록\n",
        "\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "\n",
        "    end_time = time.time() # 종료 시간 기록\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):.3f}')"
      ],
      "metadata": {
        "id": "7ZgtWNTlTAR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'transformer_model_tokenizer_okt_dialect_1.pth')"
      ],
      "metadata": {
        "id": "Ww5X4eaHTBum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### C. T5 model + finetuning"
      ],
      "metadata": {
        "id": "f3_jBFr-TEYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
        "from transformers import AdamW\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# 디바이스 설정\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# 데이터셋 클래스 정의\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        questions = self.data.iloc[idx]['speaker_1']\n",
        "        answers = self.data.iloc[idx]['speaker_2']\n",
        "\n",
        "        # 토큰 수를 max_length에 맞춰서 자르거나 패딩\n",
        "        inputs = self.tokenizer(questions, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "        labels = self.tokenizer(answers, max_length=self.max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs.input_ids[0],\n",
        "            'labels': labels.input_ids[0]\n",
        "        }\n",
        "\n",
        "# 토크나이저와 모델 로드\n",
        "# tokenizer = T5TokenizerFast.from_pretrained('paust/pko-chat-t5-base')\n",
        "# model = T5ForConditionalGeneration.from_pretrained('paust/pko-chat-t5-base')\n",
        "\n",
        "tokenizer = T5TokenizerFast.from_pretrained('paust/pko-t5-base')\n",
        "model = T5ForConditionalGeneration.from_pretrained('paust/pko-t5-base')\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# tokenizer = T5TokenizerFast.from_pretrained('paust/pko-t5-base')\n",
        "# model = T5ForConditionalGeneration.from_pretrained('paust/pko-t5-base')\n",
        "\n",
        "# 학습 설정\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 5\n",
        "log_interval = 100\n",
        "checkpoint_interval = 0.5  # 0.5 에폭마다 체크포인트 저장\n",
        "\n",
        "# DataLoader를 사용하여 데이터 로드\n",
        "batch_size = 4\n",
        "dataset = CustomDataset(data_2, tokenizer)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 학습 루프에서 데이터 로더 사용\n",
        "total_steps = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in tqdm(data_loader, total=len(data_loader)):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # 모델 학습\n",
        "        outputs = model(input_ids=input_ids, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # 역전파 및 가중치 업데이트\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_steps += 1\n",
        "\n",
        "        # 0.5 에폭마다 체크포인트 저장\n",
        "        if total_steps % int(len(dataset) / (batch_size * 2)) == 0:\n",
        "            checkpoint_dir = f\"checkpoint_{total_steps}\"\n",
        "            model.save_pretrained(checkpoint_dir)"
      ],
      "metadata": {
        "id": "6-O1TBvKTuX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 상태 사전을 저장할 경로 지정\n",
        "save_path = '/kaggle/working/t5_finetuning_3.pth'\n",
        "\n",
        "# 모델 상태 사전 저장\n",
        "torch.save(model.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "HezHw6vST5nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### D. KoGPT2 + finetuning"
      ],
      "metadata": {
        "id": "CuE7dBJNT846"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "import urllib.request\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel"
      ],
      "metadata": {
        "id": "x7oxL9g9UBRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BOS = \"</s>\"\n",
        "EOS = \"</s>\"\n",
        "PAD = \"<pad>\"\n",
        "MASK = \"<unused0>\"\n",
        "UNK = \"<unk>\"\n",
        "\n",
        "Q_TKN = \"<usr>\"\n",
        "A_TKN = \"<sys>\"\n",
        "SENT = \"<unused1>\"\n",
        "\n",
        "# 허깅페이스 transformers 에 등록된 사전 학습된 koGTP2 토크나이저를 가져온다.\n",
        "koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token=BOS, eos_token=EOS, unk_token=UNK, pad_token=PAD, mask_token=MASK,)\n",
        "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')"
      ],
      "metadata": {
        "id": "2OMlS2aTUAGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 챗봇 데이터를 처리하는 클래스를 만든다.\n",
        "class ChatbotDataset(Dataset):\n",
        "    def __init__(self, chats, max_len=128):  # 데이터셋의 전처리를 해주는 부분\n",
        "        self._data = chats\n",
        "        self.max_len = max_len\n",
        "        self.q_token = Q_TKN\n",
        "        self.a_token = A_TKN\n",
        "        self.sent_token = SENT\n",
        "        self.eos = EOS\n",
        "        self.mask = MASK\n",
        "        self.tokenizer = koGPT2_TOKENIZER\n",
        "\n",
        "    def __len__(self):  # chatbotdata 의 길이를 리턴한다.\n",
        "        return len(self._data)\n",
        "\n",
        "    def __getitem__(self, idx):  # 로드한 챗봇 데이터를 차례차례 DataLoader로 넘겨주는 메서드\n",
        "        turn = self._data.iloc[idx]\n",
        "\n",
        "        q = turn[\"speaker_1\"]  # 질문을 가져온다.\n",
        "        a = turn[\"speaker_2\"]  # 답변을 가져온다.\n",
        "\n",
        "        q_toked = self.tokenizer.tokenize(self.q_token + q + self.sent_token)\n",
        "        q_len = len(q_toked)\n",
        "\n",
        "        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)\n",
        "        a_len = len(a_toked)\n",
        "\n",
        "        #질문의 길이가 최대길이보다 크면\n",
        "        if q_len > self.max_len:\n",
        "            a_len = self.max_len - q_len        #답변의 길이를 최대길이 - 질문길이\n",
        "            if a_len <= 0:       #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n",
        "                q_toked = q_toked[-(int(self.max_len / 2)) :]   #질문길이를 최대길이의 반으로\n",
        "                q_len = len(q_toked)\n",
        "                a_len = self.max_len - q_len              #답변의 길이를 최대길이 - 질문길이\n",
        "            a_toked = a_toked[:a_len]\n",
        "            a_len = len(a_toked)\n",
        "\n",
        "        #질문의 길이 + 답변의 길이가 최대길이보다 크면\n",
        "        if q_len + a_len > self.max_len:\n",
        "            a_len = self.max_len - q_len        #답변의 길이를 최대길이 - 질문길이\n",
        "            if a_len <= 0:       #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n",
        "                q_toked = q_toked[-(int(self.max_len / 2)) :]   #질문길이를 최대길이의 반으로\n",
        "                q_len = len(q_toked)\n",
        "                a_len = self.max_len - q_len              #답변의 길이를 최대길이 - 질문길이\n",
        "            a_toked = a_toked[:a_len]\n",
        "            a_len = len(a_toked)\n",
        "\n",
        "        # 답변 labels = [mask, mask, ...., mask, ..., <bos>,..답변.. <eos>, <pad>....]\n",
        "        labels = [self.mask,] * q_len + a_toked[1:]\n",
        "\n",
        "        # mask = 질문길이 0 + 답변길이 1 + 나머지 0\n",
        "        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)\n",
        "        # 답변 labels을 index 로 만든다.\n",
        "        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)\n",
        "        # 최대길이만큼 PADDING\n",
        "        while len(labels_ids) < self.max_len:\n",
        "            labels_ids += [self.tokenizer.pad_token_id]\n",
        "\n",
        "        # 질문 + 답변을 index 로 만든다.\n",
        "        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)\n",
        "        # 최대길이만큼 PADDING\n",
        "        while len(token_ids) < self.max_len:\n",
        "            token_ids += [self.tokenizer.pad_token_id]\n",
        "\n",
        "        # #질문+답변, 마스크, 답변\n",
        "        # return (token_ids, np.array(mask), labels_ids)\n",
        "\n",
        "        return torch.LongTensor(token_ids), torch.LongTensor(mask), torch.LongTensor(labels_ids)"
      ],
      "metadata": {
        "id": "qPiBSI5dUHiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    data = [item[0] for item in batch]\n",
        "    mask = [item[1] for item in batch]\n",
        "    label = [item[2] for item in batch]\n",
        "\n",
        "    # 각 리스트를 텐서로 변환하고 배치 차원 추가\n",
        "    data = torch.stack(data)\n",
        "    mask = torch.stack(mask)\n",
        "    label = torch.stack(label)\n",
        "\n",
        "    return data, mask, label"
      ],
      "metadata": {
        "id": "RMm_QbafUOzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = ChatbotDataset(data_2, max_len=128)\n",
        "\n",
        "#윈도우 환경에서 num_workers 는 무조건 0으로 지정, 리눅스에서는 2\n",
        "train_dataloader = DataLoader(train_set, batch_size=8, num_workers=2, shuffle=True, collate_fn=collate_batch,)"
      ],
      "metadata": {
        "id": "6rq5-7ouUQFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"start\")\n",
        "for batch_idx, samples in enumerate(train_dataloader):\n",
        "    token_ids, mask, label = samples\n",
        "    print(\"token_ids ====> \", token_ids)\n",
        "    print(\"mask =====> \", mask)\n",
        "    print(\"label =====> \", label)\n",
        "    break\n",
        "print(\"end\")"
      ],
      "metadata": {
        "id": "w7We03aNUREW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "model.train()"
      ],
      "metadata": {
        "id": "qpRvIGLgUSbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼 파라미터 설정\n",
        "\n",
        "learning_rate = 3e-5\n",
        "criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "num_epoch = 5\n",
        "Sneg = -1e18"
      ],
      "metadata": {
        "id": "Scg4CpAjUTeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm  # tqdm 모듈 임포트\n",
        "\n",
        "total_steps = 0\n",
        "for epoch in range(num_epoch):\n",
        "    print(f\"Epoch {epoch+1}/{num_epoch}\")\n",
        "\n",
        "    # tqdm을 사용하여 진행 상황 표시\n",
        "    for batch_idx, samples in enumerate(tqdm(train_dataloader, total=len(train_dataloader))):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # GPU로 텐서를 이동\n",
        "        token_ids, mask, label = [tensor.to(device) for tensor in samples]\n",
        "\n",
        "        # 모델의 출력 계산\n",
        "        out = model(token_ids)\n",
        "        out = out.logits  # logits 계산\n",
        "\n",
        "        # 마스크 적용\n",
        "        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)\n",
        "        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out))\n",
        "\n",
        "        # 손실 계산\n",
        "        loss = criterion(mask_out.transpose(2, 1), label)\n",
        "        avg_loss = loss.sum() / mask.sum()\n",
        "\n",
        "        # 역전파 및 가중치 업데이트\n",
        "        avg_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_steps += 1\n",
        "\n",
        "        # 0.5 에폭마다 체크포인트 저장\n",
        "        if total_steps % int(len(train_dataloader) / 2) == 0:\n",
        "            checkpoint_dir = f\"checkpoint_{total_steps}\"\n",
        "            model.save_pretrained(checkpoint_dir)\n",
        "            print(f\"Checkpoint saved at step {total_steps}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "id": "Qhg9FmaGUWEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 상태 사전을 저장할 경로 지정\n",
        "save_path = '/kaggle/working/kogpt2_finetuning.pth'\n",
        "\n",
        "# 모델 상태 사전 저장\n",
        "torch.save(model.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "T5tZF7jeUXVP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}