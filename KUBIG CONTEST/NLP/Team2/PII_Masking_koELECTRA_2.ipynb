{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Btso2j6WYwQ"
      },
      "source": [
        "- 변경사항\n",
        "  - koELECTRA 모델 사용\n",
        "  - 모델에 맞게 코드 수정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "aqZK90HzWRkb",
        "outputId": "c9f959cd-83fd-4d0e-fe97-dee2708751c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3595' max='3595' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3595/3595 22:24, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.198600</td>\n",
              "      <td>0.180708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.136400</td>\n",
              "      <td>0.126268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.095600</td>\n",
              "      <td>0.119148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.088200</td>\n",
              "      <td>0.111439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.080100</td>\n",
              "      <td>0.110998</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='219' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [219/219 00:25]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed and model saved.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import torch\n",
        "from transformers import ElectraTokenizerFast, ElectraForTokenClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "\n",
        "def generate_fake_data(num_samples):\n",
        "    names = [\"홍길동\", \"김철수\", \"이영희\", \"박지성\", \"최민수\", \"정수연\", \"강태풍\", \"윤미래\", \"송하늘\", \"조해성\"]\n",
        "    places = [\"서울\", \"부산\", \"대구\", \"인천\", \"광주\", \"대전\", \"울산\", \"세종\", \"제주\", \"수원\", \"창원\"]\n",
        "    organizations = [\"삼성\", \"LG\", \"현대\", \"SK\", \"롯데\", \"네이버\", \"카카오\", \"쿠팡\", \"배달의민족\", \"토스\"]\n",
        "    phone_prefixes = [\"010\", \"011\", \"016\", \"017\", \"018\", \"019\"]\n",
        "    rrn_prefixes = [\"990101\", \"850505\", \"770303\", \"880808\", \"920414\", \"660606\", \"950707\", \"001212\"]\n",
        "    bank_names = [\"국민은행\", \"신한은행\", \"우리은행\", \"하나은행\", \"기업은행\", \"농협은행\", \"케이뱅크\", \"카카오뱅크\"]\n",
        "    job_titles = [\"사원\", \"대리\", \"과장\", \"차장\", \"부장\", \"이사\", \"상무\", \"전무\", \"대표\", \"인턴\"]\n",
        "    departments = [\"영업부\", \"마케팅부\", \"인사부\", \"재무부\", \"개발부\", \"디자인부\", \"고객서비스팀\", \"연구소\"]\n",
        "\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        name = random.choice(names)\n",
        "        place = random.choice(places)\n",
        "        org = random.choice(organizations)\n",
        "        phone = f\"{random.choice(phone_prefixes)}-{random.randint(1000, 9999)}-{random.randint(1000, 9999)}\"\n",
        "        rrn = f\"{random.choice(rrn_prefixes)}-{random.randint(1000000, 9999999)}\"\n",
        "        bank = random.choice(bank_names)\n",
        "        account = f\"{random.randint(100, 999)}-{random.randint(10000, 99999)}-{random.randint(10000, 99999)}\"\n",
        "        job_title = random.choice(job_titles)\n",
        "        department = random.choice(departments)\n",
        "\n",
        "        # 다양한 구조의 문장 중 하나를 무작위로 선택하여 가상의 데이터를 생성\n",
        "        structures = [\n",
        "            lambda: f\"{name}의 인적사항: 거주지 {place}, 연락처 {phone}, 주민등록번호 {rrn}. {org} {department} {job_title}로 근무 중. {bank} 계좌번호: {account}\",\n",
        "            lambda: f\"{org} {department}의 {job_title} {name}씨 정보 - 전화번호: {phone}, 주소지: {place}, 주민번호: {rrn}, {bank} 계좌: {account}\",\n",
        "            lambda: f\"이름: {name}, 직장: {org} ({department}, {job_title}), 연락처: {phone}, 거주지: {place}, 주민등록번호: {rrn}, 급여계좌: {bank} {account}\",\n",
        "            lambda: f\"{place}에 거주하는 {name}({rrn})씨는 {org} {department}에서 {job_title}으로 일하고 있습니다. 연락처는 {phone}이며, {bank} 계좌({account})를 사용합니다.\",\n",
        "            lambda: f\"{bank} 고객정보 - 성명: {name}, 계좌번호: {account}, 전화번호: {phone}, 직장: {org} {department} ({job_title}), 주민등록번호: {rrn}, 주소: {place}\",\n",
        "            lambda: f\"[인사기록] 직원명: {name}, 부서: {department}, 직위: {job_title}, 주민번호: {rrn}, 연락처: {phone}, 주소: {place}, 급여계좌: {bank} {account}\",\n",
        "            lambda: f\"{org} {department} {name} {job_title}님의 연락처는 {phone}입니다. 주민등록번호 {rrn}, {place} 거주, {bank} 계좌번호 {account}.\",\n",
        "            lambda: f\"성명 {name} (주민등록번호: {rrn}) / 근무처: {org} {department} / 직위: {job_title} / 주소: {place} / 연락처: {phone} / 계좌: {bank} {account}\",\n",
        "            lambda: f\"{place}의 {name}씨는 {org} {department}에서 {job_title}으로 일합니다. 연락처 {phone}, 주민번호 {rrn}, {bank} 계좌 {account}를 사용 중입니다.\",\n",
        "            lambda: f\"직원정보: {name} / {org} / {department} / {job_title} / {place} 거주 / {phone} / {rrn} / {bank} {account}\",\n",
        "            lambda: f\"{bank} {account} 계좌의 예금주 {name}님 정보 - 주소: {place}, 전화: {phone}, 주민등록번호: {rrn}, 직장: {org} {department} {job_title}\",\n",
        "            lambda: f\"{name}씨는 {org}의 {job_title}입니다. {department} 소속이며, {place}에 삽니다. 연락처는 {phone}, 주민번호는 {rrn}, {bank} 계좌는 {account}입니다.\",\n",
        "            lambda: f\"[{org} 직원 데이터] 이름: {name}, 부서: {department}, 직책: {job_title}, 전화번호: {phone}, 주소: {place}, 주민등록번호: {rrn}, 급여계좌: {bank} {account}\",\n",
        "            lambda: f\"주민등록번호 {rrn}의 {name}님은 {org} {department} {job_title}입니다. {place}에 거주 중이며, 연락처는 {phone}, {bank} 계좌번호는 {account}입니다.\",\n",
        "            lambda: f\"{place} 주민 {name}의 신상정보 - 직장: {org} {department}, 직위: {job_title}, 전화번호: {phone}, 주민번호: {rrn}, 계좌정보: {bank} {account}\",\n",
        "            lambda: f\"{org} {department}의 {name} {job_title}에 대한 정보입니다. 주소: {place}, 전화: {phone}, 주민등록번호: {rrn}, {bank} 계좌번호: {account}\",\n",
        "            lambda: f\"이름: {name} / 주민번호: {rrn} / 주소: {place} / 직장: {org} / 부서: {department} / 직위: {job_title} / 연락처: {phone} / 계좌: {bank} {account}\",\n",
        "            lambda: f\"{bank} 고객 {name}님 ({rrn})의 계좌번호는 {account}입니다. {org} {department} {job_title}이며, {place}에 거주합니다. 연락처: {phone}\",\n",
        "            lambda: f\"{name} ({org} {department}, {job_title}) - 주소: {place}, 전화번호: {phone}, 주민등록번호: {rrn}, {bank} 계좌: {account}\",\n",
        "            lambda: f\"[개인정보] 성명: {name}, 주민번호: {rrn}, 근무지: {org} {department}, 직책: {job_title}, 거주지: {place}, 연락처: {phone}, 계좌: {bank} {account}\"\n",
        "        ]\n",
        "\n",
        "        sample = random.choice(structures)()\n",
        "        data.append(sample)\n",
        "    return data\n",
        "\n",
        "\n",
        "def tokenize_and_label(text):\n",
        "    tokens = []\n",
        "    labels = []\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        if \"-\" in word:\n",
        "            parts = word.split(\"-\")\n",
        "            if len(parts) == 3 and parts[0] in [\"010\", \"011\", \"016\", \"017\", \"018\", \"019\"]:\n",
        "                tokens.extend(parts)\n",
        "                labels.extend([\"B-PH\", \"I-PH\", \"I-PH\"])\n",
        "            elif len(parts) == 2 and len(parts[0]) == 6 and parts[0].isdigit():\n",
        "                tokens.extend(parts)\n",
        "                labels.extend([\"B-RRN\", \"I-RRN\"])\n",
        "            elif len(parts) == 3 and len(parts[0]) == 3 and all(part.isdigit() for part in parts):\n",
        "                tokens.extend(parts)\n",
        "                labels.extend([\"B-ACC\", \"I-ACC\", \"I-ACC\"])\n",
        "            else:\n",
        "                tokens.append(word)\n",
        "                labels.append(\"O\")  # 개인 정보가 아닌 경우 'O' 라벨 부여\n",
        "        else:\n",
        "            tokens.append(word)\n",
        "            labels.append(\"O\")\n",
        "    return tokens, labels\n",
        "\n",
        "def load_data_with_new_labels(file_path, new_data):\n",
        "    texts, labels = [], []\n",
        "    label_map = {\n",
        "        'O': 0, 'B-PS': 1, 'I-PS': 2, 'B-LC': 3, 'I-LC': 4,\n",
        "        'B-OG': 5, 'I-OG': 6, 'B-QT': 7, 'I-QT': 8, 'B-DT': 9,\n",
        "        'I-DT': 10, 'B-TI': 11, 'I-TI': 12, 'B-LT': 13, 'I-LT': 14,\n",
        "        'B-PO': 15, 'I-PO': 16, 'B-PH': 17, 'I-PH': 18, 'B-RRN': 19, 'I-RRN': 20,\n",
        "        'B-ACC': 21, 'I-ACC': 22  # 새로 추가된 계좌번호 라벨\n",
        "    }\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text, label = [], []\n",
        "        for line in file:\n",
        "            if line.startswith('##') or not line.strip():\n",
        "                if text:\n",
        "                    texts.append(\" \".join(text))\n",
        "                    labels.append([label_map[l] for l in label])\n",
        "                    text, label = [], []\n",
        "                continue\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) != 2:\n",
        "                continue\n",
        "            char, tag = parts\n",
        "            text.append(char)\n",
        "            label.append(tag)\n",
        "\n",
        "    # 새로 생성한 가상 데이터셋 추가\n",
        "    for sample in new_data:\n",
        "        tokens, tags = tokenize_and_label(sample)\n",
        "        texts.append(\" \".join(tokens))\n",
        "        labels.append([label_map[tag] for tag in tags])\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "# 가상 데이터셋 생성\n",
        "fake_data = generate_fake_data(2000)\n",
        "# 기존 데이터에 가상 데이터셋을 추가하여 로드\n",
        "train_texts, train_labels = load_data_with_new_labels('klue-ner-v1.1_train (2).tsv', fake_data)\n",
        "dev_texts, dev_labels = load_data_with_new_labels('klue-ner-v1.1_dev (3).tsv', fake_data)\n",
        "\n",
        "# 학습 데이터셋을 정의하는 클래스\n",
        "class PIIDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        labels = self.labels[item]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text.split(),  # 단어 단위로 분리하여 토크나이징\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            is_split_into_words=True,  # 단어 단위로 토크나이징\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        aligned_labels = [-100] * len(input_ids)  # 패딩된 토큰은 -100으로 마스킹\n",
        "        word_ids = encoding.word_ids()  # 원본 텍스트의 단어 위치\n",
        "\n",
        "        for i, word_id in enumerate(word_ids):\n",
        "            if word_id is None:\n",
        "                continue\n",
        "            aligned_labels[i] = labels[word_id]  # 원본 단어의 라벨을 토큰에 매핑\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': torch.tensor(aligned_labels, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "# \"Fast\" 버전의 토크나이저로 변경\n",
        "tokenizer = ElectraTokenizerFast.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
        "model = ElectraForTokenClassification.from_pretrained('monologg/koelectra-base-v3-discriminator', num_labels=23)\n",
        "\n",
        "max_len = 128\n",
        "\n",
        "train_dataset = PIIDataset(train_texts, train_labels, tokenizer, max_len)\n",
        "dev_dataset = PIIDataset(dev_texts, dev_labels, tokenizer, max_len)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False\n",
        ")\n",
        "\n",
        "# Early Stopping 설정 (성능이 개선되지 않으면 학습 중단)\n",
        "early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,\n",
        "    early_stopping_threshold=0.0\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        "    callbacks=[early_stopping_callback]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()\n",
        "\n",
        "# 모델과 토크나이저 저장\n",
        "model.save_pretrained('./pii_masking_model')\n",
        "tokenizer.save_pretrained('./pii_masking_model')\n",
        "\n",
        "print(\"Training completed and model saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BMwuqujhW7iF"
      },
      "outputs": [],
      "source": [
        "def detect_and_mask_pii(text, model, tokenizer, device):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    predictions = torch.argmax(outputs.logits, dim=2)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "\n",
        "    label_map = {\n",
        "        0: 'O', 1: 'B-PS', 2: 'I-PS', 3: 'B-LC', 4: 'I-LC',\n",
        "        5: 'B-OG', 6: 'I-OG', 7: 'B-QT', 8: 'I-QT', 9: 'B-DT',\n",
        "        10: 'I-DT', 11: 'B-TI', 12: 'I-TI', 13: 'B-LT', 14: 'I-LT',\n",
        "        15: 'B-PO', 16: 'I-PO', 17: 'B-PH', 18: 'I-PH', 19: 'B-RRN', 20: 'I-RRN',\n",
        "        21: 'B-ACC', 22: 'I-ACC'  # Added new labels for account numbers\n",
        "    }\n",
        "\n",
        "    masked_text = []\n",
        "    current_mask = None\n",
        "    for token, pred in zip(tokens, predictions[0]):\n",
        "        if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
        "            continue\n",
        "        label = label_map[pred.item()]\n",
        "        if label.startswith('B-'):\n",
        "            current_mask = label[2:]  # Extract entity type (PS, LC, OG, etc.)\n",
        "            masked_text.append(f'[MASK-{current_mask}]')\n",
        "        elif label.startswith('I-'):\n",
        "            if current_mask != label[2:]:\n",
        "                current_mask = label[2:]\n",
        "                masked_text.append(f'[MASK-{current_mask}]')\n",
        "        else:\n",
        "            if token.startswith(\"##\"):\n",
        "                masked_text[-1] += token[2:]\n",
        "            else:\n",
        "                masked_text.append(token)\n",
        "            current_mask = None\n",
        "\n",
        "    masked_text = \" \".join(masked_text)\n",
        "    masked_text = masked_text.replace(\" ##\", \"\").replace(\" .\", \".\").strip()\n",
        "    return masked_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"강감찬 장군은 고려시대 인물로 , 현재 서울시 관악구 봉천동 1234번지에 그의 동상이 있습니다 . 그의 생년은 948년으로 알려져 있습니다 .\"\n",
        "masked_text = detect_and_mask_pii(test_text, model, tokenizer, device)\n",
        "print(masked_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EtifjWcdcju",
        "outputId": "b73a9780-6853-4bd7-c6f4-cdc6972da4f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MASK-PS] 장군은 고려시대 인물로 , 현재 서울시 [MASK-LC] 1234번지에 그의 동상이 있습니다. 그의 생년은 [MASK-TI] [MASK-DT]으로 알려져 있습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"이순신의 전화번호는 010-3318-3994이다. 그의 주민등록번호는 000908-2133434이다.\"\n",
        "masked_text = detect_and_mask_pii(test_text, model, tokenizer, device)\n",
        "print(masked_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxI71Y5-dfcN",
        "outputId": "03131d87-d5c9-4c85-d607-0f81fb190d5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이순신의 전화번호는 010 - 3318 - 3994이다. 그의 주민등록번호는 000908 - 2133434이다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"2012년에 태어난 사촌 조카는 서울시 성동구에서 유치원을 다니고 있다. 조카는 우리은행 100-232344-433221 계좌를 사용한다.\"\n",
        "masked_text = detect_and_mask_pii(test_text, model, tokenizer, device)\n",
        "print(masked_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLBvTthvdixj",
        "outputId": "43ac9c61-fd48-4680-bdfe-63ef174dd40a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MASK-DT]에 태어난 사촌 조카는 서울시 [MASK-LC]에서 유치원을 다니고 있다. 조카는 우리은행 100 - 232344 - 433221 계좌를 사용한다.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}