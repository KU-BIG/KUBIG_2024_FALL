{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Btso2j6WYwQ"
      },
      "source": [
        "- ë³€ê²½ì‚¬í•­\n",
        "  - koELECTRA ëª¨ë¸ ì‚¬ìš©\n",
        "  - ëª¨ë¸ì— ë§ê²Œ ì½”ë“œ ìˆ˜ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "aqZK90HzWRkb",
        "outputId": "c9f959cd-83fd-4d0e-fe97-dee2708751c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3595' max='3595' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3595/3595 22:24, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.198600</td>\n",
              "      <td>0.180708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.136400</td>\n",
              "      <td>0.126268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.095600</td>\n",
              "      <td>0.119148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.088200</td>\n",
              "      <td>0.111439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.080100</td>\n",
              "      <td>0.110998</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='219' max='219' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [219/219 00:25]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed and model saved.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import torch\n",
        "from transformers import ElectraTokenizerFast, ElectraForTokenClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "\n",
        "def generate_fake_data(num_samples):\n",
        "    names = [\"í™ê¸¸ë™\", \"ê¹€ì² ìˆ˜\", \"ì´ì˜í¬\", \"ë°•ì§€ì„±\", \"ìµœë¯¼ìˆ˜\", \"ì •ìˆ˜ì—°\", \"ê°•íƒœí’\", \"ìœ¤ë¯¸ë˜\", \"ì†¡í•˜ëŠ˜\", \"ì¡°í•´ì„±\"]\n",
        "    places = [\"ì„œìš¸\", \"ë¶€ì‚°\", \"ëŒ€êµ¬\", \"ì¸ì²œ\", \"ê´‘ì£¼\", \"ëŒ€ì „\", \"ìš¸ì‚°\", \"ì„¸ì¢…\", \"ì œì£¼\", \"ìˆ˜ì›\", \"ì°½ì›\"]\n",
        "    organizations = [\"ì‚¼ì„±\", \"LG\", \"í˜„ëŒ€\", \"SK\", \"ë¡¯ë°\", \"ë„¤ì´ë²„\", \"ì¹´ì¹´ì˜¤\", \"ì¿ íŒ¡\", \"ë°°ë‹¬ì˜ë¯¼ì¡±\", \"í† ìŠ¤\"]\n",
        "    phone_prefixes = [\"010\", \"011\", \"016\", \"017\", \"018\", \"019\"]\n",
        "    rrn_prefixes = [\"990101\", \"850505\", \"770303\", \"880808\", \"920414\", \"660606\", \"950707\", \"001212\"]\n",
        "    bank_names = [\"êµ­ë¯¼ì€í–‰\", \"ì‹ í•œì€í–‰\", \"ìš°ë¦¬ì€í–‰\", \"í•˜ë‚˜ì€í–‰\", \"ê¸°ì—…ì€í–‰\", \"ë†í˜‘ì€í–‰\", \"ì¼€ì´ë±…í¬\", \"ì¹´ì¹´ì˜¤ë±…í¬\"]\n",
        "    job_titles = [\"ì‚¬ì›\", \"ëŒ€ë¦¬\", \"ê³¼ì¥\", \"ì°¨ì¥\", \"ë¶€ì¥\", \"ì´ì‚¬\", \"ìƒë¬´\", \"ì „ë¬´\", \"ëŒ€í‘œ\", \"ì¸í„´\"]\n",
        "    departments = [\"ì˜ì—…ë¶€\", \"ë§ˆì¼€íŒ…ë¶€\", \"ì¸ì‚¬ë¶€\", \"ì¬ë¬´ë¶€\", \"ê°œë°œë¶€\", \"ë””ìì¸ë¶€\", \"ê³ ê°ì„œë¹„ìŠ¤íŒ€\", \"ì—°êµ¬ì†Œ\"]\n",
        "\n",
        "    data = []\n",
        "    for _ in range(num_samples):\n",
        "        name = random.choice(names)\n",
        "        place = random.choice(places)\n",
        "        org = random.choice(organizations)\n",
        "        phone = f\"{random.choice(phone_prefixes)}-{random.randint(1000, 9999)}-{random.randint(1000, 9999)}\"\n",
        "        rrn = f\"{random.choice(rrn_prefixes)}-{random.randint(1000000, 9999999)}\"\n",
        "        bank = random.choice(bank_names)\n",
        "        account = f\"{random.randint(100, 999)}-{random.randint(10000, 99999)}-{random.randint(10000, 99999)}\"\n",
        "        job_title = random.choice(job_titles)\n",
        "        department = random.choice(departments)\n",
        "\n",
        "        # ë‹¤ì–‘í•œ êµ¬ì¡°ì˜ ë¬¸ì¥ ì¤‘ í•˜ë‚˜ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì—¬ ê°€ìƒì˜ ë°ì´í„°ë¥¼ ìƒì„±\n",
        "        structures = [\n",
        "            lambda: f\"{name}ì˜ ì¸ì ì‚¬í•­: ê±°ì£¼ì§€ {place}, ì—°ë½ì²˜ {phone}, ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ {rrn}. {org} {department} {job_title}ë¡œ ê·¼ë¬´ ì¤‘. {bank} ê³„ì¢Œë²ˆí˜¸: {account}\",\n",
        "            lambda: f\"{org} {department}ì˜ {job_title} {name}ì”¨ ì •ë³´ - ì „í™”ë²ˆí˜¸: {phone}, ì£¼ì†Œì§€: {place}, ì£¼ë¯¼ë²ˆí˜¸: {rrn}, {bank} ê³„ì¢Œ: {account}\",\n",
        "            lambda: f\"ì´ë¦„: {name}, ì§ì¥: {org} ({department}, {job_title}), ì—°ë½ì²˜: {phone}, ê±°ì£¼ì§€: {place}, ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸: {rrn}, ê¸‰ì—¬ê³„ì¢Œ: {bank} {account}\",\n",
        "            lambda: f\"{place}ì— ê±°ì£¼í•˜ëŠ” {name}({rrn})ì”¨ëŠ” {org} {department}ì—ì„œ {job_title}ìœ¼ë¡œ ì¼í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì—°ë½ì²˜ëŠ” {phone}ì´ë©°, {bank} ê³„ì¢Œ({account})ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\",\n",
        "            lambda: f\"{bank} ê³ ê°ì •ë³´ - ì„±ëª…: {name}, ê³„ì¢Œë²ˆí˜¸: {account}, ì „í™”ë²ˆí˜¸: {phone}, ì§ì¥: {org} {department} ({job_title}), ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸: {rrn}, ì£¼ì†Œ: {place}\",\n",
        "            lambda: f\"[ì¸ì‚¬ê¸°ë¡] ì§ì›ëª…: {name}, ë¶€ì„œ: {department}, ì§ìœ„: {job_title}, ì£¼ë¯¼ë²ˆí˜¸: {rrn}, ì—°ë½ì²˜: {phone}, ì£¼ì†Œ: {place}, ê¸‰ì—¬ê³„ì¢Œ: {bank} {account}\",\n",
        "            lambda: f\"{org} {department} {name} {job_title}ë‹˜ì˜ ì—°ë½ì²˜ëŠ” {phone}ì…ë‹ˆë‹¤. ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ {rrn}, {place} ê±°ì£¼, {bank} ê³„ì¢Œë²ˆí˜¸ {account}.\",\n",
        "            lambda: f\"ì„±ëª… {name} (ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸: {rrn}) / ê·¼ë¬´ì²˜: {org} {department} / ì§ìœ„: {job_title} / ì£¼ì†Œ: {place} / ì—°ë½ì²˜: {phone} / ê³„ì¢Œ: {bank} {account}\",\n",
        "            lambda: f\"{place}ì˜ {name}ì”¨ëŠ” {org} {department}ì—ì„œ {job_title}ìœ¼ë¡œ ì¼í•©ë‹ˆë‹¤. ì—°ë½ì²˜ {phone}, ì£¼ë¯¼ë²ˆí˜¸ {rrn}, {bank} ê³„ì¢Œ {account}ë¥¼ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤.\",\n",
        "            lambda: f\"ì§ì›ì •ë³´: {name} / {org} / {department} / {job_title} / {place} ê±°ì£¼ / {phone} / {rrn} / {bank} {account}\",\n",
        "            lambda: f\"{bank} {account} ê³„ì¢Œì˜ ì˜ˆê¸ˆì£¼ {name}ë‹˜ ì •ë³´ - ì£¼ì†Œ: {place}, ì „í™”: {phone}, ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸: {rrn}, ì§ì¥: {org} {department} {job_title}\",\n",
        "            lambda: f\"{name}ì”¨ëŠ” {org}ì˜ {job_title}ì…ë‹ˆë‹¤. {department} ì†Œì†ì´ë©°, {place}ì— ì‚½ë‹ˆë‹¤. ì—°ë½ì²˜ëŠ” {phone}, ì£¼ë¯¼ë²ˆí˜¸ëŠ” {rrn}, {bank} ê³„ì¢ŒëŠ” {account}ì…ë‹ˆë‹¤.\",\n",
        "            lambda: f\"[{org} ì§ì› ë°ì´í„°] ì´ë¦„: {name}, ë¶€ì„œ: {department}, ì§ì±…: {job_title}, ì „í™”ë²ˆí˜¸: {phone}, ì£¼ì†Œ: {place}, ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸: {rrn}, ê¸‰ì—¬ê³„ì¢Œ: {bank} {account}\",\n",
        "            lambda: f\"ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ {rrn}ì˜ {name}ë‹˜ì€ {org} {department} {job_title}ì…ë‹ˆë‹¤. {place}ì— ê±°ì£¼ ì¤‘ì´ë©°, ì—°ë½ì²˜ëŠ” {phone}, {bank} ê³„ì¢Œë²ˆí˜¸ëŠ” {account}ì…ë‹ˆë‹¤.\",\n",
        "            lambda: f\"{place} ì£¼ë¯¼ {name}ì˜ ì‹ ìƒì •ë³´ - ì§ì¥: {org} {department}, ì§ìœ„: {job_title}, ì „í™”ë²ˆí˜¸: {phone}, ì£¼ë¯¼ë²ˆí˜¸: {rrn}, ê³„ì¢Œì •ë³´: {bank} {account}\",\n",
        "            lambda: f\"{org} {department}ì˜ {name} {job_title}ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤. ì£¼ì†Œ: {place}, ì „í™”: {phone}, ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸: {rrn}, {bank} ê³„ì¢Œë²ˆí˜¸: {account}\",\n",
        "            lambda: f\"ì´ë¦„: {name} / ì£¼ë¯¼ë²ˆí˜¸: {rrn} / ì£¼ì†Œ: {place} / ì§ì¥: {org} / ë¶€ì„œ: {department} / ì§ìœ„: {job_title} / ì—°ë½ì²˜: {phone} / ê³„ì¢Œ: {bank} {account}\",\n",
        "            lambda: f\"{bank} ê³ ê° {name}ë‹˜ ({rrn})ì˜ ê³„ì¢Œë²ˆí˜¸ëŠ” {account}ì…ë‹ˆë‹¤. {org} {department} {job_title}ì´ë©°, {place}ì— ê±°ì£¼í•©ë‹ˆë‹¤. ì—°ë½ì²˜: {phone}\",\n",
        "            lambda: f\"{name} ({org} {department}, {job_title}) - ì£¼ì†Œ: {place}, ì „í™”ë²ˆí˜¸: {phone}, ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸: {rrn}, {bank} ê³„ì¢Œ: {account}\",\n",
        "            lambda: f\"[ê°œì¸ì •ë³´] ì„±ëª…: {name}, ì£¼ë¯¼ë²ˆí˜¸: {rrn}, ê·¼ë¬´ì§€: {org} {department}, ì§ì±…: {job_title}, ê±°ì£¼ì§€: {place}, ì—°ë½ì²˜: {phone}, ê³„ì¢Œ: {bank} {account}\"\n",
        "        ]\n",
        "\n",
        "        sample = random.choice(structures)()\n",
        "        data.append(sample)\n",
        "    return data\n",
        "\n",
        "\n",
        "def tokenize_and_label(text):\n",
        "    tokens = []\n",
        "    labels = []\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        if \"-\" in word:\n",
        "            parts = word.split(\"-\")\n",
        "            if len(parts) == 3 and parts[0] in [\"010\", \"011\", \"016\", \"017\", \"018\", \"019\"]:\n",
        "                tokens.extend(parts)\n",
        "                labels.extend([\"B-PH\", \"I-PH\", \"I-PH\"])\n",
        "            elif len(parts) == 2 and len(parts[0]) == 6 and parts[0].isdigit():\n",
        "                tokens.extend(parts)\n",
        "                labels.extend([\"B-RRN\", \"I-RRN\"])\n",
        "            elif len(parts) == 3 and len(parts[0]) == 3 and all(part.isdigit() for part in parts):\n",
        "                tokens.extend(parts)\n",
        "                labels.extend([\"B-ACC\", \"I-ACC\", \"I-ACC\"])\n",
        "            else:\n",
        "                tokens.append(word)\n",
        "                labels.append(\"O\")  # ê°œì¸ ì •ë³´ê°€ ì•„ë‹Œ ê²½ìš° 'O' ë¼ë²¨ ë¶€ì—¬\n",
        "        else:\n",
        "            tokens.append(word)\n",
        "            labels.append(\"O\")\n",
        "    return tokens, labels\n",
        "\n",
        "def load_data_with_new_labels(file_path, new_data):\n",
        "    texts, labels = [], []\n",
        "    label_map = {\n",
        "        'O': 0, 'B-PS': 1, 'I-PS': 2, 'B-LC': 3, 'I-LC': 4,\n",
        "        'B-OG': 5, 'I-OG': 6, 'B-QT': 7, 'I-QT': 8, 'B-DT': 9,\n",
        "        'I-DT': 10, 'B-TI': 11, 'I-TI': 12, 'B-LT': 13, 'I-LT': 14,\n",
        "        'B-PO': 15, 'I-PO': 16, 'B-PH': 17, 'I-PH': 18, 'B-RRN': 19, 'I-RRN': 20,\n",
        "        'B-ACC': 21, 'I-ACC': 22  # ìƒˆë¡œ ì¶”ê°€ëœ ê³„ì¢Œë²ˆí˜¸ ë¼ë²¨\n",
        "    }\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text, label = [], []\n",
        "        for line in file:\n",
        "            if line.startswith('##') or not line.strip():\n",
        "                if text:\n",
        "                    texts.append(\" \".join(text))\n",
        "                    labels.append([label_map[l] for l in label])\n",
        "                    text, label = [], []\n",
        "                continue\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) != 2:\n",
        "                continue\n",
        "            char, tag = parts\n",
        "            text.append(char)\n",
        "            label.append(tag)\n",
        "\n",
        "    # ìƒˆë¡œ ìƒì„±í•œ ê°€ìƒ ë°ì´í„°ì…‹ ì¶”ê°€\n",
        "    for sample in new_data:\n",
        "        tokens, tags = tokenize_and_label(sample)\n",
        "        texts.append(\" \".join(tokens))\n",
        "        labels.append([label_map[tag] for tag in tags])\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "# ê°€ìƒ ë°ì´í„°ì…‹ ìƒì„±\n",
        "fake_data = generate_fake_data(2000)\n",
        "# ê¸°ì¡´ ë°ì´í„°ì— ê°€ìƒ ë°ì´í„°ì…‹ì„ ì¶”ê°€í•˜ì—¬ ë¡œë“œ\n",
        "train_texts, train_labels = load_data_with_new_labels('klue-ner-v1.1_train (2).tsv', fake_data)\n",
        "dev_texts, dev_labels = load_data_with_new_labels('klue-ner-v1.1_dev (3).tsv', fake_data)\n",
        "\n",
        "# í•™ìŠµ ë°ì´í„°ì…‹ì„ ì •ì˜í•˜ëŠ” í´ë˜ìŠ¤\n",
        "class PIIDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = str(self.texts[item])\n",
        "        labels = self.labels[item]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text.split(),  # ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ í† í¬ë‚˜ì´ì§•\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            is_split_into_words=True,  # ë‹¨ì–´ ë‹¨ìœ„ë¡œ í† í¬ë‚˜ì´ì§•\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        input_ids = encoding['input_ids'].squeeze()\n",
        "        attention_mask = encoding['attention_mask'].squeeze()\n",
        "\n",
        "        aligned_labels = [-100] * len(input_ids)  # íŒ¨ë”©ëœ í† í°ì€ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹\n",
        "        word_ids = encoding.word_ids()  # ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ ìœ„ì¹˜\n",
        "\n",
        "        for i, word_id in enumerate(word_ids):\n",
        "            if word_id is None:\n",
        "                continue\n",
        "            aligned_labels[i] = labels[word_id]  # ì›ë³¸ ë‹¨ì–´ì˜ ë¼ë²¨ì„ í† í°ì— ë§¤í•‘\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': torch.tensor(aligned_labels, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "# \"Fast\" ë²„ì „ì˜ í† í¬ë‚˜ì´ì €ë¡œ ë³€ê²½\n",
        "tokenizer = ElectraTokenizerFast.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
        "model = ElectraForTokenClassification.from_pretrained('monologg/koelectra-base-v3-discriminator', num_labels=23)\n",
        "\n",
        "max_len = 128\n",
        "\n",
        "train_dataset = PIIDataset(train_texts, train_labels, tokenizer, max_len)\n",
        "dev_dataset = PIIDataset(dev_texts, dev_labels, tokenizer, max_len)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False\n",
        ")\n",
        "\n",
        "# Early Stopping ì„¤ì • (ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨)\n",
        "early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,\n",
        "    early_stopping_threshold=0.0\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        "    callbacks=[early_stopping_callback]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()\n",
        "\n",
        "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì €ì¥\n",
        "model.save_pretrained('./pii_masking_model')\n",
        "tokenizer.save_pretrained('./pii_masking_model')\n",
        "\n",
        "print(\"Training completed and model saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BMwuqujhW7iF"
      },
      "outputs": [],
      "source": [
        "def detect_and_mask_pii(text, model, tokenizer, device):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    predictions = torch.argmax(outputs.logits, dim=2)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "\n",
        "    label_map = {\n",
        "        0: 'O', 1: 'B-PS', 2: 'I-PS', 3: 'B-LC', 4: 'I-LC',\n",
        "        5: 'B-OG', 6: 'I-OG', 7: 'B-QT', 8: 'I-QT', 9: 'B-DT',\n",
        "        10: 'I-DT', 11: 'B-TI', 12: 'I-TI', 13: 'B-LT', 14: 'I-LT',\n",
        "        15: 'B-PO', 16: 'I-PO', 17: 'B-PH', 18: 'I-PH', 19: 'B-RRN', 20: 'I-RRN',\n",
        "        21: 'B-ACC', 22: 'I-ACC'  # Added new labels for account numbers\n",
        "    }\n",
        "\n",
        "    masked_text = []\n",
        "    current_mask = None\n",
        "    for token, pred in zip(tokens, predictions[0]):\n",
        "        if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
        "            continue\n",
        "        label = label_map[pred.item()]\n",
        "        if label.startswith('B-'):\n",
        "            current_mask = label[2:]  # Extract entity type (PS, LC, OG, etc.)\n",
        "            masked_text.append(f'[MASK-{current_mask}]')\n",
        "        elif label.startswith('I-'):\n",
        "            if current_mask != label[2:]:\n",
        "                current_mask = label[2:]\n",
        "                masked_text.append(f'[MASK-{current_mask}]')\n",
        "        else:\n",
        "            if token.startswith(\"##\"):\n",
        "                masked_text[-1] += token[2:]\n",
        "            else:\n",
        "                masked_text.append(token)\n",
        "            current_mask = None\n",
        "\n",
        "    masked_text = \" \".join(masked_text)\n",
        "    masked_text = masked_text.replace(\" ##\", \"\").replace(\" .\", \".\").strip()\n",
        "    return masked_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"ê°•ê°ì°¬ ì¥êµ°ì€ ê³ ë ¤ì‹œëŒ€ ì¸ë¬¼ë¡œ , í˜„ì¬ ì„œìš¸ì‹œ ê´€ì•…êµ¬ ë´‰ì²œë™ 1234ë²ˆì§€ì— ê·¸ì˜ ë™ìƒì´ ìˆìŠµë‹ˆë‹¤ . ê·¸ì˜ ìƒë…„ì€ 948ë…„ìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤ .\"\n",
        "masked_text = detect_and_mask_pii(test_text, model, tokenizer, device)\n",
        "print(masked_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EtifjWcdcju",
        "outputId": "b73a9780-6853-4bd7-c6f4-cdc6972da4f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MASK-PS] ì¥êµ°ì€ ê³ ë ¤ì‹œëŒ€ ì¸ë¬¼ë¡œ , í˜„ì¬ ì„œìš¸ì‹œ [MASK-LC] 1234ë²ˆì§€ì— ê·¸ì˜ ë™ìƒì´ ìˆìŠµë‹ˆë‹¤. ê·¸ì˜ ìƒë…„ì€ [MASK-TI] [MASK-DT]ìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"ì´ìˆœì‹ ì˜ ì „í™”ë²ˆí˜¸ëŠ” 010-3318-3994ì´ë‹¤. ê·¸ì˜ ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ëŠ” 000908-2133434ì´ë‹¤.\"\n",
        "masked_text = detect_and_mask_pii(test_text, model, tokenizer, device)\n",
        "print(masked_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxI71Y5-dfcN",
        "outputId": "03131d87-d5c9-4c85-d607-0f81fb190d5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì´ìˆœì‹ ì˜ ì „í™”ë²ˆí˜¸ëŠ” 010 - 3318 - 3994ì´ë‹¤. ê·¸ì˜ ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ëŠ” 000908 - 2133434ì´ë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \"2012ë…„ì— íƒœì–´ë‚œ ì‚¬ì´Œ ì¡°ì¹´ëŠ” ì„œìš¸ì‹œ ì„±ë™êµ¬ì—ì„œ ìœ ì¹˜ì›ì„ ë‹¤ë‹ˆê³  ìˆë‹¤. ì¡°ì¹´ëŠ” ìš°ë¦¬ì€í–‰ 100-232344-433221 ê³„ì¢Œë¥¼ ì‚¬ìš©í•œë‹¤.\"\n",
        "masked_text = detect_and_mask_pii(test_text, model, tokenizer, device)\n",
        "print(masked_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLBvTthvdixj",
        "outputId": "43ac9c61-fd48-4680-bdfe-63ef174dd40a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MASK-DT]ì— íƒœì–´ë‚œ ì‚¬ì´Œ ì¡°ì¹´ëŠ” ì„œìš¸ì‹œ [MASK-LC]ì—ì„œ ìœ ì¹˜ì›ì„ ë‹¤ë‹ˆê³  ìˆë‹¤. ì¡°ì¹´ëŠ” ìš°ë¦¬ì€í–‰ 100 - 232344 - 433221 ê³„ì¢Œë¥¼ ì‚¬ìš©í•œë‹¤.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}