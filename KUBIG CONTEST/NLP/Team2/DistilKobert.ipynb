{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install transformers torch torchcrf"
      ],
      "metadata": {
        "id": "WGH380y2AdQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchcrf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j0V_O50Ayzo",
        "outputId": "64b6092d-0160-4fa8-9c15-a8b00616588f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchcrf in /usr/local/lib/python3.10/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchcrf) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from torchcrf) (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->torchcrf) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->torchcrf) (12.6.20)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->torchcrf) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->torchcrf) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch-crf==0.4.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl-cePU5A_ZV",
        "outputId": "dc6a4e62-b2a4-4e8a-b998-f21765b09bd5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-crf==0.4.0\n",
            "  Downloading pytorch_crf-0.4.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Downloading pytorch_crf-0.4.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertTokenizerFast, DistilBertPreTrainedModel, DistilBertModel, DistilBertConfig\n",
        "from torchcrf import CRF\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 개체 및 태그 정의\n",
        "ENTITY_TAGS = {\n",
        "    \"PER\": [\"B-PER\", \"I-PER\"],\n",
        "    \"ORG\": [\"B-ORG\", \"I-ORG\"],\n",
        "    \"EDU\": [\"B-EDU\", \"I-EDU\"],\n",
        "    \"AFF\": [\"B-AFF\", \"I-AFF\"],\n",
        "    \"POS\": [\"B-POS\", \"I-POS\"],\n",
        "    \"LOC\": [\"B-LOC\", \"I-LOC\"],\n",
        "    \"DUR\": [\"B-DUR\", \"I-DUR\"]\n",
        "}\n",
        "\n",
        "class EnhancedNERModel(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.distilbert = DistilBertModel(config)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.sentence_classifier = nn.Linear(config.dim, 2)  # 문장 분류 (민감/비민감)\n",
        "        self.token_classifier = nn.Linear(config.dim * 2, config.num_labels)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        labels=None,\n",
        "        sentence_labels=None,\n",
        "    ):\n",
        "        outputs = self.distilbert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = sequence_output[:, 0]  # [CLS] 토큰의 출력\n",
        "\n",
        "        # 문장 수준 분류\n",
        "        sentence_logits = self.sentence_classifier(pooled_output)\n",
        "        sentence_loss = None\n",
        "        if sentence_labels is not None:\n",
        "            sentence_loss_fct = nn.CrossEntropyLoss()\n",
        "            sentence_loss = sentence_loss_fct(sentence_logits.view(-1, 2), sentence_labels.view(-1))\n",
        "\n",
        "        # 문장 수준 정보를 토큰 수준 정보와 결합\n",
        "        expanded_pooled_output = pooled_output.unsqueeze(1).expand(-1, sequence_output.size(1), -1)\n",
        "        combined_output = torch.cat([sequence_output, expanded_pooled_output], dim=-1)\n",
        "\n",
        "        token_logits = self.token_classifier(self.dropout(combined_output))\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            token_loss = loss_fct(token_logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "            loss = token_loss\n",
        "            if sentence_loss is not None:\n",
        "                loss += sentence_loss\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"token_logits\": token_logits,\n",
        "            \"sentence_logits\": sentence_logits,\n",
        "        }\n",
        "\n",
        "    def decode(self, token_logits, attention_mask):\n",
        "        return torch.argmax(token_logits, dim=-1)\n",
        "\n",
        "class EnhancedNERDataset(Dataset):\n",
        "    def __init__(self, texts, tags, sentence_labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.tags = tags\n",
        "        self.sentence_labels = sentence_labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.label_dict = {\"O\": 0}\n",
        "        for entity, entity_tags in ENTITY_TAGS.items():\n",
        "            for tag in entity_tags:\n",
        "                if tag not in self.label_dict:\n",
        "                    self.label_dict[tag] = len(self.label_dict)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        tags = self.tags[idx]\n",
        "        sentence_label = self.sentence_labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        # 첫 번째 토큰([CLS])의 마스크를 1로 설정\n",
        "        attention_mask[0] = 1\n",
        "\n",
        "        # [CLS]와 [SEP] 토큰을 포함한 라벨 생성\n",
        "        labels = torch.tensor([-100] + [self.label_dict.get(tag, 0) for tag in tags] + [-100])\n",
        "\n",
        "        # 패딩\n",
        "        if len(labels) < self.max_len:\n",
        "            labels = torch.cat([labels, torch.tensor([-100] * (self.max_len - len(labels)))])\n",
        "        else:\n",
        "            labels = labels[:self.max_len]\n",
        "\n",
        "        # 첫 번째 토큰([CLS])의 라벨을 0으로 설정\n",
        "        labels[0] = 0\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"sentence_labels\": torch.tensor(sentence_label),\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        sentence_labels = batch[\"sentence_labels\"].to(device)\n",
        "\n",
        "        # 마스크의 첫 번째 요소를 True로 설정\n",
        "        attention_mask[:, 0] = 1\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels, sentence_labels=sentence_labels)\n",
        "        loss = outputs[\"loss\"]\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            sentence_labels = batch[\"sentence_labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels, sentence_labels=sentence_labels)\n",
        "            total_loss += outputs[\"loss\"].item()\n",
        "    return total_loss / len(test_loader)\n",
        "\n",
        "\n",
        "def identify_entities(text, model, tokenizer, device, label_dict):\n",
        "    encoding = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        token_logits = outputs[\"token_logits\"]\n",
        "        sentence_logits = outputs[\"sentence_logits\"]\n",
        "\n",
        "    token_predictions = model.decode(token_logits, attention_mask)\n",
        "    sentence_prediction = torch.argmax(sentence_logits, dim=1).item()\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "    ner_labels = [list(label_dict.keys())[pred] for pred in token_predictions[0]]\n",
        "\n",
        "    entities = []\n",
        "    current_entity = None\n",
        "    for token, label in zip(tokens, ner_labels):\n",
        "        if label.startswith(\"B-\"):\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "            current_entity = {\"type\": label[2:], \"text\": token}\n",
        "        elif label.startswith(\"I-\") and current_entity and current_entity[\"type\"] == label[2:]:\n",
        "            current_entity[\"text\"] += \" \" + token\n",
        "        else:\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = None\n",
        "\n",
        "    if current_entity:\n",
        "        entities.append(current_entity)\n",
        "\n",
        "    sentence_sensitivity = \"민감\" if sentence_prediction == 1 else \"비민감\"\n",
        "\n",
        "    return entities, sentence_sensitivity\n",
        "\n",
        "# 메인 실행 부분\n",
        "if __name__ == \"__main__\":\n",
        "    # 데이터 준비 (예시)\n",
        "    texts = [\n",
        "        \"김철수씨는 서울대학교 컴퓨터공학과를 졸업하고 현재 구글 코리아에서 소프트웨어 엔지니어로 2년째 근무 중입니다.\",\n",
        "        \"이회사 대표이사 박영희는 연세대학교 경영학과 출신으로 알려져 있습니다.\",\n",
        "        \"저는 한국대학교 물리학과에 재학 중인 학생입니다.\"\n",
        "    ]\n",
        "    tags = [\n",
        "        [\"B-PER\", \"I-PER\", \"O\", \"B-ORG\", \"I-ORG\", \"B-EDU\", \"I-EDU\", \"O\", \"O\", \"O\", \"B-ORG\", \"I-ORG\", \"O\", \"B-POS\", \"I-POS\", \"O\", \"B-DUR\", \"I-DUR\", \"O\", \"O\", \"O\"],\n",
        "        [\"O\", \"B-POS\", \"B-PER\", \"I-PER\", \"O\", \"B-ORG\", \"I-ORG\", \"B-EDU\", \"I-EDU\", \"O\", \"O\", \"O\", \"O\"],\n",
        "        [\"O\", \"B-ORG\", \"I-ORG\", \"B-EDU\", \"I-EDU\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
        "    ]\n",
        "    sentence_labels = [1, 1, 0]  # 1: 민감, 0: 비민감\n",
        "\n",
        "    # 데이터 분할\n",
        "    train_texts, test_texts, train_tags, test_tags, train_sentence_labels, test_sentence_labels = train_test_split(\n",
        "        texts, tags, sentence_labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # 토크나이저 및 모델 초기화\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"monologg/distilkobert\")\n",
        "    config = DistilBertConfig.from_pretrained(\"monologg/distilkobert\")\n",
        "    config.num_labels = len(ENTITY_TAGS) * 2 + 1  # B, I for each entity + O\n",
        "\n",
        "    # 데이터셋 및 데이터로더 생성\n",
        "    train_dataset = EnhancedNERDataset(train_texts, train_tags, train_sentence_labels, tokenizer)\n",
        "    test_dataset = EnhancedNERDataset(test_texts, test_tags, test_sentence_labels, tokenizer)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=2)\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = EnhancedNERModel(config)\n",
        "\n",
        "    # 학습 설정\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    # 학습\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train(model, train_loader, optimizer, device)\n",
        "        eval_loss = evaluate(model, test_loader, device)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}\")\n",
        "\n",
        "    # 모델 저장\n",
        "    torch.save(model.state_dict(), \"enhanced_ner_model.pth\")\n",
        "\n",
        "    # 추론 예시\n",
        "    test_text = \"김영희는 고려대학교 경제학과를 졸업하고 현재 삼성전자에서 마케팅 팀장으로 일하고 있습니다.\"\n",
        "    entities, sentence_sensitivity = identify_entities(test_text, model, tokenizer, device, train_dataset.label_dict)\n",
        "\n",
        "    print(\"\\n추론 결과:\")\n",
        "    print(\"입력 텍스트:\", test_text)\n",
        "    print(\"식별된 개체:\")\n",
        "    for entity in entities:\n",
        "        print(f\"- {entity['text']} ({entity['type']})\")\n",
        "    print(f\"문장 민감도: {sentence_sensitivity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-WFy3YfAZUE",
        "outputId": "8fdebfad-3172-406c-b948-cbe73c1fce41"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 3.5570, Eval Loss: 2.9723\n",
            "Epoch 2/10, Train Loss: 2.2772, Eval Loss: 3.4505\n",
            "Epoch 3/10, Train Loss: 2.3759, Eval Loss: 3.2653\n",
            "Epoch 4/10, Train Loss: 2.1237, Eval Loss: 2.8385\n",
            "Epoch 5/10, Train Loss: 1.6698, Eval Loss: 2.6309\n",
            "Epoch 6/10, Train Loss: 1.4479, Eval Loss: 2.7175\n",
            "Epoch 7/10, Train Loss: 1.4055, Eval Loss: 2.8179\n",
            "Epoch 8/10, Train Loss: 1.4852, Eval Loss: 2.9339\n",
            "Epoch 9/10, Train Loss: 1.1470, Eval Loss: 3.0918\n",
            "Epoch 10/10, Train Loss: 1.0359, Eval Loss: 3.1265\n",
            "\n",
            "추론 결과:\n",
            "입력 텍스트: 김영희는 고려대학교 경제학과를 졸업하고 현재 삼성전자에서 마케팅 팀장으로 일하고 있습니다.\n",
            "식별된 개체:\n",
            "- [UNK] (ORG)\n",
            "- [UNK] (PER)\n",
            "- [UNK] (EDU)\n",
            "- [UNK] (ORG)\n",
            "- [UNK] (EDU)\n",
            "문장 민감도: 비민감\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertTokenizerFast, DistilBertPreTrainedModel, DistilBertModel, DistilBertConfig\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 개체 및 태그 정의\n",
        "ENTITY_TAGS = {\n",
        "    \"PER\": [\"B-PER\", \"I-PER\"],\n",
        "    \"ORG\": [\"B-ORG\", \"I-ORG\"],\n",
        "    \"EDU\": [\"B-EDU\", \"I-EDU\"],\n",
        "    \"AFF\": [\"B-AFF\", \"I-AFF\"],\n",
        "    \"POS\": [\"B-POS\", \"I-POS\"],\n",
        "    \"LOC\": [\"B-LOC\", \"I-LOC\"],\n",
        "    \"DUR\": [\"B-DUR\", \"I-DUR\"]\n",
        "}\n",
        "\n",
        "class EnhancedNERModel(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.distilbert = DistilBertModel(config)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.sentence_classifier = nn.Linear(config.dim, 2)  # 문장 분류 (민감/비민감)\n",
        "        self.token_classifier = nn.Linear(config.dim * 2, config.num_labels)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        labels=None,\n",
        "        sentence_labels=None,\n",
        "    ):\n",
        "        outputs = self.distilbert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = sequence_output[:, 0]  # [CLS] 토큰의 출력\n",
        "\n",
        "        # 문장 수준 분류\n",
        "        sentence_logits = self.sentence_classifier(pooled_output)\n",
        "        sentence_loss = None\n",
        "        if sentence_labels is not None:\n",
        "            sentence_loss_fct = nn.CrossEntropyLoss()\n",
        "            sentence_loss = sentence_loss_fct(sentence_logits.view(-1, 2), sentence_labels.view(-1))\n",
        "\n",
        "        # 문장 수준 정보를 토큰 수준 정보와 결합\n",
        "        expanded_pooled_output = pooled_output.unsqueeze(1).expand(-1, sequence_output.size(1), -1)\n",
        "        combined_output = torch.cat([sequence_output, expanded_pooled_output], dim=-1)\n",
        "\n",
        "        token_logits = self.token_classifier(self.dropout(combined_output))\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            active_loss = attention_mask.view(-1) == 1\n",
        "            active_logits = token_logits.view(-1, self.num_labels)\n",
        "            active_labels = torch.where(\n",
        "                active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "            )\n",
        "            loss = loss_fct(active_logits, active_labels)\n",
        "            if sentence_loss is not None:\n",
        "                loss += sentence_loss\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"token_logits\": token_logits,\n",
        "            \"sentence_logits\": sentence_logits,\n",
        "        }\n",
        "\n",
        "    def decode(self, token_logits, attention_mask):\n",
        "        predictions = torch.argmax(token_logits, dim=-1)\n",
        "        return predictions\n",
        "\n",
        "\n",
        "class EnhancedNERDataset(Dataset):\n",
        "    def __init__(self, texts, tags, sentence_labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.tags = tags\n",
        "        self.sentence_labels = sentence_labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.label_dict = {\"O\": 0}\n",
        "        for entity, entity_tags in ENTITY_TAGS.items():\n",
        "            for tag in entity_tags:\n",
        "                if tag not in self.label_dict:\n",
        "                    self.label_dict[tag] = len(self.label_dict)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        tags = self.tags[idx]\n",
        "        sentence_label = self.sentence_labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        # 첫 번째 토큰([CLS])의 마스크를 1로 설정\n",
        "        attention_mask[0] = 1\n",
        "\n",
        "        # [CLS]와 [SEP] 토큰을 포함한 라벨 생성\n",
        "        labels = torch.tensor([-100] + [self.label_dict.get(tag, 0) for tag in tags] + [-100])\n",
        "\n",
        "        # 패딩\n",
        "        if len(labels) < self.max_len:\n",
        "            labels = torch.cat([labels, torch.tensor([-100] * (self.max_len - len(labels)))])\n",
        "        else:\n",
        "            labels = labels[:self.max_len]\n",
        "\n",
        "        # 첫 번째 토큰([CLS])의 라벨을 0으로 설정\n",
        "        labels[0] = 0\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"sentence_labels\": torch.tensor(sentence_label),\n",
        "        }\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        sentence_labels = batch[\"sentence_labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels, sentence_labels=sentence_labels)\n",
        "        loss = outputs[\"loss\"]\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            sentence_labels = batch[\"sentence_labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels, sentence_labels=sentence_labels)\n",
        "            total_loss += outputs[\"loss\"].item()\n",
        "    return total_loss / len(test_loader)\n",
        "\n",
        "\n",
        "def identify_entities(text, model, tokenizer, device, label_dict):\n",
        "    encoding = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        token_logits = outputs[\"token_logits\"]\n",
        "        sentence_logits = outputs[\"sentence_logits\"]\n",
        "\n",
        "    token_predictions = model.decode(token_logits, attention_mask)\n",
        "    sentence_prediction = torch.argmax(sentence_logits, dim=1).item()\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "    ner_labels = [list(label_dict.keys())[pred] for pred in token_predictions[0]]\n",
        "\n",
        "    entities = []\n",
        "    current_entity = None\n",
        "    for token, label in zip(tokens, ner_labels):\n",
        "        if label.startswith(\"B-\"):\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "            current_entity = {\"type\": label[2:], \"text\": token}\n",
        "        elif label.startswith(\"I-\") and current_entity and current_entity[\"type\"] == label[2:]:\n",
        "            current_entity[\"text\"] += \" \" + token\n",
        "        else:\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = None\n",
        "\n",
        "    if current_entity:\n",
        "        entities.append(current_entity)\n",
        "\n",
        "    sentence_sensitivity = \"민감\" if sentence_prediction == 1 else \"비민감\"\n",
        "\n",
        "    return entities, sentence_sensitivity\n",
        "\n",
        "# 메인 실행 부분\n",
        "if __name__ == \"__main__\":\n",
        "    # 데이터 준비 (예시)\n",
        "    texts = [\n",
        "        \"김철수씨는 서울대학교 컴퓨터공학과를 졸업하고 현재 구글 코리아에서 소프트웨어 엔지니어로 2년째 근무 중입니다.\",\n",
        "        \"이회사 대표이사 박영희는 연세대학교 경영학과 출신으로 알려져 있습니다.\",\n",
        "        \"저는 한국대학교 물리학과에 재학 중인 학생입니다.\"\n",
        "    ]\n",
        "    tags = [\n",
        "        [\"B-PER\", \"I-PER\", \"O\", \"B-ORG\", \"I-ORG\", \"B-EDU\", \"I-EDU\", \"O\", \"O\", \"O\", \"B-ORG\", \"I-ORG\", \"O\", \"B-POS\", \"I-POS\", \"O\", \"B-DUR\", \"I-DUR\", \"O\", \"O\", \"O\"],\n",
        "        [\"O\", \"B-POS\", \"B-PER\", \"I-PER\", \"O\", \"B-ORG\", \"I-ORG\", \"B-EDU\", \"I-EDU\", \"O\", \"O\", \"O\", \"O\"],\n",
        "        [\"O\", \"B-ORG\", \"I-ORG\", \"B-EDU\", \"I-EDU\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
        "    ]\n",
        "    sentence_labels = [1, 1, 0]  # 1: 민감, 0: 비민감\n",
        "\n",
        "    # 데이터 분할\n",
        "    train_texts, test_texts, train_tags, test_tags, train_sentence_labels, test_sentence_labels = train_test_split(\n",
        "        texts, tags, sentence_labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # 토크나이저 및 모델 초기화\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"monologg/distilkobert\")\n",
        "    config = DistilBertConfig.from_pretrained(\"monologg/distilkobert\")\n",
        "    config.num_labels = len(ENTITY_TAGS) * 2 + 1  # B, I for each entity + O\n",
        "\n",
        "    # 데이터셋 및 데이터로더 생성\n",
        "    train_dataset = EnhancedNERDataset(train_texts, train_tags, train_sentence_labels, tokenizer)\n",
        "    test_dataset = EnhancedNERDataset(test_texts, test_tags, test_sentence_labels, tokenizer)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=2)\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = EnhancedNERModel(config)\n",
        "\n",
        "    # 학습 설정\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    # 학습\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train(model, train_loader, optimizer, device)\n",
        "        eval_loss = evaluate(model, test_loader, device)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}\")\n",
        "\n",
        "    # 모델 저장\n",
        "    torch.save(model.state_dict(), \"enhanced_ner_model.pth\")\n",
        "\n",
        "    # 추론 예시\n",
        "    test_text = \"김영희는 고려대학교 경제학과를 졸업하고 현재 삼성전자에서 마케팅 팀장으로 일하고 있습니다.\"\n",
        "    entities, sentence_sensitivity = identify_entities(test_text, model, tokenizer, device, train_dataset.label_dict)\n",
        "\n",
        "    print(\"\\n추론 결과:\")\n",
        "    print(\"입력 텍스트:\", test_text)\n",
        "    print(\"식별된 개체:\")\n",
        "    for entity in entities:\n",
        "        print(f\"- {entity['text']} ({entity['type']})\")\n",
        "    print(f\"문장 민감도: {sentence_sensitivity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjfKOejoH2cf",
        "outputId": "d468ae93-f567-4f8f-d22b-d52f15ca4875"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 3.6476, Eval Loss: 2.7678\n",
            "Epoch 2/10, Train Loss: 2.9260, Eval Loss: 2.8414\n",
            "Epoch 3/10, Train Loss: 2.5470, Eval Loss: 2.5781\n",
            "Epoch 4/10, Train Loss: 2.0865, Eval Loss: 2.4768\n",
            "Epoch 5/10, Train Loss: 1.5965, Eval Loss: 2.5025\n",
            "Epoch 6/10, Train Loss: 1.5950, Eval Loss: 2.5183\n",
            "Epoch 7/10, Train Loss: 1.5232, Eval Loss: 2.5943\n",
            "Epoch 8/10, Train Loss: 1.3054, Eval Loss: 2.6888\n",
            "Epoch 9/10, Train Loss: 1.0037, Eval Loss: 2.8350\n",
            "Epoch 10/10, Train Loss: 0.8831, Eval Loss: 2.9779\n",
            "\n",
            "추론 결과:\n",
            "입력 텍스트: 김영희는 고려대학교 경제학과를 졸업하고 현재 삼성전자에서 마케팅 팀장으로 일하고 있습니다.\n",
            "식별된 개체:\n",
            "- [UNK] (ORG)\n",
            "- [UNK] (PER)\n",
            "- [UNK] (EDU)\n",
            "문장 민감도: 민감\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertTokenizerFast, DistilBertPreTrainedModel, DistilBertModel, DistilBertConfig\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 개체 및 태그 정의\n",
        "ENTITY_TAGS = {\n",
        "    \"PER\": [\"B-PER\", \"I-PER\"],\n",
        "    \"ORG\": [\"B-ORG\", \"I-ORG\"],\n",
        "    \"EDU\": [\"B-EDU\", \"I-EDU\"],\n",
        "    \"AFF\": [\"B-AFF\", \"I-AFF\"],\n",
        "    \"POS\": [\"B-POS\", \"I-POS\"],\n",
        "    \"LOC\": [\"B-LOC\", \"I-LOC\"],\n",
        "    \"DUR\": [\"B-DUR\", \"I-DUR\"]\n",
        "}\n",
        "\n",
        "class EnhancedNERModel(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.distilbert = DistilBertModel(config)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.sentence_classifier = nn.Linear(config.dim, 2)  # 문장 분류 (민감/비민감)\n",
        "        self.token_classifier = nn.Linear(config.dim * 2, config.num_labels)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        labels=None,\n",
        "        sentence_labels=None,\n",
        "    ):\n",
        "        outputs = self.distilbert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = sequence_output[:, 0]  # [CLS] 토큰의 출력\n",
        "\n",
        "        # 문장 수준 분류\n",
        "        sentence_logits = self.sentence_classifier(pooled_output)\n",
        "        sentence_loss = None\n",
        "        if sentence_labels is not None:\n",
        "            sentence_loss_fct = nn.CrossEntropyLoss()\n",
        "            sentence_loss = sentence_loss_fct(sentence_logits.view(-1, 2), sentence_labels.view(-1))\n",
        "\n",
        "        # 문장 수준 정보를 토큰 수준 정보와 결합\n",
        "        expanded_pooled_output = pooled_output.unsqueeze(1).expand(-1, sequence_output.size(1), -1)\n",
        "        combined_output = torch.cat([sequence_output, expanded_pooled_output], dim=-1)\n",
        "\n",
        "        token_logits = self.token_classifier(self.dropout(combined_output))\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            active_loss = attention_mask.view(-1) == 1\n",
        "            active_logits = token_logits.view(-1, self.num_labels)\n",
        "            active_labels = torch.where(\n",
        "                active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "            )\n",
        "            loss = loss_fct(active_logits, active_labels)\n",
        "            if sentence_loss is not None:\n",
        "                loss += sentence_loss\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"token_logits\": token_logits,\n",
        "            \"sentence_logits\": sentence_logits,\n",
        "        }\n",
        "\n",
        "    def decode(self, token_logits, attention_mask):\n",
        "        predictions = torch.argmax(token_logits, dim=-1)\n",
        "        return predictions\n",
        "\n",
        "\n",
        "class EnhancedNERDataset(Dataset):\n",
        "    def __init__(self, texts, tags, sentence_labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.tags = tags\n",
        "        self.sentence_labels = sentence_labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.label_dict = {\"O\": 0}\n",
        "        for entity, entity_tags in ENTITY_TAGS.items():\n",
        "            for tag in entity_tags:\n",
        "                if tag not in self.label_dict:\n",
        "                    self.label_dict[tag] = len(self.label_dict)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        tags = self.tags[idx]\n",
        "        sentence_label = self.sentence_labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        # 첫 번째 토큰([CLS])의 마스크를 1로 설정\n",
        "        attention_mask[0] = 1\n",
        "\n",
        "        # [CLS]와 [SEP] 토큰을 포함한 라벨 생성\n",
        "        labels = torch.tensor([-100] + [self.label_dict.get(tag, 0) for tag in tags] + [-100])\n",
        "\n",
        "        # 패딩\n",
        "        if len(labels) < self.max_len:\n",
        "            labels = torch.cat([labels, torch.tensor([-100] * (self.max_len - len(labels)))])\n",
        "        else:\n",
        "            labels = labels[:self.max_len]\n",
        "\n",
        "        # 첫 번째 토큰([CLS])의 라벨을 0으로 설정\n",
        "        labels[0] = 0\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"sentence_labels\": torch.tensor(sentence_label),\n",
        "        }\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        sentence_labels = batch[\"sentence_labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels, sentence_labels=sentence_labels)\n",
        "        loss = outputs[\"loss\"]\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            sentence_labels = batch[\"sentence_labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels, sentence_labels=sentence_labels)\n",
        "            total_loss += outputs[\"loss\"].item()\n",
        "    return total_loss / len(test_loader)\n",
        "\n",
        "\n",
        "def identify_entities(text, model, tokenizer, device, label_dict):\n",
        "    encoding = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        token_logits = outputs[\"token_logits\"]\n",
        "        sentence_logits = outputs[\"sentence_logits\"]\n",
        "\n",
        "    token_predictions = model.decode(token_logits, attention_mask)\n",
        "    sentence_prediction = torch.argmax(sentence_logits, dim=1).item()\n",
        "\n",
        "    # Instead of using tokenizer.convert_ids_to_tokens, use the original text's tokens\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0], skip_special_tokens=True)\n",
        "    token_predictions = token_predictions[0].cpu().numpy()\n",
        "\n",
        "    entities = []\n",
        "    current_entity = None\n",
        "    for token, label_id in zip(tokens, token_predictions):\n",
        "        label = list(label_dict.keys())[label_id]\n",
        "        if label.startswith(\"B-\"):\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "            current_entity = {\"type\": label[2:], \"text\": token}\n",
        "        elif label.startswith(\"I-\") and current_entity and current_entity[\"type\"] == label[2:]:\n",
        "            current_entity[\"text\"] += \" \" + token\n",
        "        else:\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = None\n",
        "\n",
        "    if current_entity:\n",
        "        entities.append(current_entity)\n",
        "\n",
        "    sentence_sensitivity = \"민감\" if sentence_prediction == 1 else \"비민감\"\n",
        "\n",
        "    return entities, sentence_sensitivity\n",
        "\n",
        "# 메인 실행 부분\n",
        "if __name__ == \"__main__\":\n",
        "    # 데이터 준비 (예시)\n",
        "    texts = [\n",
        "        \"김철수씨는 서울대학교 컴퓨터공학과를 졸업하고 현재 구글 코리아에서 소프트웨어 엔지니어로 2년째 근무 중입니다.\",\n",
        "        \"이회사 대표이사 박영희는 연세대학교 경영학과 출신으로 알려져 있습니다.\",\n",
        "        \"저는 한국대학교 물리학과에 재학 중인 학생입니다.\"\n",
        "    ]\n",
        "    tags = [\n",
        "        [\"B-PER\", \"I-PER\", \"O\", \"B-ORG\", \"I-ORG\", \"B-EDU\", \"I-EDU\", \"O\", \"O\", \"O\", \"B-ORG\", \"I-ORG\", \"O\", \"B-POS\", \"I-POS\", \"O\", \"B-DUR\", \"I-DUR\", \"O\", \"O\", \"O\"],\n",
        "        [\"O\", \"B-POS\", \"B-PER\", \"I-PER\", \"O\", \"B-ORG\", \"I-ORG\", \"B-EDU\", \"I-EDU\", \"O\", \"O\", \"O\", \"O\"],\n",
        "        [\"O\", \"B-ORG\", \"I-ORG\", \"B-EDU\", \"I-EDU\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
        "    ]\n",
        "    sentence_labels = [1, 1, 0]  # 1: 민감, 0: 비민감\n",
        "\n",
        "    # 데이터 분할\n",
        "    train_texts, test_texts, train_tags, test_tags, train_sentence_labels, test_sentence_labels = train_test_split(\n",
        "        texts, tags, sentence_labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # 토크나이저 및 모델 초기화\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"monologg/distilkobert\")\n",
        "    config = DistilBertConfig.from_pretrained(\"monologg/distilkobert\")\n",
        "    config.num_labels = len(ENTITY_TAGS) * 2 + 1  # B, I for each entity + O\n",
        "\n",
        "    # 데이터셋 및 데이터로더 생성\n",
        "    train_dataset = EnhancedNERDataset(train_texts, train_tags, train_sentence_labels, tokenizer)\n",
        "    test_dataset = EnhancedNERDataset(test_texts, test_tags, test_sentence_labels, tokenizer)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=2)\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = EnhancedNERModel(config)\n",
        "\n",
        "    # 학습 설정\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    # 학습\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train(model, train_loader, optimizer, device)\n",
        "        eval_loss = evaluate(model, test_loader, device)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}\")\n",
        "\n",
        "    # 모델 저장\n",
        "    torch.save(model.state_dict(), \"enhanced_ner_model.pth\")\n",
        "\n",
        "    # 추론 예시\n",
        "    test_text = \"김영희는 고려대학교 경제학과를 졸업하고 현재 삼성전자에서 마케팅 팀장으로 일하고 있습니다.\"\n",
        "    entities, sentence_sensitivity = identify_entities(test_text, model, tokenizer, device, train_dataset.label_dict)\n",
        "\n",
        "    print(\"\\n추론 결과:\")\n",
        "    print(\"입력 텍스트:\", test_text)\n",
        "    print(\"식별된 개체:\")\n",
        "    for entity in entities:\n",
        "        print(f\"- {entity['text']} ({entity['type']})\")\n",
        "    print(f\"문장 민감도: {sentence_sensitivity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxeqjvwzIb5f",
        "outputId": "b602d8e5-ad88-411b-8bfb-a51151e51f7b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 3.5436, Eval Loss: 3.2226\n",
            "Epoch 2/10, Train Loss: 2.5932, Eval Loss: 3.0084\n",
            "Epoch 3/10, Train Loss: 2.3537, Eval Loss: 2.8014\n",
            "Epoch 4/10, Train Loss: 1.8110, Eval Loss: 2.8706\n",
            "Epoch 5/10, Train Loss: 1.7926, Eval Loss: 3.0120\n",
            "Epoch 6/10, Train Loss: 1.5126, Eval Loss: 2.9965\n",
            "Epoch 7/10, Train Loss: 1.2298, Eval Loss: 2.9322\n",
            "Epoch 8/10, Train Loss: 1.1261, Eval Loss: 2.9384\n",
            "Epoch 9/10, Train Loss: 1.1242, Eval Loss: 3.0252\n",
            "Epoch 10/10, Train Loss: 0.8748, Eval Loss: 3.1504\n",
            "\n",
            "추론 결과:\n",
            "입력 텍스트: 김영희는 고려대학교 경제학과를 졸업하고 현재 삼성전자에서 마케팅 팀장으로 일하고 있습니다.\n",
            "식별된 개체:\n",
            "- . (POS)\n",
            "문장 민감도: 민감\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertTokenizerFast, DistilBertPreTrainedModel, DistilBertModel, DistilBertConfig\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 개체 및 태그 정의\n",
        "ENTITY_TAGS = {\n",
        "    \"PER\": [\"B-PER\", \"I-PER\"],\n",
        "    \"ORG\": [\"B-ORG\", \"I-ORG\"],\n",
        "    \"EDU\": [\"B-EDU\", \"I-EDU\"],\n",
        "    \"AFF\": [\"B-AFF\", \"I-AFF\"],\n",
        "    \"POS\": [\"B-POS\", \"I-POS\"],\n",
        "    \"LOC\": [\"B-LOC\", \"I-LOC\"],\n",
        "    \"DUR\": [\"B-DUR\", \"I-DUR\"]\n",
        "}\n",
        "\n",
        "class EnhancedNERModel(DistilBertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.distilbert = DistilBertModel(config)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.sentence_classifier = nn.Linear(config.dim, 2)  # 문장 분류 (민감/비민감)\n",
        "        self.token_classifier = nn.Linear(config.dim * 2, config.num_labels)\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        labels=None,\n",
        "        sentence_labels=None,\n",
        "    ):\n",
        "        outputs = self.distilbert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = sequence_output[:, 0]  # [CLS] 토큰의 출력\n",
        "\n",
        "        # 문장 수준 분류\n",
        "        sentence_logits = self.sentence_classifier(pooled_output)\n",
        "        sentence_loss = None\n",
        "        if sentence_labels is not None:\n",
        "            sentence_loss_fct = nn.CrossEntropyLoss()\n",
        "            sentence_loss = sentence_loss_fct(sentence_logits.view(-1, 2), sentence_labels.view(-1))\n",
        "\n",
        "        # 문장 수준 정보를 토큰 수준 정보와 결합\n",
        "        expanded_pooled_output = pooled_output.unsqueeze(1).expand(-1, sequence_output.size(1), -1)\n",
        "        combined_output = torch.cat([sequence_output, expanded_pooled_output], dim=-1)\n",
        "\n",
        "        token_logits = self.token_classifier(self.dropout(combined_output))\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            active_loss = attention_mask.view(-1) == 1\n",
        "            active_logits = token_logits.view(-1, self.num_labels)\n",
        "            active_labels = torch.where(\n",
        "                active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "            )\n",
        "            loss = loss_fct(active_logits, active_labels)\n",
        "            if sentence_loss is not None:\n",
        "                loss += sentence_loss\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"token_logits\": token_logits,\n",
        "            \"sentence_logits\": sentence_logits,\n",
        "        }\n",
        "\n",
        "    def decode(self, token_logits, attention_mask):\n",
        "        predictions = torch.argmax(token_logits, dim=-1)\n",
        "        return predictions\n",
        "\n",
        "\n",
        "class EnhancedNERDataset(Dataset):\n",
        "    def __init__(self, texts, tags, sentence_labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.tags = tags\n",
        "        self.sentence_labels = sentence_labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.label_dict = {\"O\": 0}\n",
        "        for entity, entity_tags in ENTITY_TAGS.items():\n",
        "            for tag in entity_tags:\n",
        "                if tag not in self.label_dict:\n",
        "                    self.label_dict[tag] = len(self.label_dict)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        tags = self.tags[idx]\n",
        "        sentence_label = self.sentence_labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
        "        input_ids = encoding[\"input_ids\"].squeeze()\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "        # 첫 번째 토큰([CLS])의 마스크를 1로 설정\n",
        "        attention_mask[0] = 1\n",
        "\n",
        "        # [CLS]와 [SEP] 토큰을 포함한 라벨 생성\n",
        "        labels = torch.tensor([-100] + [self.label_dict.get(tag, 0) for tag in tags] + [-100])\n",
        "\n",
        "        # 패딩\n",
        "        if len(labels) < self.max_len:\n",
        "            labels = torch.cat([labels, torch.tensor([-100] * (self.max_len - len(labels)))])\n",
        "        else:\n",
        "            labels = labels[:self.max_len]\n",
        "\n",
        "        # 첫 번째 토큰([CLS])의 라벨을 0으로 설정\n",
        "        labels[0] = 0\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"sentence_labels\": torch.tensor(sentence_label),\n",
        "        }\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        sentence_labels = batch[\"sentence_labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels, sentence_labels=sentence_labels)\n",
        "        loss = outputs[\"loss\"]\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate(model, test_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            sentence_labels = batch[\"sentence_labels\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels, sentence_labels=sentence_labels)\n",
        "            total_loss += outputs[\"loss\"].item()\n",
        "    return total_loss / len(test_loader)\n",
        "\n",
        "\n",
        "def identify_entities(text, model, tokenizer, device, label_dict):\n",
        "    encoding = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    input_ids = encoding[\"input_ids\"].to(device)\n",
        "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        token_logits = outputs[\"token_logits\"]\n",
        "        sentence_logits = outputs[\"sentence_logits\"]\n",
        "\n",
        "    token_predictions = model.decode(token_logits, attention_mask)\n",
        "    sentence_prediction = torch.argmax(sentence_logits, dim=1).item()\n",
        "\n",
        "    # Display the tokenized input\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "    token_predictions = token_predictions[0].cpu().numpy()\n",
        "\n",
        "    print(\"\\n[Tokenized Input and Predicted Tags]\")\n",
        "    for token, label_id in zip(tokens, token_predictions):\n",
        "        label = list(label_dict.keys())[label_id]\n",
        "        print(f\"Token: {token}, Tag: {label}\")\n",
        "\n",
        "    # Extract entities from the predicted tags\n",
        "    entities = []\n",
        "    current_entity = None\n",
        "    for token, label_id in zip(tokens, token_predictions):\n",
        "        label = list(label_dict.keys())[label_id]\n",
        "        if label.startswith(\"B-\"):\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "            current_entity = {\"type\": label[2:], \"text\": token}\n",
        "        elif label.startswith(\"I-\") and current_entity and current_entity[\"type\"] == label[2:]:\n",
        "            current_entity[\"text\"] += \" \" + token\n",
        "        else:\n",
        "            if current_entity:\n",
        "                entities.append(current_entity)\n",
        "                current_entity = None\n",
        "\n",
        "    if current_entity:\n",
        "        entities.append(current_entity)\n",
        "\n",
        "    sentence_sensitivity = \"민감\" if sentence_prediction == 1 else \"비민감\"\n",
        "\n",
        "    return entities, sentence_sensitivity\n",
        "\n",
        "# 메인 실행 부분\n",
        "if __name__ == \"__main__\":\n",
        "    # 데이터 준비 (예시)\n",
        "    texts = [\n",
        "        \"김철수씨는 서울대학교 컴퓨터공학과를 졸업하고 현재 구글 코리아에서 소프트웨어 엔지니어로 2년째 근무 중입니다.\",\n",
        "        \"이회사 대표이사 박영희는 연세대학교 경영학과 출신으로 알려져 있습니다.\",\n",
        "        \"저는 한국대학교 물리학과에 재학 중인 학생입니다.\"\n",
        "    ]\n",
        "    tags = [\n",
        "        [\"B-PER\", \"I-PER\", \"O\", \"B-ORG\", \"I-ORG\", \"B-EDU\", \"I-EDU\", \"O\", \"O\", \"O\", \"B-ORG\", \"I-ORG\", \"O\", \"B-POS\", \"I-POS\", \"O\", \"B-DUR\", \"I-DUR\", \"O\", \"O\", \"O\"],\n",
        "        [\"O\", \"B-POS\", \"B-PER\", \"I-PER\", \"O\", \"B-ORG\", \"I-ORG\", \"B-EDU\", \"I-EDU\", \"O\", \"O\", \"O\", \"O\"],\n",
        "        [\"O\", \"B-ORG\", \"I-ORG\", \"B-EDU\", \"I-EDU\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
        "    ]\n",
        "    sentence_labels = [1, 1, 0]  # 1: 민감, 0: 비민감\n",
        "\n",
        "    # 데이터 분할\n",
        "    train_texts, test_texts, train_tags, test_tags, train_sentence_labels, test_sentence_labels = train_test_split(\n",
        "        texts, tags, sentence_labels, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # 토크나이저 및 모델 초기화\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"monologg/distilkobert\")\n",
        "    config = DistilBertConfig.from_pretrained(\"monologg/distilkobert\")\n",
        "    config.num_labels = len(ENTITY_TAGS) * 2 + 1  # B, I for each entity + O\n",
        "\n",
        "    # 데이터셋 및 데이터로더 생성\n",
        "    train_dataset = EnhancedNERDataset(train_texts, train_tags, train_sentence_labels, tokenizer)\n",
        "    test_dataset = EnhancedNERDataset(test_texts, test_tags, test_sentence_labels, tokenizer)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=2)\n",
        "\n",
        "    # 모델 초기화\n",
        "    model = EnhancedNERModel(config)\n",
        "\n",
        "    # 학습 설정\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    # 학습\n",
        "    num_epochs = 10\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train(model, train_loader, optimizer, device)\n",
        "        eval_loss = evaluate(model, test_loader, device)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Eval Loss: {eval_loss:.4f}\")\n",
        "\n",
        "    # 모델 저장\n",
        "    torch.save(model.state_dict(), \"enhanced_ner_model.pth\")\n",
        "\n",
        "    # 추론 예시\n",
        "    test_text = \"김영희는 고려대학교 경제학과를 졸업하고 현재 삼성전자에서 마케팅 팀장으로 일하고 있습니다.\"\n",
        "    entities, sentence_sensitivity = identify_entities(test_text, model, tokenizer, device, train_dataset.label_dict)\n",
        "\n",
        "    print(\"\\n[Final Extracted Entities]\")\n",
        "    print(\"입력 텍스트:\", test_text)\n",
        "    print(\"식별된 개체:\")\n",
        "    for entity in entities:\n",
        "        print(f\"- {entity['text']} ({entity['type']})\")\n",
        "    print(f\"문장 민감도: {sentence_sensitivity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64xagYNbJYxQ",
        "outputId": "5b6627a8-5fac-49e3-e68b-582beac8df72"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 3.7597, Eval Loss: 2.6959\n",
            "Epoch 2/10, Train Loss: 2.6071, Eval Loss: 2.4316\n",
            "Epoch 3/10, Train Loss: 2.4268, Eval Loss: 2.3588\n",
            "Epoch 4/10, Train Loss: 1.8176, Eval Loss: 2.4500\n",
            "Epoch 5/10, Train Loss: 1.8143, Eval Loss: 2.5761\n",
            "Epoch 6/10, Train Loss: 1.6547, Eval Loss: 2.7241\n",
            "Epoch 7/10, Train Loss: 1.3008, Eval Loss: 2.7493\n",
            "Epoch 8/10, Train Loss: 1.1318, Eval Loss: 2.6602\n",
            "Epoch 9/10, Train Loss: 0.9772, Eval Loss: 2.5753\n",
            "Epoch 10/10, Train Loss: 1.0795, Eval Loss: 2.6232\n",
            "\n",
            "[Tokenized Input and Predicted Tags]\n",
            "Token: [CLS], Tag: O\n",
            "Token: [UNK], Tag: O\n",
            "Token: [UNK], Tag: B-POS\n",
            "Token: [UNK], Tag: B-PER\n",
            "Token: [UNK], Tag: B-EDU\n",
            "Token: 현재, Tag: I-EDU\n",
            "Token: [UNK], Tag: O\n",
            "Token: 마케팅, Tag: O\n",
            "Token: [UNK], Tag: B-EDU\n",
            "Token: [UNK], Tag: O\n",
            "Token: [UNK], Tag: O\n",
            "Token: ., Tag: I-EDU\n",
            "Token: [SEP], Tag: O\n",
            "\n",
            "[Final Extracted Entities]\n",
            "입력 텍스트: 김영희는 고려대학교 경제학과를 졸업하고 현재 삼성전자에서 마케팅 팀장으로 일하고 있습니다.\n",
            "식별된 개체:\n",
            "- [UNK] (POS)\n",
            "- [UNK] (PER)\n",
            "- [UNK] 현재 (EDU)\n",
            "- [UNK] (EDU)\n",
            "문장 민감도: 민감\n"
          ]
        }
      ]
    }
  ]
}