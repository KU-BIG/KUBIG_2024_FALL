{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "collapsed_sections": [
        "RoW37xCxVCUD",
        "LXGH0R8oVg4c",
        "_2KjSMiaVkhS",
        "E3wO7KOmtH5-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ☑️ T5"
      ],
      "metadata": {
        "id": "RoW37xCxVCUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 기존"
      ],
      "metadata": {
        "id": "LXGH0R8oVg4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers datasets"
      ],
      "metadata": {
        "id": "gCUiJXaRVHVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "Nd7cZ8VUVKKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# T5 모델 및 토크나이저 로드\n",
        "model_name = \"google/mt5-small\"  # T5의 한국어 성능을 높이기 위해 Multilingual T5 모델을 사용\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 데이터셋 로드\n",
        "train_dataset = load_dataset('json', data_files='/content/drive/MyDrive/일상대화요약_데이터/일상대화요약_train_processed.json')\n",
        "dev_dataset = load_dataset('json', data_files='/content/drive/MyDrive/일상대화요약_데이터/일상대화요약_dev_processed.json')"
      ],
      "metadata": {
        "id": "nH77z13eVTYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    # 모든 대화를 하나의 텍스트로 결합\n",
        "    inputs = [\" \".join([turn[\"utterance\"] for turn in example[\"conversation\"]]) for example in examples[\"input\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "    # 목표 요약 생성\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"output\"], max_length=128, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# 데이터셋 토크나이징 및 입력 데이터 준비\n",
        "train_tokenized_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "dev_tokenized_dataset = dev_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "YPglbtNQVUNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=50,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized_dataset['train'],\n",
        "    eval_dataset=dev_tokenized_dataset['train'],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "VZM_leLIVVL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model and tokenizer\n",
        "trainer.save_model(\"./t5-summarization\")\n",
        "tokenizer.save_pretrained(\"./t5-summarization\")\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)"
      ],
      "metadata": {
        "id": "Yr5lcCq-VWsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "def summarize_conversation(conversation, tokenizer, model, max_length=150, num_sentences=5):\n",
        "    conversation_text = \" \".join([utterance['utterance'] for utterance in conversation])\n",
        "\n",
        "    # Tokenize the input conversation\n",
        "    inputs = tokenizer(conversation_text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=512)\n",
        "\n",
        "    # Move input tensors to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Generate the summary using the model\n",
        "    summary_ids = model.generate(inputs['input_ids'], max_length=max_length, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode the generated summary\n",
        "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # 문장 단위로 분리하고, 첫 5문장만 선택\n",
        "    sentences = generated_summary.split('. ')\n",
        "    selected_summary = '. '.join(sentences[:num_sentences])\n",
        "\n",
        "    # 선택된 문장이 5개보다 적다면, 마지막에 온점을 추가\n",
        "    if not selected_summary.endswith('.'):\n",
        "        selected_summary += '.'\n",
        "\n",
        "    return selected_summary\n",
        "\n",
        "# 테스트 데이터 로드\n",
        "test_data = json.load(open('/content/drive/MyDrive/일상대화요약_데이터/일상대화요약_test_processed.json'))\n",
        "\n",
        "# 테스트 데이터에 대한 요약 생성 및 출력\n",
        "for i, item in enumerate(test_data):\n",
        "    example_conversation = item['input']['conversation']\n",
        "    conversation_text = \" \".join([utterance['utterance'] for utterance in example_conversation])\n",
        "    generated_summary = summarize_conversation(example_conversation, tokenizer, model)\n",
        "\n",
        "    print(f\"ID: {item['id']}\")\n",
        "    print(f\"Conversation:\\n{conversation_text}\\n\")\n",
        "    print(f\"Generated Summary:\\n{generated_summary}\\n\")\n",
        "    print(\"-\" * 80)  # Separator for readability"
      ],
      "metadata": {
        "id": "Jj63lDA4VZDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PEFT(LORA) 적용"
      ],
      "metadata": {
        "id": "_2KjSMiaVkhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers datasets"
      ],
      "metadata": {
        "id": "paFroHGkVnAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "id": "p4xPQoTKVrbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "from peft import LoraConfig, get_peft_model"
      ],
      "metadata": {
        "id": "pqHl95brVsp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 및 토크나이저 로드\n",
        "model_name = \"google/mt5-small\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# PEFT 설정 (LoRA 적용)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "# PEFT 적용\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# 훈련 인자 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=30,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    remove_unused_columns=False, # 불필요한 열 제거하지 않음\n",
        "    fp16=True,  # 16비트 부동소수점 사용, GPU 효율\n",
        ")"
      ],
      "metadata": {
        "id": "xQH_LKcaVt3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 토크나이징 + 프롬프트\n",
        "def preprocess_function(examples):\n",
        "    # 대화 문장을 하나의 텍스트로 결합하고, 요약 프롬프트를 추가\n",
        "    inputs = [\"summarize: \" + \" \".join([turn[\"utterance\"] for turn in example[\"conversation\"]]) for example in examples[\"input\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # 목표 요약 생성\n",
        "    labels = tokenizer(examples[\"output\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# 데이터셋 로드\n",
        "dataset = load_dataset('json', data_files={\n",
        "    'train': '/content/drive/MyDrive/일상대화요약_데이터/일상대화요약_train_processed.json',\n",
        "    'validation': '/content/drive/MyDrive/일상대화요약_데이터/일상대화요약_dev_processed.json'\n",
        "})\n",
        "\n",
        "# 데이터셋 전처리\n",
        "train_tokenized_dataset = dataset['train'].map(preprocess_function, batched=True)\n",
        "dev_tokenized_dataset = dataset['validation'].map(preprocess_function, batched=True)\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "Em4LL-nFVux1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer 설정\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized_dataset,\n",
        "    eval_dataset=dev_tokenized_dataset,\n",
        "    data_collator=data_collator  # Seq2Seq에 맞는 데이터 콜레이터 설정\n",
        ")\n",
        "\n",
        "# 모델 훈련\n",
        "trainer.train()\n",
        "\n",
        "# 훈련 후 모델 저장\n",
        "model.save_pretrained(\"./t5_lora\")\n",
        "tokenizer.save_pretrained(\"./t5_lora\")"
      ],
      "metadata": {
        "id": "3JsHmvNLVwxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리된 데이터 확인\n",
        "print(train_tokenized_dataset[0])"
      ],
      "metadata": {
        "id": "VmkAANKPVy6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ KoBart v2"
      ],
      "metadata": {
        "id": "LNgrJgJ-AUhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터 및 데이터로더 준비"
      ],
      "metadata": {
        "id": "wp6lgtrDtAfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "a18ej0xui6Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjfbMCELpRRK",
        "outputId": "74f51f12-8a74-4400-d74d-57d5b5ab9245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback"
      ],
      "metadata": {
        "id": "qiCisGB4jCkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gogamza/kobart-base-v2\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"gogamza/kobart-base-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB1f4Qc_7AcU",
        "outputId": "10b78005-d800-40c9-bbb1-937c7e62a8e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 사용자 정의 데이터셋 클래스\n",
        "class ConversationSummaryDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        conversation = \" \".join([utterance['utterance'] for utterance in item['input']['conversation']])\n",
        "        summary = item['output']\n",
        "\n",
        "        # 입력과 출력을 토큰화\n",
        "        input_encodings = self.tokenizer(conversation, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "        output_encodings = self.tokenizer(summary, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        # 디코더를 위한 레이블을 이동시키기 위해 타겟 출력 텐서를 설정\n",
        "        labels = output_encodings['input_ids']\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_encodings['input_ids'].squeeze(),\n",
        "            'attention_mask': input_encodings['attention_mask'].squeeze(),\n",
        "            'labels': labels.squeeze()\n",
        "        }\n",
        "\n",
        "# 예제 학습 데이터\n",
        "train_data = json.load(open('/content/drive/MyDrive/24-summer KUBIG NLP/PROJECT/일상대화요약_데이터/일상대화요약_train.json'))\n",
        "eval_data = json.load(open('/content/drive/MyDrive/24-summer KUBIG NLP/PROJECT/일상대화요약_데이터/일상대화요약_dev.json'))\n",
        "test_data = json.load(open('/content/drive/MyDrive/24-summer KUBIG NLP/PROJECT/일상대화요약_데이터/일상대화요약_test.json'))\n",
        "\n",
        "# 데이터셋 인스턴스화\n",
        "train_dataset = ConversationSummaryDataset(train_data, tokenizer)\n",
        "eval_dataset = ConversationSummaryDataset(eval_data, tokenizer)\n",
        "\n",
        "# DataLoader 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "zVAjrfzcAWml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "f3y_BSs6tDrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./kobart-summarization\",\n",
        "    num_train_epochs=50,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    eval_accumulation_steps=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_steps=2000,\n",
        "    eval_steps=2000,\n",
        "    save_total_limit=3,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,\n",
        "    learning_rate=1e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgzJ7gG4EOhW",
        "outputId": "e403efca-31bd-49e8-bae2-6460f93f7d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")"
      ],
      "metadata": {
        "id": "UfjB78kKmRR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "1FxXk4C2AeCk",
        "outputId": "1104d845-3944-4c51-b5d1-d2d53352fac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8000' max='12650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 8000/12650 18:55 < 11:00, 7.04 it/s, Epoch 31/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.059200</td>\n",
              "      <td>2.848609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.308800</td>\n",
              "      <td>3.184103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.724900</td>\n",
              "      <td>3.415820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.477400</td>\n",
              "      <td>3.560427</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
            "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=8000, training_loss=1.472987486243248, metrics={'train_runtime': 1136.907, 'train_samples_per_second': 22.253, 'train_steps_per_second': 11.127, 'total_flos': 4877891665920000.0, 'train_loss': 1.472987486243248, 'epoch': 31.620553359683793})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 제출 모델 저장\n",
        "trainer.save_model(\"./kobart-summarization_0822_1\")\n",
        "tokenizer.save_pretrained(\"./kobart-summarization_0822_1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeIDpDSktMud",
        "outputId": "6418c641-c4ba-4afe-b59d-2b7473e97fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 1}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./kobart-summarization_0822_1/tokenizer_config.json',\n",
              " './kobart-summarization_0822_1/special_tokens_map.json',\n",
              " './kobart-summarization_0822_1/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation"
      ],
      "metadata": {
        "id": "E3wO7KOmtH5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "vO4VtsF_Eyu4",
        "outputId": "01cad14d-b821-45dd-c28e-fe228b5cbd4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [51/51 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 2.848609209060669,\n",
              " 'eval_runtime': 1.8215,\n",
              " 'eval_samples_per_second': 55.997,\n",
              " 'eval_steps_per_second': 27.999,\n",
              " 'epoch': 31.620553359683793}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 생성"
      ],
      "metadata": {
        "id": "I5DBwQF1F9OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast"
      ],
      "metadata": {
        "id": "j3l0hFA5GWcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./kobart-summarization_0822_1\"\n",
        "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xkAqsweGDvc",
        "outputId": "2cac8a5e-690d-4fc2-ca8d-6b65f6987c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "5jPG-kU0Gwg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "341ce253-b735-45cb-dff4-1e5c57cc7de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(30000, 768, padding_idx=3)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "results = []\n",
        "\n",
        "# 빔 서치, length_penalty 및 n-gram 크기 설정\n",
        "beam_size = 6\n",
        "max_length_summary = 128\n",
        "length_penalty = 0.9\n",
        "no_repeat_ngram_size = 3\n",
        "\n",
        "# test_data의 각 항목을 처리\n",
        "for item in test_data:\n",
        "    # 대화 발화를 하나의 문자열로 결합\n",
        "    conversation = \" \".join([utterance['utterance'] for utterance in item['input']['conversation']])\n",
        "\n",
        "    # 입력 텍스트를 토큰화\n",
        "    inputs = tokenizer(conversation, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # 요약 생성\n",
        "    with torch.no_grad():\n",
        "        summaries = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=max_length_summary,\n",
        "            num_beams=beam_size,\n",
        "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "            early_stopping=True,\n",
        "            length_penalty=length_penalty,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # 요약을 디코딩\n",
        "    decoded_summaries = [tokenizer.decode(summary, skip_special_tokens=True) for summary in summaries]\n",
        "\n",
        "    # 후처리: 공백 제거 및 사소한 문제 수정\n",
        "    processed_summary = decoded_summaries[0].strip()\n",
        "\n",
        "    # 출력 JSON 구조 생성\n",
        "    result = {\n",
        "        \"id\": item[\"id\"],  # test data의 동일한 ID 사용\n",
        "        \"input\": item[\"input\"],  # 입력 데이터를 그대로 포함\n",
        "        \"subject_keyword\": item.get(\"subject_keyword\", \"\"),  # 존재하는 경우 주제 키워드 포함\n",
        "        \"output\": processed_summary  # 처리된 요약을 출력으로 추가\n",
        "    }\n",
        "\n",
        "    # 결과 목록에 추가\n",
        "    results.append(result)\n",
        "\n",
        "    # 선택적으로 각 결과를 확인하기 위해 출력\n",
        "    print(f\"ID {item['id']}에 대한 생성된 결과:\")\n",
        "    print(json.dumps(result, ensure_ascii=False, indent=4))\n",
        "    print(\"-\" * 50)  # 가독성을 위한 구분선\n",
        "\n",
        "# 결과를 JSON 파일로 저장\n",
        "with open('/content/drive/MyDrive/24-summer KUBIG NLP/inference_results_3.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"추론 결과가 'inference_results.json'에 저장되었습니다.\")"
      ],
      "metadata": {
        "id": "Xv6Z3wiVG0Gm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}