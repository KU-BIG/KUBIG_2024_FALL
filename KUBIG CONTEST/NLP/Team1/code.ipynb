{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "collapsed_sections": [
        "RoW37xCxVCUD",
        "LXGH0R8oVg4c",
        "_2KjSMiaVkhS",
        "E3wO7KOmtH5-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# â˜‘ï¸ T5"
      ],
      "metadata": {
        "id": "RoW37xCxVCUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ê¸°ì¡´"
      ],
      "metadata": {
        "id": "LXGH0R8oVg4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers datasets"
      ],
      "metadata": {
        "id": "gCUiJXaRVHVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "Nd7cZ8VUVKKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# T5 ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "model_name = \"google/mt5-small\"  # T5ì˜ í•œêµ­ì–´ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ Multilingual T5 ëª¨ë¸ì„ ì‚¬ìš©\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "train_dataset = load_dataset('json', data_files='/content/drive/MyDrive/ì¼ìƒëŒ€í™”ìš”ì•½_ë°ì´í„°/ì¼ìƒëŒ€í™”ìš”ì•½_train_processed.json')\n",
        "dev_dataset = load_dataset('json', data_files='/content/drive/MyDrive/ì¼ìƒëŒ€í™”ìš”ì•½_ë°ì´í„°/ì¼ìƒëŒ€í™”ìš”ì•½_dev_processed.json')"
      ],
      "metadata": {
        "id": "nH77z13eVTYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    # ëª¨ë“  ëŒ€í™”ë¥¼ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©\n",
        "    inputs = [\" \".join([turn[\"utterance\"] for turn in example[\"conversation\"]]) for example in examples[\"input\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "    # ëª©í‘œ ìš”ì•½ ìƒì„±\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"output\"], max_length=128, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• ë° ì…ë ¥ ë°ì´í„° ì¤€ë¹„\n",
        "train_tokenized_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "dev_tokenized_dataset = dev_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "YPglbtNQVUNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í›ˆë ¨\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=50,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized_dataset['train'],\n",
        "    eval_dataset=dev_tokenized_dataset['train'],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "VZM_leLIVVL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model and tokenizer\n",
        "trainer.save_model(\"./t5-summarization\")\n",
        "tokenizer.save_pretrained(\"./t5-summarization\")\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(eval_results)"
      ],
      "metadata": {
        "id": "Yr5lcCq-VWsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "def summarize_conversation(conversation, tokenizer, model, max_length=150, num_sentences=5):\n",
        "    conversation_text = \" \".join([utterance['utterance'] for utterance in conversation])\n",
        "\n",
        "    # Tokenize the input conversation\n",
        "    inputs = tokenizer(conversation_text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=512)\n",
        "\n",
        "    # Move input tensors to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Generate the summary using the model\n",
        "    summary_ids = model.generate(inputs['input_ids'], max_length=max_length, num_beams=4, early_stopping=True)\n",
        "\n",
        "    # Decode the generated summary\n",
        "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ê³ , ì²« 5ë¬¸ì¥ë§Œ ì„ íƒ\n",
        "    sentences = generated_summary.split('. ')\n",
        "    selected_summary = '. '.join(sentences[:num_sentences])\n",
        "\n",
        "    # ì„ íƒëœ ë¬¸ì¥ì´ 5ê°œë³´ë‹¤ ì ë‹¤ë©´, ë§ˆì§€ë§‰ì— ì˜¨ì ì„ ì¶”ê°€\n",
        "    if not selected_summary.endswith('.'):\n",
        "        selected_summary += '.'\n",
        "\n",
        "    return selected_summary\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
        "test_data = json.load(open('/content/drive/MyDrive/ì¼ìƒëŒ€í™”ìš”ì•½_ë°ì´í„°/ì¼ìƒëŒ€í™”ìš”ì•½_test_processed.json'))\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ìš”ì•½ ìƒì„± ë° ì¶œë ¥\n",
        "for i, item in enumerate(test_data):\n",
        "    example_conversation = item['input']['conversation']\n",
        "    conversation_text = \" \".join([utterance['utterance'] for utterance in example_conversation])\n",
        "    generated_summary = summarize_conversation(example_conversation, tokenizer, model)\n",
        "\n",
        "    print(f\"ID: {item['id']}\")\n",
        "    print(f\"Conversation:\\n{conversation_text}\\n\")\n",
        "    print(f\"Generated Summary:\\n{generated_summary}\\n\")\n",
        "    print(\"-\" * 80)  # Separator for readability"
      ],
      "metadata": {
        "id": "Jj63lDA4VZDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PEFT(LORA) ì ìš©"
      ],
      "metadata": {
        "id": "_2KjSMiaVkhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers datasets"
      ],
      "metadata": {
        "id": "paFroHGkVnAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "id": "p4xPQoTKVrbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n",
        "from peft import LoraConfig, get_peft_model"
      ],
      "metadata": {
        "id": "pqHl95brVsp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "model_name = \"google/mt5-small\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# PEFT ì„¤ì • (LoRA ì ìš©)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "# PEFT ì ìš©\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# í›ˆë ¨ ì¸ì ì„¤ì •\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=30,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    remove_unused_columns=False, # ë¶ˆí•„ìš”í•œ ì—´ ì œê±°í•˜ì§€ ì•ŠìŒ\n",
        "    fp16=True,  # 16ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  ì‚¬ìš©, GPU íš¨ìœ¨\n",
        ")"
      ],
      "metadata": {
        "id": "xQH_LKcaVt3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë°ì´í„°ì…‹ í† í¬ë‚˜ì´ì§• + í”„ë¡¬í”„íŠ¸\n",
        "def preprocess_function(examples):\n",
        "    # ëŒ€í™” ë¬¸ì¥ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©í•˜ê³ , ìš”ì•½ í”„ë¡¬í”„íŠ¸ë¥¼ ì¶”ê°€\n",
        "    inputs = [\"summarize: \" + \" \".join([turn[\"utterance\"] for turn in example[\"conversation\"]]) for example in examples[\"input\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # ëª©í‘œ ìš”ì•½ ìƒì„±\n",
        "    labels = tokenizer(examples[\"output\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# ë°ì´í„°ì…‹ ë¡œë“œ\n",
        "dataset = load_dataset('json', data_files={\n",
        "    'train': '/content/drive/MyDrive/ì¼ìƒëŒ€í™”ìš”ì•½_ë°ì´í„°/ì¼ìƒëŒ€í™”ìš”ì•½_train_processed.json',\n",
        "    'validation': '/content/drive/MyDrive/ì¼ìƒëŒ€í™”ìš”ì•½_ë°ì´í„°/ì¼ìƒëŒ€í™”ìš”ì•½_dev_processed.json'\n",
        "})\n",
        "\n",
        "# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬\n",
        "train_tokenized_dataset = dataset['train'].map(preprocess_function, batched=True)\n",
        "dev_tokenized_dataset = dataset['validation'].map(preprocess_function, batched=True)\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "id": "Em4LL-nFVux1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer ì„¤ì •\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized_dataset,\n",
        "    eval_dataset=dev_tokenized_dataset,\n",
        "    data_collator=data_collator  # Seq2Seqì— ë§ëŠ” ë°ì´í„° ì½œë ˆì´í„° ì„¤ì •\n",
        ")\n",
        "\n",
        "# ëª¨ë¸ í›ˆë ¨\n",
        "trainer.train()\n",
        "\n",
        "# í›ˆë ¨ í›„ ëª¨ë¸ ì €ì¥\n",
        "model.save_pretrained(\"./t5_lora\")\n",
        "tokenizer.save_pretrained(\"./t5_lora\")"
      ],
      "metadata": {
        "id": "3JsHmvNLVwxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì „ì²˜ë¦¬ëœ ë°ì´í„° í™•ì¸\n",
        "print(train_tokenized_dataset[0])"
      ],
      "metadata": {
        "id": "VmkAANKPVy6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœ… KoBart v2"
      ],
      "metadata": {
        "id": "LNgrJgJ-AUhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ë°ì´í„° ë° ë°ì´í„°ë¡œë” ì¤€ë¹„"
      ],
      "metadata": {
        "id": "wp6lgtrDtAfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "a18ej0xui6Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjfbMCELpRRK",
        "outputId": "74f51f12-8a74-4400-d74d-57d5b5ab9245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback"
      ],
      "metadata": {
        "id": "qiCisGB4jCkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gogamza/kobart-base-v2\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"gogamza/kobart-base-v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB1f4Qc_7AcU",
        "outputId": "10b78005-d800-40c9-bbb1-937c7e62a8e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n",
        "class ConversationSummaryDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        conversation = \" \".join([utterance['utterance'] for utterance in item['input']['conversation']])\n",
        "        summary = item['output']\n",
        "\n",
        "        # ì…ë ¥ê³¼ ì¶œë ¥ì„ í† í°í™”\n",
        "        input_encodings = self.tokenizer(conversation, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "        output_encodings = self.tokenizer(summary, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
        "\n",
        "        # ë””ì½”ë”ë¥¼ ìœ„í•œ ë ˆì´ë¸”ì„ ì´ë™ì‹œí‚¤ê¸° ìœ„í•´ íƒ€ê²Ÿ ì¶œë ¥ í…ì„œë¥¼ ì„¤ì •\n",
        "        labels = output_encodings['input_ids']\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_encodings['input_ids'].squeeze(),\n",
        "            'attention_mask': input_encodings['attention_mask'].squeeze(),\n",
        "            'labels': labels.squeeze()\n",
        "        }\n",
        "\n",
        "# ì˜ˆì œ í•™ìŠµ ë°ì´í„°\n",
        "train_data = json.load(open('/content/drive/MyDrive/24-summer KUBIG NLP/PROJECT/ì¼ìƒëŒ€í™”ìš”ì•½_ë°ì´í„°/ì¼ìƒëŒ€í™”ìš”ì•½_train.json'))\n",
        "eval_data = json.load(open('/content/drive/MyDrive/24-summer KUBIG NLP/PROJECT/á„‹á…µá†¯á„‰á…¡á†¼á„ƒá…¢á„’á…ªá„‹á…­á„‹á…£á†¨_á„ƒá…¦á„‹á…µá„á…¥/á„‹á…µá†¯á„‰á…¡á†¼á„ƒá…¢á„’á…ªá„‹á…­á„‹á…£á†¨_dev.json'))\n",
        "test_data = json.load(open('/content/drive/MyDrive/24-summer KUBIG NLP/PROJECT/ì¼ìƒëŒ€í™”ìš”ì•½_ë°ì´í„°/ì¼ìƒëŒ€í™”ìš”ì•½_test.json'))\n",
        "\n",
        "# ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤í™”\n",
        "train_dataset = ConversationSummaryDataset(train_data, tokenizer)\n",
        "eval_dataset = ConversationSummaryDataset(eval_data, tokenizer)\n",
        "\n",
        "# DataLoader ìƒì„±\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "zVAjrfzcAWml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "f3y_BSs6tDrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./kobart-summarization\",\n",
        "    num_train_epochs=50,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    eval_accumulation_steps=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_steps=2000,\n",
        "    eval_steps=2000,\n",
        "    save_total_limit=3,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,\n",
        "    learning_rate=1e-5,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgzJ7gG4EOhW",
        "outputId": "e403efca-31bd-49e8-bae2-6460f93f7d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")"
      ],
      "metadata": {
        "id": "UfjB78kKmRR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "1FxXk4C2AeCk",
        "outputId": "1104d845-3944-4c51-b5d1-d2d53352fac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8000' max='12650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 8000/12650 18:55 < 11:00, 7.04 it/s, Epoch 31/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.059200</td>\n",
              "      <td>2.848609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.308800</td>\n",
              "      <td>3.184103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.724900</td>\n",
              "      <td>3.415820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.477400</td>\n",
              "      <td>3.560427</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 1}\n",
            "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=8000, training_loss=1.472987486243248, metrics={'train_runtime': 1136.907, 'train_samples_per_second': 22.253, 'train_steps_per_second': 11.127, 'total_flos': 4877891665920000.0, 'train_loss': 1.472987486243248, 'epoch': 31.620553359683793})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ì œì¶œ ëª¨ë¸ ì €ì¥\n",
        "trainer.save_model(\"./kobart-summarization_0822_1\")\n",
        "tokenizer.save_pretrained(\"./kobart-summarization_0822_1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeIDpDSktMud",
        "outputId": "6418c641-c4ba-4afe-b59d-2b7473e97fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 1}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./kobart-summarization_0822_1/tokenizer_config.json',\n",
              " './kobart-summarization_0822_1/special_tokens_map.json',\n",
              " './kobart-summarization_0822_1/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation"
      ],
      "metadata": {
        "id": "E3wO7KOmtH5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "vO4VtsF_Eyu4",
        "outputId": "01cad14d-b821-45dd-c28e-fe228b5cbd4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [51/51 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 2.848609209060669,\n",
              " 'eval_runtime': 1.8215,\n",
              " 'eval_samples_per_second': 55.997,\n",
              " 'eval_steps_per_second': 27.999,\n",
              " 'epoch': 31.620553359683793}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ìƒì„±"
      ],
      "metadata": {
        "id": "I5DBwQF1F9OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast"
      ],
      "metadata": {
        "id": "j3l0hFA5GWcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./kobart-summarization_0822_1\"\n",
        "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xkAqsweGDvc",
        "outputId": "2cac8a5e-690d-4fc2-ca8d-6b65f6987c16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "id": "5jPG-kU0Gwg1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "341ce253-b735-45cb-dff4-1e5c57cc7de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(30000, 768, padding_idx=3)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "results = []\n",
        "\n",
        "# ë¹” ì„œì¹˜, length_penalty ë° n-gram í¬ê¸° ì„¤ì •\n",
        "beam_size = 6\n",
        "max_length_summary = 128\n",
        "length_penalty = 0.9\n",
        "no_repeat_ngram_size = 3\n",
        "\n",
        "# test_dataì˜ ê° í•­ëª©ì„ ì²˜ë¦¬\n",
        "for item in test_data:\n",
        "    # ëŒ€í™” ë°œí™”ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©\n",
        "    conversation = \" \".join([utterance['utterance'] for utterance in item['input']['conversation']])\n",
        "\n",
        "    # ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”\n",
        "    inputs = tokenizer(conversation, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # ìš”ì•½ ìƒì„±\n",
        "    with torch.no_grad():\n",
        "        summaries = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_length=max_length_summary,\n",
        "            num_beams=beam_size,\n",
        "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "            early_stopping=True,\n",
        "            length_penalty=length_penalty,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # ìš”ì•½ì„ ë””ì½”ë”©\n",
        "    decoded_summaries = [tokenizer.decode(summary, skip_special_tokens=True) for summary in summaries]\n",
        "\n",
        "    # í›„ì²˜ë¦¬: ê³µë°± ì œê±° ë° ì‚¬ì†Œí•œ ë¬¸ì œ ìˆ˜ì •\n",
        "    processed_summary = decoded_summaries[0].strip()\n",
        "\n",
        "    # ì¶œë ¥ JSON êµ¬ì¡° ìƒì„±\n",
        "    result = {\n",
        "        \"id\": item[\"id\"],  # test dataì˜ ë™ì¼í•œ ID ì‚¬ìš©\n",
        "        \"input\": item[\"input\"],  # ì…ë ¥ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ í¬í•¨\n",
        "        \"subject_keyword\": item.get(\"subject_keyword\", \"\"),  # ì¡´ì¬í•˜ëŠ” ê²½ìš° ì£¼ì œ í‚¤ì›Œë“œ í¬í•¨\n",
        "        \"output\": processed_summary  # ì²˜ë¦¬ëœ ìš”ì•½ì„ ì¶œë ¥ìœ¼ë¡œ ì¶”ê°€\n",
        "    }\n",
        "\n",
        "    # ê²°ê³¼ ëª©ë¡ì— ì¶”ê°€\n",
        "    results.append(result)\n",
        "\n",
        "    # ì„ íƒì ìœ¼ë¡œ ê° ê²°ê³¼ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ì¶œë ¥\n",
        "    print(f\"ID {item['id']}ì— ëŒ€í•œ ìƒì„±ëœ ê²°ê³¼:\")\n",
        "    print(json.dumps(result, ensure_ascii=False, indent=4))\n",
        "    print(\"-\" * 50)  # ê°€ë…ì„±ì„ ìœ„í•œ êµ¬ë¶„ì„ \n",
        "\n",
        "# ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\n",
        "with open('/content/drive/MyDrive/24-summer KUBIG NLP/inference_results_3.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"ì¶”ë¡  ê²°ê³¼ê°€ 'inference_results.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "id": "Xv6Z3wiVG0Gm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}